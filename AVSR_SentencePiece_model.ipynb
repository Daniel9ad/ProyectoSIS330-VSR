{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Dk1GpTQt3RNG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir1K8fKWzyDM",
        "outputId": "299a3e09-aa83-4047-f993-88288cda54f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'auto_avsr'...\n",
            "remote: Enumerating objects: 268, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 268 (delta 34), reused 22 (delta 22), pack-reused 216\u001b[K\n",
            "Receiving objects: 100% (268/268), 31.46 MiB | 15.08 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mpc001/auto_avsr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "crloPN-J0jRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generar input.txt"
      ],
      "metadata": {
        "id": "Dk1GpTQt3RNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener la ruta de la carpeta\n",
        "ruta_carpeta = \"/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing/data/text\"\n",
        "\n",
        "# Lista para almacenar las oraciones\n",
        "oraciones = []\n",
        "\n",
        "# Recorrer los archivos .txt en la carpeta\n",
        "for archivo in tqdm(os.listdir(ruta_carpeta)):\n",
        "    # Si el archivo es un .txt\n",
        "    if archivo.endswith(\".txt\"):\n",
        "        # Abrir el archivo en modo lectura\n",
        "        with open(os.path.join(ruta_carpeta, archivo), \"r\") as f:\n",
        "            # Leer cada línea del archivo\n",
        "            for linea in f:\n",
        "                # Agregar la línea a la lista de oraciones\n",
        "                oraciones.append(linea.strip())\n",
        "\n",
        "# Abrir un nuevo archivo .txt en modo escritura\n",
        "with open(\"/content/auto_avsr/spm/input.txt\", \"w\") as f:\n",
        "    # Escribir cada oración en el nuevo archivo\n",
        "    for oracion in oraciones:\n",
        "        f.write(oracion + \"\\n\")\n",
        "\n",
        "print(\"¡Archivo 'oraciones.txt' generado con éxito!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prGo08ig2BR6",
        "outputId": "b520e87f-0f45-4bed-c635-c8de9dd93689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:13<00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Archivo 'oraciones.txt' generado con éxito!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar SentencePiece"
      ],
      "metadata": {
        "id": "TutblO6J3ZsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/auto_avsr/spm\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynxzM9mv4tVM",
        "outputId": "77e46246-1919-4669-8987-78ccaddc5ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/auto_avsr/spm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash train.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CUzfZoi4pld",
        "outputId": "6b2580ac-0250-4cc8-91b0-14dc327824fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=input.txt --vocab_size=40 --model_type=unigram --model_prefix=unigram/unigram40 --input_sentence_size=100000000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: input.txt\n",
            "  input_format: \n",
            "  model_prefix: unigram/unigram40\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 40\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 100000000\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: input.txt\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 42 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=9831\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9695% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=36\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999695\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 42 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=5335\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 1454 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 42\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 588\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 588 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=681 obj=10.7273 num_tokens=1377 num_tokens/piece=2.02203\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=587 obj=9.65831 num_tokens=1395 num_tokens/piece=2.37649\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=440 obj=10.0002 num_tokens=1523 num_tokens/piece=3.46136\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=438 obj=9.83893 num_tokens=1524 num_tokens/piece=3.47945\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=328 obj=10.6451 num_tokens=1746 num_tokens/piece=5.32317\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=327 obj=10.5094 num_tokens=1746 num_tokens/piece=5.33945\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=245 obj=11.6815 num_tokens=2055 num_tokens/piece=8.38776\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=245 obj=11.4518 num_tokens=2055 num_tokens/piece=8.38776\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=183 obj=12.6844 num_tokens=2390 num_tokens/piece=13.0601\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=183 obj=12.4915 num_tokens=2391 num_tokens/piece=13.0656\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=137 obj=14.0127 num_tokens=2741 num_tokens/piece=20.0073\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=137 obj=13.664 num_tokens=2741 num_tokens/piece=20.0073\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=102 obj=14.8752 num_tokens=3099 num_tokens/piece=30.3824\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=102 obj=14.7094 num_tokens=3122 num_tokens/piece=30.6078\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=76 obj=15.7364 num_tokens=3428 num_tokens/piece=45.1053\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=76 obj=15.4987 num_tokens=3428 num_tokens/piece=45.1053\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=57 obj=17.2038 num_tokens=3754 num_tokens/piece=65.8596\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=57 obj=16.9853 num_tokens=3754 num_tokens/piece=65.8596\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=44 obj=18.1039 num_tokens=4054 num_tokens/piece=92.1364\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=44 obj=17.7491 num_tokens=4054 num_tokens/piece=92.1364\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: unigram/unigram40.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: unigram/unigram40.vocab\n",
            "skipped 0 empty lines\n",
            "filtered 0 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0ANEf1z3jev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}