{"metadata":{"colab":{"provenance":[],"collapsed_sections":["lrOIKAfWsW9R","6LDIcdKAwaHC","N_Y-Polmt8de","PdZn0dVwu-dF","cgj1aC53vk09","EB1W0yKNwuDQ","pqTbNgn_wUkN","twaDpetGxIYG","IEGOCDxVe4qB","exinCbGXgAw4","Oj8NjiA_f8Ny","bM0t1pzbf9nX","Y-xkRUYSfc5x","CRX9rtXHfd2l","azqaGmtUj3Dw","gPL6F3hminyQ","2pSgOj6Liukd","7qQLfrWejOjd","FiwOPaYzkiE7","MwoN53F0kkFq","s9rqrInBks8b","JDY4Nxr7dbzg","YvlnuzwKl8ya","JAA0ah8jnioL","apmMcHctoPrh","iE1DYbe_pAeg","dQAWTi24pfBa","Cf9JstLvmyhc","D6w-KSvPtOO3","nnas4FCNuK-3","w-Y9I92RqrVT","P4_0WJ1RrLa0","6-ciO8kw0Wda","3nQEmdFqNcjT","5bvueiRMObJw","gZ980wJWQULi","0nHmNDnWQxcu","9KU4cv09Qrkz","kJJTggAkS5Pe","TXA40fu0zMBS","IzIaMo14x8me"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8dcde6792a8f4c4a8cd670d6ec7780d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f327c20b68d428183a1b0972227a30d","IPY_MODEL_39183003182d4e71860eb2c4ba49a434","IPY_MODEL_aeaebc80b25d435ba16ed85926821cdb"],"layout":"IPY_MODEL_06c169d08fe94e94a3042c91c50621ad"}},"5f327c20b68d428183a1b0972227a30d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b202dc6a3724c91a32ca06d57ff9233","placeholder":"​","style":"IPY_MODEL_48c99d692d0a4e4e91d939f2fdd45a26","value":"Epoch 5:   0%"}},"39183003182d4e71860eb2c4ba49a434":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e70054be0c2e43dc97553a87b5c501e3","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d789f2de1e7f46438cbe4842cee6f263","value":0}},"aeaebc80b25d435ba16ed85926821cdb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e89a251e9404c1ea940739de254af48","placeholder":"​","style":"IPY_MODEL_9dde1c06c1094dcd8dc9fd410c05078a","value":" 0/6 [00:00&lt;?, ?it/s, v_num=1]"}},"06c169d08fe94e94a3042c91c50621ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"6b202dc6a3724c91a32ca06d57ff9233":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48c99d692d0a4e4e91d939f2fdd45a26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e70054be0c2e43dc97553a87b5c501e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d789f2de1e7f46438cbe4842cee6f263":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e89a251e9404c1ea940739de254af48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dde1c06c1094dcd8dc9fd410c05078a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c20d9f403b594e499f0d61bf5c797204":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f10cf814b9ff4b4399a4d8e04e3f37a5","IPY_MODEL_955e583c27ee453b997a7a361d2e152b","IPY_MODEL_70442ddd990543509a116a14e3736e1c"],"layout":"IPY_MODEL_e9edfcca7eac4418892ae2b45e642f86"}},"f10cf814b9ff4b4399a4d8e04e3f37a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7d63b324b694d00853db7f8b9ee090a","placeholder":"​","style":"IPY_MODEL_cca092384bbc4b63ae36f0060daed41f","value":"Validation DataLoader 0: 100%"}},"955e583c27ee453b997a7a361d2e152b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_99199c0d46b043d0bcf750e9049af880","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9c9e52b487d47f0bf679e96a6c4b99b","value":6}},"70442ddd990543509a116a14e3736e1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_137b09249057418babd6a3f1a8ab4845","placeholder":"​","style":"IPY_MODEL_ec64c45351b541dfa806c25339544c4b","value":" 6/6 [00:09&lt;00:00,  0.64it/s]"}},"e9edfcca7eac4418892ae2b45e642f86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"d7d63b324b694d00853db7f8b9ee090a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cca092384bbc4b63ae36f0060daed41f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99199c0d46b043d0bcf750e9049af880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9c9e52b487d47f0bf679e96a6c4b99b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"137b09249057418babd6a3f1a8ab4845":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec64c45351b541dfa806c25339544c4b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37be8de018694624a41cbc5752393675":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1dc2f7029df64bdbb3f5a435f6566dbb","IPY_MODEL_c74ba06bfa5c4e2db6f6c394c7b101ef","IPY_MODEL_590d0226ec6748c993cf678a5ace7fb3"],"layout":"IPY_MODEL_6f59abc2c3674181ae6b228b03023bdd"}},"1dc2f7029df64bdbb3f5a435f6566dbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bbab8d198364b0cb1b8b06540cc26a2","placeholder":"​","style":"IPY_MODEL_4420d700dc5842a8bd1416ae84eb9dd9","value":"Validation DataLoader 0: 100%"}},"c74ba06bfa5c4e2db6f6c394c7b101ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc34a518cf6246a1855f69d975a8db2e","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11363e68ee204a209b8eec19aade6da1","value":6}},"590d0226ec6748c993cf678a5ace7fb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed32ae1fcb3e424f8d1e4255e1de4f2f","placeholder":"​","style":"IPY_MODEL_02e357894b614c17a1e86d8b08805b25","value":" 6/6 [00:09&lt;00:00,  0.61it/s]"}},"6f59abc2c3674181ae6b228b03023bdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"7bbab8d198364b0cb1b8b06540cc26a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4420d700dc5842a8bd1416ae84eb9dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc34a518cf6246a1855f69d975a8db2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11363e68ee204a209b8eec19aade6da1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed32ae1fcb3e424f8d1e4255e1de4f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02e357894b614c17a1e86d8b08805b25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b598d402f874dea94eb2bdd4142489b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_071ed9bc7b97418cbea3a26cd2f89613","IPY_MODEL_518610cd11344fd1be914624fb59169a","IPY_MODEL_d1e0a6c553fd432da3d1c5d60b7444d3"],"layout":"IPY_MODEL_3b23a41bddfe402cab70a5dfab848d1b"}},"071ed9bc7b97418cbea3a26cd2f89613":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3b14874d4f64c1b916f6913b8351d94","placeholder":"​","style":"IPY_MODEL_780d3a53c02440c291d614ed4c24af75","value":"Validation DataLoader 0: 100%"}},"518610cd11344fd1be914624fb59169a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ce819ef0dc6480a8c5aeee9b85c0780","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec5d9a598cee4c9389f10d1711cda6a8","value":6}},"d1e0a6c553fd432da3d1c5d60b7444d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ffc251c1545403b94814ca87ceffb7c","placeholder":"​","style":"IPY_MODEL_90cafacd21894e7f8fa3b0d38b4c0ba2","value":" 6/6 [00:09&lt;00:00,  0.63it/s]"}},"3b23a41bddfe402cab70a5dfab848d1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"e3b14874d4f64c1b916f6913b8351d94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"780d3a53c02440c291d614ed4c24af75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ce819ef0dc6480a8c5aeee9b85c0780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec5d9a598cee4c9389f10d1711cda6a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ffc251c1545403b94814ca87ceffb7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90cafacd21894e7f8fa3b0d38b4c0ba2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcaa5c8a699e421f9e5ab1d2aabebdac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35997b55fd4d4340a45bc760570790a9","IPY_MODEL_5c1b3d2439114a308dccd23084582663","IPY_MODEL_0a2338d3e1ca49b78679d18a825038ce"],"layout":"IPY_MODEL_27de67756c314679b39bf06444c60576"}},"35997b55fd4d4340a45bc760570790a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d1713d1a16641ee9fc03e5024c055be","placeholder":"​","style":"IPY_MODEL_559c645fccee45ee903d7418d81d6664","value":"Validation DataLoader 0: 100%"}},"5c1b3d2439114a308dccd23084582663":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_92f71c757ca1468c899232943deb02bf","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9f3e28d8b6941449014cb03a9ef0e02","value":6}},"0a2338d3e1ca49b78679d18a825038ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afd7b2402b7046b38b3d2c70478025ad","placeholder":"​","style":"IPY_MODEL_2ce15fc22d3240558aeb330d1e145ea4","value":" 6/6 [00:09&lt;00:00,  0.61it/s]"}},"27de67756c314679b39bf06444c60576":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"0d1713d1a16641ee9fc03e5024c055be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"559c645fccee45ee903d7418d81d6664":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92f71c757ca1468c899232943deb02bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f3e28d8b6941449014cb03a9ef0e02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"afd7b2402b7046b38b3d2c70478025ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ce15fc22d3240558aeb330d1e145ea4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b13f1db49b24474d9b4b24b2de734c2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80bbd68de74d45bcb3adadda464eb928","IPY_MODEL_c78b62b7ff064268a84bba448d94e12f","IPY_MODEL_45e4c228598b45c6a881f49491cff000"],"layout":"IPY_MODEL_35e92c8127844aa988cd1dff0b4a2653"}},"80bbd68de74d45bcb3adadda464eb928":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cffd4b5a463429e979fbbffaa91d881","placeholder":"​","style":"IPY_MODEL_ea5796b40e2048099f916917d3361579","value":"Validation DataLoader 0: 100%"}},"c78b62b7ff064268a84bba448d94e12f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e9b659ca9e74cc9b039e0e8886619dd","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99c79108dae8471996edf9730b952b61","value":6}},"45e4c228598b45c6a881f49491cff000":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6ca995486d84befb924a7bcfb877331","placeholder":"​","style":"IPY_MODEL_f9e950b700b2447a8d81050a9c8e490b","value":" 6/6 [00:09&lt;00:00,  0.61it/s]"}},"35e92c8127844aa988cd1dff0b4a2653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"6cffd4b5a463429e979fbbffaa91d881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea5796b40e2048099f916917d3361579":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e9b659ca9e74cc9b039e0e8886619dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99c79108dae8471996edf9730b952b61":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6ca995486d84befb924a7bcfb877331":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9e950b700b2447a8d81050a9c8e490b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8140204,"sourceType":"datasetVersion","datasetId":4785917},{"sourceId":31689,"sourceType":"modelInstanceVersion","modelInstanceId":26577},{"sourceId":32605,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":27295},{"sourceId":32630,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":27318}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/mpc001/auto_avsr","metadata":{"id":"T-nHfbP_HGZf","outputId":"cde08d2a-4947-4095-93a4-b1c27880b3f1","execution":{"iopub.status.busy":"2024-04-17T01:50:35.129550Z","iopub.execute_input":"2024-04-17T01:50:35.129898Z","iopub.status.idle":"2024-04-17T01:50:38.237589Z","shell.execute_reply.started":"2024-04-17T01:50:35.129865Z","shell.execute_reply":"2024-04-17T01:50:38.236459Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'auto_avsr'...\nremote: Enumerating objects: 272, done.\u001b[K\nremote: Counting objects: 100% (56/56), done.\u001b[K\nremote: Compressing objects: 100% (32/32), done.\u001b[K\nremote: Total 272 (delta 36), reused 25 (delta 24), pack-reused 216\u001b[K\nReceiving objects: 100% (272/272), 31.46 MiB | 28.13 MiB/s, done.\nResolving deltas: 100% (92/92), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install fairseq","metadata":{"id":"ZCZMfAdNISz2","outputId":"e098cd26-7f60-4da1-aa5b-8e98d0247dbb","execution":{"iopub.status.busy":"2024-04-17T01:50:38.243124Z","iopub.execute_input":"2024-04-17T01:50:38.243863Z","iopub.status.idle":"2024-04-17T01:50:52.132765Z","shell.execute_reply.started":"2024-04-17T01:50:38.243810Z","shell.execute_reply":"2024-04-17T01:50:52.131448Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: fairseq in /opt/conda/lib/python3.10/site-packages (0.12.2)\nRequirement already satisfied: cffi in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.16.0)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from fairseq) (3.0.8)\nRequirement already satisfied: hydra-core<1.1,>=1.0.7 in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.0.7)\nRequirement already satisfied: omegaconf<2.1 in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.0.6)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from fairseq) (2023.12.25)\nRequirement already satisfied: sacrebleu>=1.4.12 in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.4.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fairseq) (4.66.1)\nRequirement already satisfied: bitarray in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.9.2)\nRequirement already satisfied: torchaudio>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.26.4)\nRequirement already satisfied: antlr4-python3-runtime==4.8 in /opt/conda/lib/python3.10/site-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\nRequirement already satisfied: PyYAML>=5.1.* in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (6.0.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (4.9.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (2.8.2)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (5.2.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi->fairseq) (2.21)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->fairseq) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->fairseq) (1.3.0)\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# !git clone https://github.com/pytorch/fairseq\n# %cd 'fairseq'\n# !pip install --editable ./","metadata":{"id":"zYwM8B1YXYSa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-lightning","metadata":{"id":"qiIp4XKezxsC","outputId":"57446db8-d9f7-4944-cc0b-b576410823bb","execution":{"iopub.status.busy":"2024-04-17T01:50:52.134315Z","iopub.execute_input":"2024-04-17T01:50:52.134714Z","iopub.status.idle":"2024-04-17T01:51:06.278518Z","shell.execute_reply.started":"2024-04-17T01:50:52.134677Z","shell.execute_reply":"2024-04-17T01:51:06.277438Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.26.4)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (2.1.2)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (6.0.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.3.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.9.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.11.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch-lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"p46Ifkno2UgJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-lightning==1.5.10","metadata":{"id":"LEyaaRzj2UdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install jedi>=0.16\n!pip install setuptools>=65.5.1\n!pip install pytorch-lightning==1.5.10","metadata":{"id":"pXn1RJgOXSUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"9bdX-RJWhIZ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install pytorch-lightning==1.5.10\n#!pip install sentencepiece\n!pip install av\n!pip install gdown","metadata":{"id":"8ldscqWJfg82","outputId":"7e50826c-fddc-4fe2-e2ac-570a374b4b1d","execution":{"iopub.status.busy":"2024-04-17T01:51:06.280061Z","iopub.execute_input":"2024-04-17T01:51:06.280421Z","iopub.status.idle":"2024-04-17T01:51:35.827780Z","shell.execute_reply.started":"2024-04-17T01:51:06.280374Z","shell.execute_reply":"2024-04-17T01:51:35.826366Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nDownloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: av\nSuccessfully installed av-12.0.0\nCollecting gdown\n  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.1.0-py3-none-any.whl (17 kB)\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: gdown\nSuccessfully installed gdown-5.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install hydra-core --upgrade","metadata":{"id":"6VyyOoCBfj4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show hydra-core","metadata":{"id":"-grpGMgZbYdY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install comet_ml","metadata":{"execution":{"iopub.status.busy":"2024-04-17T01:51:35.831662Z","iopub.execute_input":"2024-04-17T01:51:35.832134Z","iopub.status.idle":"2024-04-17T01:51:53.010552Z","shell.execute_reply.started":"2024-04-17T01:51:35.832087Z","shell.execute_reply":"2024-04-17T01:51:53.009425Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting comet_ml\n  Downloading comet_ml-3.40.0-py3-none-any.whl.metadata (4.0 kB)\nCollecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (4.20.0)\nRequirement already satisfied: psutil>=5.6.3 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (5.9.3)\nCollecting python-box<7.0.0 (from comet_ml)\n  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\nRequirement already satisfied: requests-toolbelt>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (0.10.1)\nRequirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (2.31.0)\nCollecting semantic-version>=2.8.0 (from comet_ml)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: sentry-sdk>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.44.1)\nRequirement already satisfied: simplejson in /opt/conda/lib/python3.10/site-packages (from comet_ml) (3.19.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.16.0)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.26.18)\nCollecting websocket-client<1.4.0,>=0.55.0 (from comet_ml)\n  Downloading websocket_client-1.3.3-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: wrapt>=1.11.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.14.1)\nCollecting wurlitzer>=1.0.2 (from comet_ml)\n  Downloading wurlitzer-3.0.3-py3-none-any.whl.metadata (1.9 kB)\nCollecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n  Downloading dulwich-0.21.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: rich>=13.3.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (13.7.0)\nCollecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n  Downloading configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.16.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.3.2->comet_ml) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.3.2->comet_ml) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\nDownloading comet_ml-3.40.0-py3-none-any.whl (645 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.1/645.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading dulwich-0.21.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (514 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.7/514.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\nDownloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\nDownloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: everett, wurlitzer, websocket-client, semantic-version, python-box, dulwich, configobj, comet_ml\n  Attempting uninstall: websocket-client\n    Found existing installation: websocket-client 1.7.0\n    Uninstalling websocket-client-1.7.0:\n      Successfully uninstalled websocket-client-1.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed comet_ml-3.40.0 configobj-5.0.8 dulwich-0.21.7 everett-3.1.0 python-box-6.1.0 semantic-version-2.10.0 websocket-client-1.3.3 wurlitzer-3.0.3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Descarga modelo prentrenado y dataset LRS3","metadata":{"id":"lrOIKAfWsW9R"}},{"cell_type":"code","source":"import gdown","metadata":{"id":"ZGjZ9GFpPbcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdown.download(id='1G2-rEUNeGotJ9EtTIj0UzqbvCSbn6CJy', output='model.zip')","metadata":{"id":"tvQcEa4nxZJJ","outputId":"7daf7f3c-634e-4db2-e41b-e04b8ba4bc74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -o /content/model.zip -d /content/","metadata":{"id":"WaS2AywRyE_o","outputId":"0c995f09-158a-471c-ad23-60bacb3c5414"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"px1FHSdyxZF9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"KRTLdvQpxZDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdown.download(id='1mLAuCnK2y7zbmiHlAXMqPSF_ApGqfbAD', output='model.zip')","metadata":{"id":"JJKedkz0N-hW","outputId":"1d3d5892-007d-4b7b-a84b-9f9ae8d1c529"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /content/auto_avsr","metadata":{"id":"w0pXBmEBUW0V","outputId":"c52d752a-452a-4f00-f73e-577bb978baef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python demo.py  data.modality=['visual'] \\\n                pretrained_model_path=['/content/model.zip'] \\\n                file_path=['/content/clip.mp4']","metadata":{"id":"6b3zhh7oODmd","outputId":"f8d668c5-743d-4855-a45d-292d1567d207"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!","metadata":{"id":"bxr6h_wGUAh2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"KjQCh5vsscjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Librerias","metadata":{"id":"GUufWC2jsdEW"}},{"cell_type":"code","source":"import os\nimport hydra\nfrom omegaconf import OmegaConf, DictConfig\nimport logging\nimport comet_ml\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.nn.utils.rnn import pad_sequence\nimport torchaudio\nimport torchvision\nfrom torch.utils.data import Dataset, DistributedSampler, RandomSampler\nfrom torch.utils.data.sampler import Sampler\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning import LightningDataModule\nfrom pytorch_lightning import seed_everything, Trainer\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import CometLogger\n#from pytorch_lightning.plugins import DDPPlugin\n\nfrom fairseq.data import data_utils\n\nimport numpy as np\nimport math\nimport random\nimport sentencepiece\nfrom typing import Iterator, Optional\nfrom operator import itemgetter\nimport warnings\nfrom typing import Any, List, Tuple, Dict, NamedTuple, Union\nimport six\nfrom itertools import chain\nimport copy","metadata":{"id":"7-vB2bNLtIeK","execution":{"iopub.status.busy":"2024-04-17T01:54:22.466909Z","iopub.execute_input":"2024-04-17T01:54:22.467747Z","iopub.status.idle":"2024-04-17T01:55:12.951325Z","shell.execute_reply.started":"2024-04-17T01:54:22.467712Z","shell.execute_reply":"2024-04-17T01:55:12.950422Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-04-17 01:54:36.595576: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-17 01:54:36.595705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-17 01:54:36.726798: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# DataModule (Dataset)","metadata":{"id":"6LDIcdKAwaHC"}},{"cell_type":"markdown","source":"## AVDataset","metadata":{"id":"N_Y-Polmt8de"}},{"cell_type":"code","source":"def cut_or_pad(data, size, dim=0):\n    \"\"\"\n    Pads or trims the data along a dimension.\n    \"\"\"\n    if data.size(dim) < size:\n        padding = size - data.size(dim)\n        data = torch.nn.functional.pad(data, (0, 0, 0, padding), \"constant\")\n        size = data.size(dim)\n    elif data.size(dim) > size:\n        data = data[:size]\n    assert data.size(dim) == size\n    return data\n\n\ndef load_video(path):\n    \"\"\"\n    rtype: torch, T x C x H x W\n    \"\"\"\n    vid = torchvision.io.read_video(path, pts_unit=\"sec\", output_format=\"THWC\")[0]\n    vid = vid.permute((0, 3, 1, 2))\n    return vid\n\n\ndef load_audio(path):\n    \"\"\"\n    rtype: torch, T x 1\n    \"\"\"\n    waveform, sample_rate = torchaudio.load(path[:-4] + \".wav\", normalize=True)\n    return waveform.transpose(1, 0)\n\n\nclass AVDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        root_dir,\n        label_path,\n        subset,\n        modality,\n        audio_transform,\n        video_transform,\n        rate_ratio=640,\n    ):\n\n        self.root_dir = root_dir\n\n        self.modality = modality\n        self.rate_ratio = rate_ratio\n\n        self.list = self.load_list(label_path)\n\n        self.audio_transform = audio_transform\n        self.video_transform = video_transform\n\n    def load_list(self, label_path):\n        paths_counts_labels = []\n        for path_count_label in open(label_path).read().splitlines()[1:]:\n            id, dataset_name, rel_path, input_length, token_id = path_count_label.split(\",\")\n            #print(input_length)\n            paths_counts_labels.append(\n                (\n                    dataset_name,\n                    rel_path,\n                    int(input_length),\n                    torch.tensor([int(_) for _ in token_id.split()]),\n                )\n            )\n        return paths_counts_labels\n\n    def __getitem__(self, idx):\n        dataset_name, rel_path, input_length, token_id = self.list[idx]\n        path = os.path.join(self.root_dir, dataset_name, rel_path)\n        if self.modality == \"video\":\n            video = load_video(path)\n            video = self.video_transform(video)\n            return {\"input\": video, \"target\": token_id}\n        elif self.modality == \"audio\":\n            audio = load_audio(path)\n            audio = self.audio_transform(audio)\n            return {\"input\": audio, \"target\": token_id}\n        elif self.modality == \"audiovisual\":\n            video = load_video(path)\n            audio = load_audio(path)\n            audio = cut_or_pad(audio, len(video) * self.rate_ratio)\n            video = self.video_transform(video)\n            audio = self.audio_transform(audio)\n            return {\"video\": video, \"audio\": audio, \"target\": token_id}\n\n    def __len__(self):\n        return len(self.list)","metadata":{"id":"ZLIGNqVUt_i9","execution":{"iopub.status.busy":"2024-04-17T01:55:17.125549Z","iopub.execute_input":"2024-04-17T01:55:17.126677Z","iopub.status.idle":"2024-04-17T01:55:17.147228Z","shell.execute_reply.started":"2024-04-17T01:55:17.126638Z","shell.execute_reply":"2024-04-17T01:55:17.146164Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"ds_args = cfg.data.dataset","metadata":{"id":"DXoQ69DBlPVJ","execution":{"iopub.status.busy":"2024-04-16T18:50:57.053351Z","iopub.execute_input":"2024-04-16T18:50:57.053778Z","iopub.status.idle":"2024-04-16T18:50:57.431348Z","shell.execute_reply.started":"2024-04-16T18:50:57.053745Z","shell.execute_reply":"2024-04-16T18:50:57.429857Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_args \u001b[38;5;241m=\u001b[39m \u001b[43mcfg\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdataset\n","\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"],"ename":"NameError","evalue":"name 'cfg' is not defined","output_type":"error"}]},{"cell_type":"code","source":"train_ds = AVDataset(\n    root_dir=ds_args.root_dir,\n    label_path=os.path.join(\n        ds_args.root_dir, ds_args.label_dir, ds_args.train_file\n    ),\n    subset=\"train\",\n    modality=cfg.data.modality,\n    audio_transform=AudioTransform(\"train\"),\n    video_transform=VideoTransform(\"train\"),\n)","metadata":{"id":"1wf4V_nSlB4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_ds)","metadata":{"id":"daSs8nlinX11","outputId":"370b6117-6df1-47d2-a028-6b6002023cc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ejemplo0 = train_ds[3]","metadata":{"id":"zt6YoY8Elm9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ejemplo0['input'].shape, ejemplo0['target'].shape","metadata":{"id":"t-bBybBXmjiW","outputId":"d6bd70f0-bdb8-4446-b148-609c23064d63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ejemplo0['input'].device.type","metadata":{"id":"vY6dryF_oGiB","outputId":"2358969c-36e3-41a7-c143-d8bade48d660"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tranformaciones","metadata":{"id":"PdZn0dVwu-dF"}},{"cell_type":"code","source":"NOISE_FILENAME = os.path.join(\n    os.path.dirname(os.path.abspath('')),\n    \"working\",\n    \"auto_avsr\",\n    \"datamodule\",\n    \"babble_noise.wav\"\n)\n\nSP_MODEL_PATH = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n    \"kaggle\",\n    \"input\",\n    \"sentencepiece_castellano\",\n    \"other\",\n    \"v2\",\n    \"1\",\n    \"unigram100.model\",\n)\n\nDICT_PATH = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n    \"kaggle\",\n    \"input\",\n    \"sentencepiece_castellano\",\n    \"other\",\n    \"v2\",\n    \"1\",\n    \"unigram100_units.txt\",\n)\n\n\nclass FunctionalModule(torch.nn.Module):\n    def __init__(self, functional):\n        super().__init__()\n        self.functional = functional\n\n    def forward(self, input):\n        return self.functional(input)\n\n\nclass AdaptiveTimeMask(torch.nn.Module):\n    def __init__(self, window, stride):\n        super().__init__()\n        self.window = window\n        self.stride = stride\n\n    def forward(self, x):\n        # x: [T, ...]\n        cloned = x.clone()\n        length = cloned.size(0)\n        n_mask = int((length + self.stride - 0.1) // self.stride)\n        ts = torch.randint(0, self.window, size=(n_mask, 2))\n        for t, t_end in ts:\n            if length - t <= 0:\n                continue\n            t_start = random.randrange(0, length - t)\n            if t_start == t_start + t:\n                continue\n            t_end += t_start\n            cloned[t_start:t_end] = 0\n        return cloned\n\n\nclass AddNoise(torch.nn.Module):\n    def __init__(\n        self,\n        noise_filename=NOISE_FILENAME,\n        snr_target=None,\n    ):\n        super().__init__()\n        self.snr_levels = [snr_target] if snr_target else [-5, 0, 5, 10, 15, 20, 999999]\n        self.noise, sample_rate = torchaudio.load(noise_filename)\n        assert sample_rate == 16000\n\n    def forward(self, speech):\n        # speech: T x 1\n        # return: T x 1\n        speech = speech.t()\n        start_idx = random.randint(0, self.noise.shape[1] - speech.shape[1])\n        noise_segment = self.noise[:, start_idx : start_idx + speech.shape[1]]\n        snr_level = torch.tensor([random.choice(self.snr_levels)])\n        noisy_speech = torchaudio.functional.add_noise(speech, noise_segment, snr_level)\n        return noisy_speech.t()\n\n\nclass VideoTransform:\n    def __init__(self, subset):\n        if subset == \"train\":\n            self.video_pipeline = torch.nn.Sequential(\n                FunctionalModule(lambda x: x / 255.0),\n                torchvision.transforms.RandomCrop(88),\n                torchvision.transforms.Grayscale(),\n                AdaptiveTimeMask(10, 25),\n                torchvision.transforms.Normalize(0.421, 0.165),\n            )\n        elif subset == \"val\" or subset == \"test\":\n            self.video_pipeline = torch.nn.Sequential(\n                FunctionalModule(lambda x: x / 255.0),\n                torchvision.transforms.CenterCrop(88),\n                torchvision.transforms.Grayscale(),\n                torchvision.transforms.Normalize(0.421, 0.165),\n            )\n\n    def __call__(self, sample):\n        # sample: T x C x H x W\n        # rtype: T x 1 x H x W\n        return self.video_pipeline(sample)\n\n\nclass AudioTransform:\n    def __init__(self, subset, snr_target=None):\n        if subset == \"train\":\n            self.audio_pipeline = torch.nn.Sequential(\n                AdaptiveTimeMask(6400, 16000),\n                AddNoise(),\n                FunctionalModule(\n                    lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=1e-8)\n                ),\n            )\n        elif subset == \"val\" or subset == \"test\":\n            self.audio_pipeline = torch.nn.Sequential(\n                AddNoise(snr_target=snr_target)\n                if snr_target is not None\n                else FunctionalModule(lambda x: x),\n                FunctionalModule(\n                    lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=1e-8)\n                ),\n            )\n\n    def __call__(self, sample):\n        # sample: T x 1\n        # rtype: T x 1\n        return self.audio_pipeline(sample)\n\n\nclass TextTransform:\n    \"\"\"Mapping Dictionary Class for SentencePiece tokenization.\"\"\"\n\n    def __init__(\n        self,\n        sp_model_path=SP_MODEL_PATH,\n        dict_path=DICT_PATH,\n    ):\n\n        # Load SentencePiece model\n        self.spm = sentencepiece.SentencePieceProcessor(model_file=sp_model_path)\n\n        # Load units and create dictionary\n        units = open(dict_path, encoding='utf8').read().splitlines()\n        self.hashmap = {unit.split()[0]: unit.split()[-1] for unit in units}\n        # 0 will be used for \"blank\" in CTC\n        self.token_list = [\"<blank>\"] + list(self.hashmap.keys()) + [\"<eos>\"]\n        self.ignore_id = -1\n\n    def tokenize(self, text):\n        tokens = self.spm.EncodeAsPieces(text)\n        token_ids = [self.hashmap.get(token, self.hashmap[\"<unk>\"]) for token in tokens]\n        return torch.tensor(list(map(int, token_ids)))\n\n    def post_process(self, token_ids):\n        token_ids = token_ids[token_ids != -1]\n        text = self._ids_to_str(token_ids, self.token_list)\n        text = text.replace(\"\\u2581\", \" \").strip()\n        return text\n\n    def _ids_to_str(self, token_ids, char_list):\n        token_as_list = [char_list[idx] for idx in token_ids]\n        return \"\".join(token_as_list).replace(\"<space>\", \" \")","metadata":{"id":"C_o27zBmvAek","execution":{"iopub.status.busy":"2024-04-17T01:55:35.407820Z","iopub.execute_input":"2024-04-17T01:55:35.408243Z","iopub.status.idle":"2024-04-17T01:55:35.439007Z","shell.execute_reply.started":"2024-04-17T01:55:35.408211Z","shell.execute_reply":"2024-04-17T01:55:35.437751Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"EJqbLHgyvkRs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ByFrameCountSampler","metadata":{"id":"cgj1aC53vk09"}},{"cell_type":"code","source":"class ByFrameCountSampler(Sampler):\n    def __init__(self, dataset, max_frames_per_gpu, shuffle=True, seed=0):\n        self.dataset = dataset\n        self.max_frames_per_gpu = max_frames_per_gpu\n        self.sizes = [item[2] for item in self.dataset.list]\n\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        batch_indices = data_utils.batch_by_size(\n            self._get_indices(), lambda i: self.sizes[i], max_tokens=max_frames_per_gpu\n        )\n        self.num_batches = len(batch_indices)\n\n    def _get_indices(self):\n        if self.shuffle:  # shuffles indices corresponding to equal lengths\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            order = [torch.randperm(len(self.dataset), generator=g).tolist()]\n        else:\n            order = [list(range(len(self.dataset)))]\n        order.append(self.sizes)\n        return np.lexsort(order)[::-1]\n\n    def __len__(self):\n        return self.num_batches\n\n    def __iter__(self):\n        batch_indices = data_utils.batch_by_size(\n            self._get_indices(),\n            lambda i: self.sizes[i],\n            max_tokens=self.max_frames_per_gpu,\n        )\n        return iter(batch_indices)\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch","metadata":{"id":"zq2myP33vlTg","execution":{"iopub.status.busy":"2024-04-17T01:55:41.137159Z","iopub.execute_input":"2024-04-17T01:55:41.137589Z","iopub.status.idle":"2024-04-17T01:55:41.149462Z","shell.execute_reply.started":"2024-04-17T01:55:41.137547Z","shell.execute_reply":"2024-04-17T01:55:41.148284Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## DatasetFromSampler","metadata":{"id":"EB1W0yKNwuDQ"}},{"cell_type":"code","source":"class DatasetFromSampler(Dataset):\n    \"\"\"Dataset to create indexes from `Sampler`.\n    Args:\n        sampler: PyTorch sampler\n    \"\"\"\n\n    def __init__(self, sampler: Sampler):\n        \"\"\"Initialisation for DatasetFromSampler.\"\"\"\n        self.sampler = sampler\n        self.sampler_list = None\n\n    def __getitem__(self, index: int):\n        \"\"\"Gets element of the dataset.\n        Args:\n            index: index of the element in the dataset\n        Returns:\n            Single element by index\n        \"\"\"\n        if self.sampler_list is None:\n            self.sampler_list = list(self.sampler)\n        return self.sampler_list[index]\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns:\n            int: length of the dataset\n        \"\"\"\n        return len(self.sampler)\n","metadata":{"id":"Mwmb5zDLwu_e","execution":{"iopub.status.busy":"2024-04-17T01:55:42.763654Z","iopub.execute_input":"2024-04-17T01:55:42.764057Z","iopub.status.idle":"2024-04-17T01:55:42.773692Z","shell.execute_reply.started":"2024-04-17T01:55:42.764025Z","shell.execute_reply":"2024-04-17T01:55:42.772777Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## DistributedSamplerWrapper","metadata":{"id":"pqTbNgn_wUkN"}},{"cell_type":"code","source":"class DistributedSamplerWrapper(DistributedSampler):\n    \"\"\"\n    Wrapper over `Sampler` for distributed training.\n    Allows you to use any sampler in distributed mode.\n    It is especially useful in conjunction with\n    `torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSamplerWrapper instance as a DataLoader\n    sampler, and load a subset of subsampled data of the original dataset\n    that is exclusive to it.\n    .. note::\n        Sampler is assumed to be of constant size.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler,\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = True,\n        drop_last: bool = False,\n    ):\n        \"\"\"\n        Args:\n            sampler: Sampler used for subsampling\n            num_replicas (int, optional): Number of processes participating in\n                distributed training\n            rank (int, optional): Rank of the current process\n                within ``num_replicas``\n            shuffle (bool, optional): If true (default),\n                sampler will shuffle the indices\n        \"\"\"\n        super(DistributedSamplerWrapper, self).__init__(\n            DatasetFromSampler(sampler),\n            num_replicas=num_replicas,\n            rank=rank,\n            shuffle=shuffle,\n            drop_last=drop_last,\n        )\n        self.sampler = sampler\n\n    def __iter__(self) -> Iterator[int]:\n        \"\"\"Iterate over sampler.\n        Returns:\n            python iterator\n        \"\"\"\n        self.dataset = DatasetFromSampler(self.sampler)\n        indexes_of_indexes = super().__iter__()\n\n        subsampler_indexes = self.dataset\n        return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n        self.sampler.set_epoch(epoch)","metadata":{"id":"IyT2ML3IwTx7","execution":{"iopub.status.busy":"2024-04-17T01:55:44.284896Z","iopub.execute_input":"2024-04-17T01:55:44.285316Z","iopub.status.idle":"2024-04-17T01:55:44.295885Z","shell.execute_reply.started":"2024-04-17T01:55:44.285282Z","shell.execute_reply":"2024-04-17T01:55:44.294589Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## RandomSamplerWrapper","metadata":{"id":"twaDpetGxIYG"}},{"cell_type":"code","source":"class RandomSamplerWrapper(RandomSampler):\n    def __init__(self, sampler):\n        super(RandomSamplerWrapper, self).__init__(DatasetFromSampler(sampler))\n        self.sampler = sampler\n\n    def __iter__(self) -> Iterator[int]:\n        \"\"\"Iterate over sampler.\n        Returns:\n            python iterator\n        \"\"\"\n        self.dataset = DatasetFromSampler(self.sampler)\n        indexes_of_indexes = super().__iter__()\n        subsampler_indexes = self.dataset\n        return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))","metadata":{"id":"roBfR_dCxJbd","execution":{"iopub.status.busy":"2024-04-17T01:55:45.665192Z","iopub.execute_input":"2024-04-17T01:55:45.665950Z","iopub.status.idle":"2024-04-17T01:55:45.672649Z","shell.execute_reply.started":"2024-04-17T01:55:45.665915Z","shell.execute_reply":"2024-04-17T01:55:45.671617Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## DataModule","metadata":{"id":"lTz0BtKMwMdz"}},{"cell_type":"code","source":"def pad(samples, pad_val=0.0):\n    lengths = [len(s) for s in samples]\n    max_size = max(lengths)\n    sample_shape = list(samples[0].shape[1:])\n    collated_batch = samples[0].new_zeros([len(samples), max_size] + sample_shape)\n    for i, sample in enumerate(samples):\n        diff = len(sample) - max_size\n        if diff == 0:\n            collated_batch[i] = sample\n        else:\n            collated_batch[i] = torch.cat(\n                [sample, sample.new_full([-diff] + sample_shape, pad_val)]\n            )\n    if len(samples[0].shape) == 1:\n        collated_batch = collated_batch.unsqueeze(1)  # targets\n    elif len(samples[0].shape) == 2:\n        pass  # collated_batch: [B, T, 1]\n    elif len(samples[0].shape) == 4:\n        pass  # collated_batch: [B, T, C, H, W]\n    return collated_batch, lengths\n\ndef collate_pad(batch):\n    batch_out = {}\n    for data_type in batch[0].keys():\n        pad_val = -1 if data_type == \"target\" else 0.0\n        c_batch, sample_lengths = pad(\n            [s[data_type] for s in batch if s[data_type] is not None], pad_val\n        )\n        batch_out[data_type + \"s\"] = c_batch\n        batch_out[data_type + \"_lengths\"] = torch.tensor(sample_lengths)\n    return batch_out\n\n\nclass DataModule(LightningDataModule):\n    def __init__(self, cfg=None):\n        super().__init__()\n        self.cfg = cfg\n        self.cfg.gpus = torch.cuda.device_count()\n        self.total_gpus = self.cfg.gpus * self.cfg.trainer.num_nodes\n        print('Numero de gpus',self.total_gpus)\n\n    def _dataloader(self, ds, sampler, collate_fn):\n        return torch.utils.data.DataLoader(\n            ds,\n            num_workers=0,\n            pin_memory=True,\n            batch_sampler=sampler,\n            collate_fn=collate_fn,\n        )\n\n    def train_dataloader(self):\n        ds_args = self.cfg.data.dataset\n        train_ds = AVDataset(\n            root_dir=ds_args.root_dir,\n            label_path=os.path.join(\n                ds_args.root_dir, ds_args.label_dir, ds_args.train_file\n            ),\n            subset=\"train\",\n            modality=self.cfg.data.modality,\n            audio_transform=AudioTransform(\"train\"),\n            video_transform=VideoTransform(\"train\"),\n        )\n        print(f'num_train:{len(train_ds)}')\n        sampler = ByFrameCountSampler(train_ds, self.cfg.data.max_frames)\n        if self.total_gpus > 1:\n            sampler = DistributedSamplerWrapper(sampler)\n        else:\n            sampler = RandomSamplerWrapper(sampler)\n        return self._dataloader(train_ds, sampler, collate_pad)\n\n    def val_dataloader(self):\n        ds_args = self.cfg.data.dataset\n        val_ds = AVDataset(\n            root_dir=ds_args.root_dir,\n            label_path=os.path.join(\n                ds_args.root_dir, ds_args.label_dir, ds_args.val_file\n            ),\n            subset=\"val\",\n            modality=self.cfg.data.modality,\n            audio_transform=AudioTransform(\"val\"),\n            video_transform=VideoTransform(\"val\"),\n        )\n        print(f'num_val:{len(val_ds)}')\n        sampler = ByFrameCountSampler(\n            val_ds, self.cfg.data.max_frames_val, shuffle=False\n        )\n        if self.total_gpus > 1:\n            sampler = DistributedSamplerWrapper(sampler, shuffle=False, drop_last=True)\n        return self._dataloader(val_ds, sampler, collate_pad)\n\n    def test_dataloader(self):\n        ds_args = self.cfg.data.dataset\n        dataset = AVDataset(\n            root_dir=ds_args.root_dir,\n            label_path=os.path.join(\n                ds_args.root_dir, ds_args.label_dir, ds_args.test_file\n            ),\n            subset=\"test\",\n            modality=self.cfg.data.modality,\n            audio_transform=AudioTransform(\n                \"test\", snr_target=self.cfg.decode.snr_target\n            ),\n            video_transform=VideoTransform(\"test\"),\n        )\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=None)\n        return dataloader","metadata":{"id":"F2mb_uPzs-g1","execution":{"iopub.status.busy":"2024-04-17T01:55:47.157420Z","iopub.execute_input":"2024-04-17T01:55:47.157820Z","iopub.status.idle":"2024-04-17T01:55:47.180624Z","shell.execute_reply.started":"2024-04-17T01:55:47.157788Z","shell.execute_reply":"2024-04-17T01:55:47.179442Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"datamodulo = DataModule(cfg)","metadata":{"id":"xURoMdQkAacR","outputId":"734086d3-08c8-4856-9b57-147c7692afa2","execution":{"iopub.status.busy":"2024-04-16T18:51:09.580070Z","iopub.execute_input":"2024-04-16T18:51:09.581017Z","iopub.status.idle":"2024-04-16T18:51:09.637952Z","shell.execute_reply.started":"2024-04-16T18:51:09.580981Z","shell.execute_reply":"2024-04-16T18:51:09.636612Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datamodulo \u001b[38;5;241m=\u001b[39m DataModule(\u001b[43mcfg\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"],"ename":"NameError","evalue":"name 'cfg' is not defined","output_type":"error"}]},{"cell_type":"code","source":"dataloader = datamodulo.train_dataloader()","metadata":{"id":"_817J-v0AxY7","outputId":"8532241e-c836-4f0b-a649-1ad8cc513c6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(dataloader))\ninputs = batch['inputs']\ninput_lengths = batch['input_lengths']\ntargets = batch['targets']\ntarget_lengths = batch['target_lengths']","metadata":{"id":"oNP-SJ27BItd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(inputs),type(input_lengths), type(targets), type(target_lengths)","metadata":{"id":"8KBrjACsBo5b","outputId":"c8805d4a-3d4b-4074-b7de-a745a91bef0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(inputs.shape,\ninput_lengths.shape,\ntargets.shape,\ntarget_lengths.shape)","metadata":{"id":"KnXCe6mLCaqJ","outputId":"9675007b-882f-4820-c70f-04b03124be40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ModelModule (Modelo)","metadata":{"id":"SDhdr-AvzGi6"}},{"cell_type":"markdown","source":"## RelPositionalEncoding","metadata":{"id":"IEGOCDxVe4qB"}},{"cell_type":"code","source":"class RelPositionalEncoding(torch.nn.Module):\n    \"\"\"Relative positional encoding module (new implementation).\n    Details can be found in https://github.com/espnet/espnet/pull/2816.\n    See : Appendix B in https://arxiv.org/abs/1901.02860\n    Args:\n        d_model (int): Embedding dimension.\n        dropout_rate (float): Dropout rate.\n        max_len (int): Maximum input length.\n    \"\"\"\n\n    def __init__(self, d_model, dropout_rate, max_len=5000):\n        \"\"\"Construct an PositionalEncoding object.\"\"\"\n        super(RelPositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.xscale = math.sqrt(self.d_model)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.pe = None\n        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n\n    def extend_pe(self, x):\n        \"\"\"Reset the positional encodings.\"\"\"\n        if self.pe is not None:\n            # self.pe contains both positive and negative parts\n            # the length of self.pe is 2 * input_len - 1\n            if self.pe.size(1) >= x.size(1) * 2 - 1:\n                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n                return\n        # Suppose `i` means to the position of query vecotr and `j` means the\n        # position of key vector. We use position relative positions when keys\n        # are to the left (i>j) and negative relative positions otherwise (i<j).\n        pe_positive = torch.zeros(x.size(1), self.d_model)\n        pe_negative = torch.zeros(x.size(1), self.d_model)\n        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n            * -(math.log(10000.0) / self.d_model)\n        )\n        pe_positive[:, 0::2] = torch.sin(position * div_term)\n        pe_positive[:, 1::2] = torch.cos(position * div_term)\n        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)\n        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)\n\n        # Reserve the order of positive indices and concat both positive and\n        # negative indices. This is used to support the shifting trick\n        # as in https://arxiv.org/abs/1901.02860\n        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n        pe_negative = pe_negative[1:].unsqueeze(0)\n        pe = torch.cat([pe_positive, pe_negative], dim=1)\n        self.pe = pe.to(device=x.device, dtype=x.dtype)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Add positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n        \"\"\"\n        self.extend_pe(x)\n        x = x * self.xscale\n        pos_emb = self.pe[\n            :,\n            self.pe.size(1) // 2 - x.size(1) + 1 : self.pe.size(1) // 2 + x.size(1),\n        ]\n        return self.dropout(x), self.dropout(pos_emb)","metadata":{"id":"whTGzHZNe51i","execution":{"iopub.status.busy":"2024-04-17T01:55:53.054105Z","iopub.execute_input":"2024-04-17T01:55:53.054516Z","iopub.status.idle":"2024-04-17T01:55:53.071701Z","shell.execute_reply.started":"2024-04-17T01:55:53.054471Z","shell.execute_reply":"2024-04-17T01:55:53.070538Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Swish","metadata":{"id":"exinCbGXgAw4"}},{"cell_type":"code","source":"class Swish(nn.Module):\n    \"\"\"Construct an Swish object.\"\"\"\n\n    def forward(self, x):\n        \"\"\"Return Swich activation function.\"\"\"\n        return x * torch.sigmoid(x)","metadata":{"id":"ZSQU5guKfWUo","execution":{"iopub.status.busy":"2024-04-17T01:55:54.583778Z","iopub.execute_input":"2024-04-17T01:55:54.584200Z","iopub.status.idle":"2024-04-17T01:55:54.589373Z","shell.execute_reply.started":"2024-04-17T01:55:54.584167Z","shell.execute_reply":"2024-04-17T01:55:54.588315Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## ResNet 2D","metadata":{"id":"Oj8NjiA_f8Ny"}},{"cell_type":"code","source":"def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"conv3x3.\n\n    :param in_planes: int, number of channels in the input sequence.\n    :param out_planes: int,  number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False,\n    )\n\n\ndef downsample_basic_block(inplanes, outplanes, stride):\n    \"\"\"downsample_basic_block.\n\n    :param inplanes: int, number of channels in the input sequence.\n    :param outplanes: int, number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(\n            inplanes,\n            outplanes,\n            kernel_size=1,\n            stride=stride,\n            bias=False,\n        ),\n        nn.BatchNorm2d(outplanes),\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        downsample=None,\n        relu_type=\"swish\",\n    ):\n        \"\"\"__init__.\n\n        :param inplanes: int, number of channels in the input sequence.\n        :param planes: int,  number of channels produced by the convolution.\n        :param stride: int, size of the convolving kernel.\n        :param downsample: boolean, if True, the temporal resolution is downsampled.\n        :param relu_type: str, type of activation function.\n        \"\"\"\n        super(BasicBlock, self).__init__()\n\n        assert relu_type in [\"relu\", \"prelu\", \"swish\"]\n\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        if relu_type == \"relu\":\n            self.relu1 = nn.ReLU(inplace=True)\n            self.relu2 = nn.ReLU(inplace=True)\n        elif relu_type == \"prelu\":\n            self.relu1 = nn.PReLU(num_parameters=planes)\n            self.relu2 = nn.PReLU(num_parameters=planes)\n        elif relu_type == \"swish\":\n            self.relu1 = Swish()\n            self.relu2 = Swish()\n        else:\n            raise NotImplementedError\n        # --------\n\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T, H, W).\n        \"\"\"\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu2(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        block,\n        layers,\n        relu_type=\"swish\",\n    ):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.relu_type = relu_type\n        self.downsample_block = downsample_basic_block\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        \"\"\"_make_layer.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param planes: int,  number of channels produced by the convolution.\n        :param blocks: int, number of layers in a block.\n        :param stride: int, size of the convolving kernel.\n        \"\"\"\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = self.downsample_block(\n                inplanes=self.inplanes,\n                outplanes=planes * block.expansion,\n                stride=stride,\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.inplanes,\n                planes,\n                stride,\n                downsample,\n                relu_type=self.relu_type,\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    relu_type=self.relu_type,\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T, H, W).\n        \"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        return x","metadata":{"id":"E3WrvoRKf9U3","execution":{"iopub.status.busy":"2024-04-17T01:55:56.139367Z","iopub.execute_input":"2024-04-17T01:55:56.140180Z","iopub.status.idle":"2024-04-17T01:55:56.165801Z","shell.execute_reply.started":"2024-04-17T01:55:56.140143Z","shell.execute_reply":"2024-04-17T01:55:56.164870Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## ResNet 1D","metadata":{"id":"bM0t1pzbf9nX"}},{"cell_type":"code","source":"def conv3x31D(in_planes, out_planes, stride=1):\n    \"\"\"conv3x3.\n\n    :param in_planes: int, number of channels in the input sequence.\n    :param out_planes: int,  number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"\n    return nn.Conv1d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False,\n    )\n\n\ndef downsample_basic_block1D(inplanes, outplanes, stride):\n    \"\"\"downsample_basic_block.\n\n    :param inplanes: int, number of channels in the input sequence.\n    :param outplanes: int, number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv1d(\n            inplanes,\n            outplanes,\n            kernel_size=1,\n            stride=stride,\n            bias=False,\n        ),\n        nn.BatchNorm1d(outplanes),\n    )\n\n\nclass BasicBlock1D(nn.Module):\n    expansion = 1\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        downsample=None,\n        relu_type=\"relu\",\n    ):\n        \"\"\"__init__.\n\n        :param inplanes: int, number of channels in the input sequence.\n        :param planes: int,  number of channels produced by the convolution.\n        :param stride: int, size of the convolving kernel.\n        :param downsample: boolean, if True, the temporal resolution is downsampled.\n        :param relu_type: str, type of activation function.\n        \"\"\"\n        super(BasicBlock1D, self).__init__()\n\n        assert relu_type in [\"relu\", \"prelu\", \"swish\"]\n\n        self.conv1 = conv3x31D(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm1d(planes)\n\n        # type of ReLU is an input option\n        if relu_type == \"relu\":\n            self.relu1 = nn.ReLU(inplace=True)\n            self.relu2 = nn.ReLU(inplace=True)\n        elif relu_type == \"prelu\":\n            self.relu1 = nn.PReLU(num_parameters=planes)\n            self.relu2 = nn.PReLU(num_parameters=planes)\n        elif relu_type == \"swish\":\n            self.relu1 = Swish()\n            self.relu2 = Swish()\n        else:\n            raise NotImplementedError\n        # --------\n\n        self.conv2 = conv3x31D(planes, planes)\n        self.bn2 = nn.BatchNorm1d(planes)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T)\n        \"\"\"\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu2(out)\n\n        return out\n\n\nclass ResNet1D(nn.Module):\n    def __init__(\n        self,\n        block,\n        layers,\n        relu_type=\"swish\",\n        a_upsample_ratio=1,\n    ):\n        \"\"\"__init__.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param layers: List, customised layers in each block.\n        :param relu_type: str, type of activation function.\n        :param a_upsample_ratio: int, The ratio related to the \\\n            temporal resolution of output features of the frontend. \\\n            a_upsample_ratio=1 produce features with a fps of 25.\n        \"\"\"\n        super(ResNet1D, self).__init__()\n        self.inplanes = 64\n        self.relu_type = relu_type\n        self.downsample_block = downsample_basic_block1D\n        self.a_upsample_ratio = a_upsample_ratio\n\n        self.conv1 = nn.Conv1d(\n            in_channels=1,\n            out_channels=self.inplanes,\n            kernel_size=80,\n            stride=4,\n            padding=38,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm1d(self.inplanes)\n\n        if relu_type == \"relu\":\n            self.relu = nn.ReLU(inplace=True)\n        elif relu_type == \"prelu\":\n            self.relu = nn.PReLU(num_parameters=self.inplanes)\n        elif relu_type == \"swish\":\n            self.relu = Swish()\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool1d(\n            kernel_size=20 // self.a_upsample_ratio,\n            stride=20 // self.a_upsample_ratio,\n        )\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        \"\"\"_make_layer.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param planes: int,  number of channels produced by the convolution.\n        :param blocks: int, number of layers in a block.\n        :param stride: int, size of the convolving kernel.\n        \"\"\"\n\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = self.downsample_block(\n                inplanes=self.inplanes,\n                outplanes=planes * block.expansion,\n                stride=stride,\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.inplanes,\n                planes,\n                stride,\n                downsample,\n                relu_type=self.relu_type,\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    relu_type=self.relu_type,\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        return x","metadata":{"id":"6-OF93Qpf_ZO","execution":{"iopub.status.busy":"2024-04-17T01:55:57.622499Z","iopub.execute_input":"2024-04-17T01:55:57.622867Z","iopub.status.idle":"2024-04-17T01:55:57.651094Z","shell.execute_reply.started":"2024-04-17T01:55:57.622839Z","shell.execute_reply":"2024-04-17T01:55:57.649797Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Conv3dResNet","metadata":{"id":"Y-xkRUYSfc5x"}},{"cell_type":"code","source":"def threeD_to_2D_tensor(x):\n    n_batch, n_channels, s_time, sx, sy = x.shape\n    x = x.transpose(1, 2)\n    return x.reshape(n_batch * s_time, n_channels, sx, sy)\n\n\nclass Conv3dResNet(torch.nn.Module):\n    \"\"\"Conv3dResNet module\"\"\"\n\n    def __init__(self, backbone_type=\"resnet\", relu_type=\"swish\"):\n        \"\"\"__init__.\n\n        :param backbone_type: str, the type of a visual front-end.\n        :param relu_type: str, activation function used in an audio front-end.\n        \"\"\"\n        super(Conv3dResNet, self).__init__()\n        self.frontend_nout = 64\n        self.trunk = ResNet(BasicBlock, [2, 2, 2, 2], relu_type=relu_type)\n        self.frontend3D = nn.Sequential(\n            nn.Conv3d(\n                1, self.frontend_nout, (5, 7, 7), (1, 2, 2), (2, 3, 3), bias=False\n            ),\n            nn.BatchNorm3d(self.frontend_nout),\n            Swish(),\n            nn.MaxPool3d((1, 3, 3), (1, 2, 2), (0, 1, 1)),\n        )\n\n    def forward(self, xs_pad):\n        xs_pad = xs_pad.transpose(1, 2)  # [B, T, C, H, W] -> [B, C, T, H, W]\n\n        B, C, T, H, W = xs_pad.size()\n        xs_pad = self.frontend3D(xs_pad)\n        Tnew = xs_pad.shape[2]\n        xs_pad = threeD_to_2D_tensor(xs_pad)\n        xs_pad = self.trunk(xs_pad)\n        return xs_pad.view(B, Tnew, xs_pad.size(1))","metadata":{"id":"jMIj_0NBfsEC","execution":{"iopub.status.busy":"2024-04-17T01:55:59.053885Z","iopub.execute_input":"2024-04-17T01:55:59.054297Z","iopub.status.idle":"2024-04-17T01:55:59.065919Z","shell.execute_reply.started":"2024-04-17T01:55:59.054263Z","shell.execute_reply":"2024-04-17T01:55:59.064720Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Conv1dResNet","metadata":{"id":"CRX9rtXHfd2l"}},{"cell_type":"code","source":"class Conv1dResNet(torch.nn.Module):\n    def __init__(self, relu_type=\"swish\", a_upsample_ratio=1):\n        super().__init__()\n        self.a_upsample_ratio = a_upsample_ratio\n        self.trunk = ResNet1D(\n            BasicBlock1D,\n            [2, 2, 2, 2],\n            relu_type=relu_type,\n            a_upsample_ratio=a_upsample_ratio,\n        )\n\n    def forward(self, xs_pad):\n        \"\"\"forward.\n\n        :param xs_pad: torch.Tensor, batch of padded input sequences (B, Tmax, idim)\n        \"\"\"\n        B, T, C = xs_pad.size()\n        xs_pad = xs_pad[:, : T // 640 * 640, :]\n        xs_pad = xs_pad.transpose(1, 2)\n        xs_pad = self.trunk(xs_pad)\n        return xs_pad.transpose(1, 2)","metadata":{"id":"a1d5lvfEhFg_","execution":{"iopub.status.busy":"2024-04-17T01:56:00.731067Z","iopub.execute_input":"2024-04-17T01:56:00.731488Z","iopub.status.idle":"2024-04-17T01:56:00.739435Z","shell.execute_reply.started":"2024-04-17T01:56:00.731452Z","shell.execute_reply":"2024-04-17T01:56:00.738279Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## MultiHeadedAttention y RelPositionMultiHeadedAttention","metadata":{"id":"azqaGmtUj3Dw"}},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    \"\"\"Multi-Head Attention layer.\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n    \"\"\"\n\n    def __init__(self, n_head, n_feat, dropout_rate):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super(MultiHeadedAttention, self).__init__()\n        assert n_feat % n_head == 0\n        # We assume d_v always equals d_k\n        self.d_k = n_feat // n_head\n        self.h = n_head\n        self.linear_q = nn.Linear(n_feat, n_feat)\n        self.linear_k = nn.Linear(n_feat, n_feat)\n        self.linear_v = nn.Linear(n_feat, n_feat)\n        self.linear_out = nn.Linear(n_feat, n_feat)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n    def forward_qkv(self, query, key, value):\n        \"\"\"Transform query, key and value.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n        Returns:\n            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).\n            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).\n            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\n        \"\"\"\n        n_batch = query.size(0)\n        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n\n        return q, k, v\n\n    def forward_attention(self, value, scores, mask, rtn_attn=False):\n        \"\"\"Compute attention context vector.\n        Args:\n            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).\n            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).\n            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).\n            rtn_attn (boolean): Flag of return attention score\n        Returns:\n            torch.Tensor: Transformed value (#batch, time1, d_model)\n                weighted by the attention score (#batch, time1, time2).\n        \"\"\"\n        n_batch = value.size(0)\n        if mask is not None:\n            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)\n            min_value = float(\n                np.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n                #numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n            )\n            scores = scores.masked_fill(mask, min_value)\n            self.attn = torch.softmax(scores, dim=-1).masked_fill(\n                mask, 0.0\n            )  # (batch, head, time1, time2)\n        else:\n            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\n        p_attn = self.dropout(self.attn)\n        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n        x = (\n            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)\n        )  # (batch, time1, d_model)\n        if rtn_attn:\n            return self.linear_out(x), self.attn\n        return self.linear_out(x)  # (batch, time1, d_model)\n\n    def forward(self, query, key, value, mask, rtn_attn=False):\n        \"\"\"Compute scaled dot product attention.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n            rtn_attn (boolean): Flag of return attention score\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        return self.forward_attention(v, scores, mask, rtn_attn)\n\n\nclass LegacyRelPositionMultiHeadedAttention(MultiHeadedAttention):\n    \"\"\"Multi-Head Attention layer with relative position encoding (old version).\n    Details can be found in https://github.com/espnet/espnet/pull/2816.\n    Paper: https://arxiv.org/abs/1901.02860\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n        zero_triu (bool): Whether to zero the upper triangular part of attention matrix.\n    \"\"\"\n\n    def __init__(self, n_head, n_feat, dropout_rate, zero_triu=False):\n        \"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"\n        super().__init__(n_head, n_feat, dropout_rate)\n        self.zero_triu = zero_triu\n        # linear transformation for positional encoding\n        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)\n        # these two learnable bias are used in matrix c and matrix d\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        torch.nn.init.xavier_uniform_(self.pos_bias_u)\n        torch.nn.init.xavier_uniform_(self.pos_bias_v)\n\n    def rel_shift(self, x):\n        \"\"\"Compute relative positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, head, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=-1)\n\n        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))\n        x = x_padded[:, :, 1:].view_as(x)\n\n        if self.zero_triu:\n            ones = torch.ones((x.size(2), x.size(3)))\n            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]\n\n        return x\n\n    def forward(self, query, key, value, pos_emb, mask):\n        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            pos_emb (torch.Tensor): Positional embedding tensor (#batch, time1, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n\n        n_batch_pos = pos_emb.size(0)\n        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n        p = p.transpose(1, 2)  # (batch, head, time1, d_k)\n\n        # (batch, head, time1, d_k)\n        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n        # (batch, head, time1, d_k)\n        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n\n        # compute attention score\n        # first compute matrix a and matrix c\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        # (batch, head, time1, time2)\n        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n\n        # compute matrix b and matrix d\n        # (batch, head, time1, time1)\n        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n        matrix_bd = self.rel_shift(matrix_bd)\n\n        scores = (matrix_ac + matrix_bd) / math.sqrt(\n            self.d_k\n        )  # (batch, head, time1, time2)\n\n        return self.forward_attention(v, scores, mask)\n\n\nclass RelPositionMultiHeadedAttention(MultiHeadedAttention):\n    \"\"\"Multi-Head Attention layer with relative position encoding (new implementation).\n    Details can be found in https://github.com/espnet/espnet/pull/2816.\n    Paper: https://arxiv.org/abs/1901.02860\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n        zero_triu (bool): Whether to zero the upper triangular part of attention matrix.\n    \"\"\"\n\n    def __init__(self, n_head, n_feat, dropout_rate, zero_triu=False):\n        \"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"\n        super().__init__(n_head, n_feat, dropout_rate)\n        self.zero_triu = zero_triu\n        # linear transformation for positional encoding\n        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)\n        # these two learnable bias are used in matrix c and matrix d\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        torch.nn.init.xavier_uniform_(self.pos_bias_u)\n        torch.nn.init.xavier_uniform_(self.pos_bias_v)\n\n    def rel_shift(self, x):\n        \"\"\"Compute relative positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).\n            time1 means the length of query vector.\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=-1)\n\n        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))\n        x = x_padded[:, :, 1:].view_as(x)[\n            :, :, :, : x.size(-1) // 2 + 1\n        ]  # only keep the positions from 0 to time2\n\n        if self.zero_triu:\n            ones = torch.ones((x.size(2), x.size(3)), device=x.device)\n            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]\n\n        return x\n\n    def forward(self, query, key, value, pos_emb, mask):\n        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            pos_emb (torch.Tensor): Positional embedding tensor\n                (#batch, 2*time1-1, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n\n        n_batch_pos = pos_emb.size(0)\n        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n        p = p.transpose(1, 2)  # (batch, head, 2*time1-1, d_k)\n\n        # (batch, head, time1, d_k)\n        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n        # (batch, head, time1, d_k)\n        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n\n        # compute attention score\n        # first compute matrix a and matrix c\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        # (batch, head, time1, time2)\n        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n\n        # compute matrix b and matrix d\n        # (batch, head, time1, 2*time1-1)\n        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n        matrix_bd = self.rel_shift(matrix_bd)\n\n        scores = (matrix_ac + matrix_bd) / math.sqrt(\n            self.d_k\n        )  # (batch, head, time1, time2)\n\n        return self.forward_attention(v, scores, mask)","metadata":{"id":"m8jlXupij4_b","execution":{"iopub.status.busy":"2024-04-17T01:56:02.449926Z","iopub.execute_input":"2024-04-17T01:56:02.450348Z","iopub.status.idle":"2024-04-17T01:56:02.497355Z","shell.execute_reply.started":"2024-04-17T01:56:02.450314Z","shell.execute_reply":"2024-04-17T01:56:02.496452Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Bloques conformer","metadata":{"id":"gPL6F3hminyQ"}},{"cell_type":"markdown","source":"### PositionwiseFeedForward","metadata":{"id":"2pSgOj6Liukd"}},{"cell_type":"code","source":"class PositionwiseFeedForward(torch.nn.Module):\n    \"\"\"Positionwise feed forward layer.\n\n    :param int idim: input dimenstion\n    :param int hidden_units: number of hidden units\n    :param float dropout_rate: dropout rate\n\n    \"\"\"\n\n    def __init__(self, idim, hidden_units, dropout_rate):\n        \"\"\"Construct an PositionwiseFeedForward object.\"\"\"\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = torch.nn.Linear(idim, hidden_units)\n        self.w_2 = torch.nn.Linear(hidden_units, idim)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        \"\"\"Forward funciton.\"\"\"\n        return self.w_2(self.dropout(torch.relu(self.w_1(x))))","metadata":{"id":"vrZVGR-sivgW","execution":{"iopub.status.busy":"2024-04-17T01:56:04.560364Z","iopub.execute_input":"2024-04-17T01:56:04.560790Z","iopub.status.idle":"2024-04-17T01:56:04.568527Z","shell.execute_reply.started":"2024-04-17T01:56:04.560757Z","shell.execute_reply":"2024-04-17T01:56:04.567420Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### ConvolutionModule","metadata":{"id":"7qQLfrWejOjd"}},{"cell_type":"code","source":"class ConvolutionModule(nn.Module):\n    \"\"\"ConvolutionModule in Conformer model.\n\n    :param int channels: channels of cnn\n    :param int kernel_size: kernerl size of cnn\n\n    \"\"\"\n\n    def __init__(self, channels, kernel_size, bias=True):\n        \"\"\"Construct an ConvolutionModule object.\"\"\"\n        super(ConvolutionModule, self).__init__()\n        # kernerl_size should be a odd number for 'SAME' padding\n        assert (kernel_size - 1) % 2 == 0\n\n        self.pointwise_cov1 = nn.Conv1d(\n            channels,\n            2 * channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.depthwise_conv = nn.Conv1d(\n            channels,\n            channels,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n            groups=channels,\n            bias=bias,\n        )\n        self.norm = nn.BatchNorm1d(channels)\n        self.pointwise_cov2 = nn.Conv1d(\n            channels,\n            channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.activation = Swish()\n\n    def forward(self, x):\n        \"\"\"Compute covolution module.\n\n        :param torch.Tensor x: (batch, time, size)\n        :return torch.Tensor: convoluted `value` (batch, time, d_model)\n        \"\"\"\n        # exchange the temporal dimension and the feature dimension\n        x = x.transpose(1, 2)\n\n        # GLU mechanism\n        x = self.pointwise_cov1(x)  # (batch, 2*channel, dim)\n        x = nn.functional.glu(x, dim=1)  # (batch, channel, dim)\n\n        # 1D Depthwise Conv\n        x = self.depthwise_conv(x)\n        x = self.activation(self.norm(x))\n\n        x = self.pointwise_cov2(x)\n\n        return x.transpose(1, 2)","metadata":{"id":"Ou93sSi0jS1G","execution":{"iopub.status.busy":"2024-04-17T01:56:06.139858Z","iopub.execute_input":"2024-04-17T01:56:06.140264Z","iopub.status.idle":"2024-04-17T01:56:06.152870Z","shell.execute_reply.started":"2024-04-17T01:56:06.140229Z","shell.execute_reply":"2024-04-17T01:56:06.151636Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Repeat","metadata":{"id":"FiwOPaYzkiE7"}},{"cell_type":"code","source":"class MultiSequential(torch.nn.Sequential):\n    \"\"\"Multi-input multi-output torch.nn.Sequential.\"\"\"\n\n    def forward(self, *args):\n        \"\"\"Repeat.\"\"\"\n        for m in self:\n            args = m(*args)\n        return args\n\n\ndef repeat(N, fn):\n    \"\"\"Repeat module N times.\n\n    :param int N: repeat time\n    :param function fn: function to generate module\n    :return: repeated modules\n    :rtype: MultiSequential\n    \"\"\"\n    return MultiSequential(*[fn() for _ in range(N)])","metadata":{"id":"07fecIHEkjsE","execution":{"iopub.status.busy":"2024-04-17T01:56:07.917603Z","iopub.execute_input":"2024-04-17T01:56:07.918666Z","iopub.status.idle":"2024-04-17T01:56:07.927102Z","shell.execute_reply.started":"2024-04-17T01:56:07.918623Z","shell.execute_reply":"2024-04-17T01:56:07.925666Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### LayerNorm","metadata":{"id":"MwoN53F0kkFq"}},{"cell_type":"code","source":"class LayerNorm(torch.nn.LayerNorm):\n    \"\"\"Layer normalization module.\n\n    :param int nout: output dim size\n    :param int dim: dimension to be normalized\n    \"\"\"\n\n    def __init__(self, nout, dim=-1):\n        \"\"\"Construct an LayerNorm object.\"\"\"\n        super(LayerNorm, self).__init__(nout, eps=1e-12)\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"Apply layer normalization.\n\n        :param torch.Tensor x: input tensor\n        :return: layer normalized tensor\n        :rtype torch.Tensor\n        \"\"\"\n        if self.dim == -1:\n            return super(LayerNorm, self).forward(x)\n        return super(LayerNorm, self).forward(x.transpose(1, -1)).transpose(1, -1)","metadata":{"id":"Z-omRV-1koDy","execution":{"iopub.status.busy":"2024-04-17T01:56:09.751296Z","iopub.execute_input":"2024-04-17T01:56:09.751738Z","iopub.status.idle":"2024-04-17T01:56:09.759503Z","shell.execute_reply.started":"2024-04-17T01:56:09.751702Z","shell.execute_reply":"2024-04-17T01:56:09.758446Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### EncoderLayer (Conformer)","metadata":{"id":"s9rqrInBks8b"}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    \"\"\"Encoder layer module.\n\n    :param int size: input dim\n    :param espnet.nets.pytorch_backend.transformer.attention.\n        MultiHeadedAttention self_attn: self attention module\n        RelPositionMultiHeadedAttention self_attn: self attention module\n    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n        PositionwiseFeedForward feed_forward:\n        feed forward module\n    :param espnet.nets.pytorch_backend.transformer.convolution.\n        ConvolutionModule feed_foreard:\n        feed forward module\n    :param float dropout_rate: dropout rate\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    :param bool macaron_style: whether to use macaron style for PositionwiseFeedForward\n\n    \"\"\"\n\n    def __init__(\n        self,\n        size,\n        self_attn,\n        feed_forward,\n        conv_module,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n        macaron_style=False,\n    ):\n        \"\"\"Construct an EncoderLayer object.\"\"\"\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.ff_scale = 1.0\n        self.conv_module = conv_module\n        self.macaron_style = macaron_style\n        self.norm_ff = LayerNorm(size)  # for the FNN module\n        self.norm_mha = LayerNorm(size)  # for the MHA module\n        if self.macaron_style:\n            self.feed_forward_macaron = copy.deepcopy(feed_forward)\n            self.ff_scale = 0.5\n            # for another FNN module in macaron style\n            self.norm_ff_macaron = LayerNorm(size)\n        if self.conv_module is not None:\n            self.norm_conv = LayerNorm(size)  # for the CNN module\n            self.norm_final = LayerNorm(size)  # for the final output of the block\n        self.dropout = nn.Dropout(dropout_rate)\n        self.size = size\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear = nn.Linear(size + size, size)\n\n    def forward(self, x_input, mask, cache=None):\n        \"\"\"Compute encoded features.\n\n        :param torch.Tensor x_input: encoded source features (batch, max_time_in, size)\n        :param torch.Tensor mask: mask for x (batch, max_time_in)\n        :param torch.Tensor cache: cache for x (batch, max_time_in - 1, size)\n        :rtype: Tuple[torch.Tensor, torch.Tensor]\n        \"\"\"\n        if isinstance(x_input, tuple):\n            x, pos_emb = x_input[0], x_input[1]\n        else:\n            x, pos_emb = x_input, None\n\n        # whether to use macaron style\n        if self.macaron_style:\n            residual = x\n            if self.normalize_before:\n                x = self.norm_ff_macaron(x)\n            x = residual + self.ff_scale * self.dropout(self.feed_forward_macaron(x))\n            if not self.normalize_before:\n                x = self.norm_ff_macaron(x)\n\n        # multi-headed self-attention module\n        residual = x\n        if self.normalize_before:\n            x = self.norm_mha(x)\n\n        if cache is None:\n            x_q = x\n        else:\n            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)\n            x_q = x[:, -1:, :]\n            residual = residual[:, -1:, :]\n            mask = None if mask is None else mask[:, -1:, :]\n\n        if pos_emb is not None:\n            x_att = self.self_attn(x_q, x, x, pos_emb, mask)\n        else:\n            x_att = self.self_attn(x_q, x, x, mask)\n\n        if self.concat_after:\n            x_concat = torch.cat((x, x_att), dim=-1)\n            x = residual + self.concat_linear(x_concat)\n        else:\n            x = residual + self.dropout(x_att)\n        if not self.normalize_before:\n            x = self.norm_mha(x)\n\n        # convolution module\n        if self.conv_module is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.norm_conv(x)\n            x = residual + self.dropout(self.conv_module(x))\n            if not self.normalize_before:\n                x = self.norm_conv(x)\n\n        # feed forward module\n        residual = x\n        if self.normalize_before:\n            x = self.norm_ff(x)\n        x = residual + self.ff_scale * self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm_ff(x)\n\n        if self.conv_module is not None:\n            x = self.norm_final(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        if pos_emb is not None:\n            return (x, pos_emb), mask\n        else:\n            return x, mask","metadata":{"id":"iuCP5OywkuKg","execution":{"iopub.status.busy":"2024-04-17T01:56:11.326513Z","iopub.execute_input":"2024-04-17T01:56:11.327680Z","iopub.status.idle":"2024-04-17T01:56:11.350758Z","shell.execute_reply.started":"2024-04-17T01:56:11.327641Z","shell.execute_reply":"2024-04-17T01:56:11.349720Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Encoder","metadata":{"id":"JDY4Nxr7dbzg"}},{"cell_type":"code","source":"\"\"\"Encoder definition.\"\"\"\n\n# import torch\n# from espnet.nets.pytorch_backend.backbones.conv1d_extractor import Conv1dResNet\n# from espnet.nets.pytorch_backend.backbones.conv3d_extractor import Conv3dResNet\n\n# from espnet.nets.pytorch_backend.nets_utils import rename_state_dict\n\n# from espnet.nets.pytorch_backend.transformer.attention import (\n#     MultiHeadedAttention,  # noqa: H301\n#     RelPositionMultiHeadedAttention,  # noqa: H301\n# )\n# from espnet.nets.pytorch_backend.transformer.convolution import ConvolutionModule\n# from espnet.nets.pytorch_backend.transformer.embedding import (\n#     PositionalEncoding,  # noqa: H301\n#     RelPositionalEncoding,  # noqa: H301\n# )\n# from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n# from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n# from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n\n# from espnet.nets.pytorch_backend.transformer.repeat import repeat\n\n\n# def _pre_hook(\n#     state_dict,\n#     prefix,\n#     local_metadata,\n#     strict,\n#     missing_keys,\n#     unexpected_keys,\n#     error_msgs,\n# ):\n#     # https://github.com/espnet/espnet/commit/21d70286c354c66c0350e65dc098d2ee236faccc#diff-bffb1396f038b317b2b64dd96e6d3563\n#     rename_state_dict(prefix + \"input_layer.\", prefix + \"embed.\", state_dict)\n#     # https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563\n#     rename_state_dict(prefix + \"norm.\", prefix + \"after_norm.\", state_dict)\n\n\nclass Encoder(torch.nn.Module):\n    \"\"\"Transformer encoder module.\n\n    :param int attention_dim: dimention of attention\n    :param int attention_heads: the number of heads of multi head attention\n    :param int linear_units: the number of units of position-wise feed forward\n    :param int num_blocks: the number of decoder blocks\n    :param float dropout_rate: dropout rate\n    :param float attention_dropout_rate: dropout rate in attention\n    :param float positional_dropout_rate: dropout rate after adding positional encoding\n    :param str or torch.nn.Module input_layer: input layer type\n    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    :param int positionwise_conv_kernel_size: kernel size of positionwise conv1d layer\n    :param str encoder_attn_layer_type: encoder attention layer type\n    :param bool macaron_style: whether to use macaron style for positionwise layer\n    :param bool use_cnn_module: whether to use convolution module\n    :param bool zero_triu: whether to zero the upper triangular part of attention matrix\n    :param int cnn_module_kernel: kernerl size of convolution module\n    :param int padding_idx: padding_idx for input_layer=embed\n    \"\"\"\n\n    def __init__(\n        self,\n        attention_dim=768,\n        attention_heads=12,\n        linear_units=3072,\n        num_blocks=12,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        attention_dropout_rate=0.0,\n        input_layer=\"conv2d\",\n        pos_enc_class=RelPositionalEncoding,\n        normalize_before=True,\n        concat_after=False,\n        positionwise_conv_kernel_size=1,\n        macaron_style=False,\n        encoder_attn_layer_type=\"mha\",\n        use_cnn_module=False,\n        zero_triu=False,\n        cnn_module_kernel=31,\n        padding_idx=-1,\n        relu_type=\"prelu\",\n        a_upsample_ratio=1,\n    ):\n        \"\"\"Construct an Encoder object.\"\"\"\n        super(Encoder, self).__init__()\n        #self._register_load_state_dict_pre_hook(_pre_hook)\n\n        # -- frontend module.\n        if input_layer == \"conv1d\":\n            self.frontend = Conv1dResNet(relu_type=relu_type, a_upsample_ratio=a_upsample_ratio)\n        elif input_layer == \"conv3d\":\n            self.frontend = Conv3dResNet(relu_type=relu_type)\n        else:\n            self.frontend = None\n\n        # -- Embedding\n        if encoder_attn_layer_type == \"rel_mha\":\n            pos_enc_class = RelPositionalEncoding\n        if input_layer in [\"conv1d\", \"conv3d\"]:\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(512, attention_dim),\n                pos_enc_class(attention_dim, positional_dropout_rate)\n                )\n        else:\n            raise NotImplementedError(\"Support only conv1d and conv3d\")\n\n        # -- backend module.\n        self.normalize_before = normalize_before\n        positionwise_layer = PositionwiseFeedForward\n        positionwise_layer_args = (attention_dim, linear_units, dropout_rate)\n\n        if encoder_attn_layer_type == \"mha\":\n            encoder_attn_layer = MultiHeadedAttention\n            encoder_attn_layer_args = (\n                attention_heads,\n                attention_dim,\n                attention_dropout_rate,\n            )\n        elif encoder_attn_layer_type == \"rel_mha\":\n            encoder_attn_layer = RelPositionMultiHeadedAttention\n            encoder_attn_layer_args = (\n                attention_heads,\n                attention_dim,\n                attention_dropout_rate,\n                zero_triu,\n            )\n        else:\n            raise ValueError(\"unknown encoder_attn_layer: \" + encoder_attn_layer)\n\n        convolution_layer = ConvolutionModule\n        convolution_layer_args = (attention_dim, cnn_module_kernel)\n\n        self.encoders = repeat(\n            num_blocks,\n            lambda: EncoderLayer(\n                attention_dim,\n                encoder_attn_layer(*encoder_attn_layer_args),\n                positionwise_layer(*positionwise_layer_args),\n                convolution_layer(*convolution_layer_args) if use_cnn_module else None,\n                dropout_rate,\n                normalize_before,\n                concat_after,\n                macaron_style,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n\n    def forward(self, xs, masks):\n        \"\"\"Encode input sequence.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :return: position embedded tensor and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        if isinstance(self.frontend, (Conv1dResNet, Conv3dResNet)):\n            xs = self.frontend(xs)\n\n        xs = self.embed(xs)\n        xs, masks = self.encoders(xs, masks)\n\n        if isinstance(xs, tuple):\n            xs = xs[0]\n\n        if self.normalize_before:\n            xs = self.after_norm(xs)\n\n        return xs, masks\n\n    def forward_one_step(self, xs, masks, cache=None):\n        \"\"\"Encode input frame.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :param List[torch.Tensor] cache: cache tensors\n        :return: position embedded tensor, mask and new cache\n        :rtype Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        \"\"\"\n        if isinstance(self.frontend, (Conv1dResNet, Conv3dResNet)):\n            xs = self.frontend(xs)\n\n        xs = self.embed(xs)\n\n        if cache is None:\n            cache = [None for _ in range(len(self.encoders))]\n        new_cache = []\n        for c, e in zip(cache, self.encoders):\n            xs, masks = e(xs, masks, cache=c)\n            new_cache.append(xs)\n        if self.normalize_before:\n            xs = self.after_norm(xs)\n        return xs, masks, new_cache","metadata":{"id":"cLRIFkwjdfBL","execution":{"iopub.status.busy":"2024-04-17T01:56:12.868786Z","iopub.execute_input":"2024-04-17T01:56:12.869169Z","iopub.status.idle":"2024-04-17T01:56:12.894551Z","shell.execute_reply.started":"2024-04-17T01:56:12.869121Z","shell.execute_reply":"2024-04-17T01:56:12.893264Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## MLPHead","metadata":{"id":"YvlnuzwKl8ya"}},{"cell_type":"code","source":"class MLPHead(torch.nn.Module):\n    def __init__(self, idim, hdim, odim, norm=\"batchnorm\"):\n        super(MLPHead, self).__init__()\n        self.norm = norm\n\n        self.fc1 = torch.nn.Linear(idim, hdim)\n        if norm == \"batchnorm\":\n            self.bn1 = torch.nn.BatchNorm1d(hdim)\n        elif norm == \"layernorm\":\n            self.norm1 = torch.nn.LayerNorm(hdim)\n        self.nonlin1 = torch.nn.ReLU(inplace=True)\n        self.fc2 = torch.nn.Linear(hdim, odim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        if self.norm == \"batchnorm\":\n            x = self.bn1(x.transpose(1, 2)).transpose(1, 2)\n        elif self.norm == \"layernorm\":\n            x = self.norm1(x)\n        x = self.nonlin1(x)\n        x = self.fc2(x)\n        return x","metadata":{"id":"5WS_YEjnl-73","execution":{"iopub.status.busy":"2024-04-17T01:56:14.675726Z","iopub.execute_input":"2024-04-17T01:56:14.676132Z","iopub.status.idle":"2024-04-17T01:56:14.685764Z","shell.execute_reply.started":"2024-04-17T01:56:14.676099Z","shell.execute_reply":"2024-04-17T01:56:14.684695Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## ScorerInterface","metadata":{"id":"JAA0ah8jnioL"}},{"cell_type":"code","source":"class ScorerInterface:\n    \"\"\"Scorer interface for beam search.\n\n    The scorer performs scoring of the all tokens in vocabulary.\n\n    Examples:\n        * Search heuristics\n            * :class:`espnet.nets.scorers.length_bonus.LengthBonus`\n        * Decoder networks of the sequence-to-sequence models\n            * :class:`espnet.nets.pytorch_backend.nets.transformer.decoder.Decoder`\n            * :class:`espnet.nets.pytorch_backend.nets.rnn.decoders.Decoder`\n        * Neural language models\n            * :class:`espnet.nets.pytorch_backend.lm.transformer.TransformerLM`\n            * :class:`espnet.nets.pytorch_backend.lm.default.DefaultRNNLM`\n            * :class:`espnet.nets.pytorch_backend.lm.seq_rnn.SequentialRNNLM`\n\n    \"\"\"\n\n    def init_state(self, x: torch.Tensor) -> Any:\n        \"\"\"Get an initial state for decoding (optional).\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"\n        return None\n\n    def select_state(self, state: Any, i: int, new_id: int = None) -> Any:\n        \"\"\"Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n            new_id (int): New label index to select a state if necessary\n\n        Returns:\n            state: pruned state\n\n        \"\"\"\n        return None if state is None else state[i]\n\n    def score(\n        self, y: torch.Tensor, state: Any, x: torch.Tensor\n    ) -> Tuple[torch.Tensor, Any]:\n        \"\"\"Score new token (required).\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): The encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                scores for next token that has a shape of `(n_vocab)`\n                and next state for ys\n\n        \"\"\"\n        raise NotImplementedError\n\n    def final_score(self, state: Any) -> float:\n        \"\"\"Score eos (optional).\n\n        Args:\n            state: Scorer state for prefix tokens\n\n        Returns:\n            float: final score\n\n        \"\"\"\n        return 0.0\n\n\nclass BatchScorerInterface(ScorerInterface):\n    \"\"\"Batch scorer interface.\"\"\"\n\n    def batch_init_state(self, x: torch.Tensor) -> Any:\n        \"\"\"Get an initial state for decoding (optional).\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"\n        return self.init_state(x)\n\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        \"\"\"Score new token batch (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        \"\"\"\n        warnings.warn(\n            \"{} batch score is implemented through for loop not parallelized\".format(\n                self.__class__.__name__\n            )\n        )\n        scores = list()\n        outstates = list()\n        for i, (y, state, x) in enumerate(zip(ys, states, xs)):\n            score, outstate = self.score(y, state, x)\n            outstates.append(outstate)\n            scores.append(score)\n        scores = torch.cat(scores, 0).view(ys.shape[0], -1)\n        return scores, outstates\n\n\nclass PartialScorerInterface(ScorerInterface):\n    \"\"\"Partial scorer interface for beam search.\n\n    The partial scorer performs scoring when non-partial scorer finished scoring,\n    and receives pre-pruned next tokens to score because it is too heavy to score\n    all the tokens.\n\n    Examples:\n         * Prefix search for connectionist-temporal-classification models\n             * :class:`espnet.nets.scorers.ctc.CTCPrefixScorer`\n\n    \"\"\"\n\n    def score_partial(\n        self, y: torch.Tensor, next_tokens: torch.Tensor, state: Any, x: torch.Tensor\n    ) -> Tuple[torch.Tensor, Any]:\n        \"\"\"Score new token (required).\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            next_tokens (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): The encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"\n        raise NotImplementedError\n\n\nclass BatchPartialScorerInterface(BatchScorerInterface, PartialScorerInterface):\n    \"\"\"Batch partial scorer interface for beam search.\"\"\"\n\n    def batch_score_partial(\n        self,\n        ys: torch.Tensor,\n        next_tokens: torch.Tensor,\n        states: List[Any],\n        xs: torch.Tensor,\n    ) -> Tuple[torch.Tensor, Any]:\n        \"\"\"Score new token (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            next_tokens (torch.Tensor): torch.int64 tokens to score (n_batch, n_token).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for ys that has a shape `(n_batch, n_vocab)`\n                and next states for ys\n        \"\"\"\n        raise NotImplementedError","metadata":{"id":"qCdIDkbinj3V","execution":{"iopub.status.busy":"2024-04-17T01:56:16.211806Z","iopub.execute_input":"2024-04-17T01:56:16.212216Z","iopub.status.idle":"2024-04-17T01:56:16.232341Z","shell.execute_reply.started":"2024-04-17T01:56:16.212186Z","shell.execute_reply":"2024-04-17T01:56:16.231068Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## PositionalEncoding","metadata":{"id":"apmMcHctoPrh"}},{"cell_type":"code","source":"class PositionalEncoding(torch.nn.Module):\n    \"\"\"Positional encoding.\n    Args:\n        d_model (int): Embedding dimension.\n        dropout_rate (float): Dropout rate.\n        max_len (int): Maximum input length.\n        reverse (bool): Whether to reverse the input position. Only for\n        the class LegacyRelPositionalEncoding. We remove it in the current\n        class RelPositionalEncoding.\n    \"\"\"\n\n    def __init__(self, d_model, dropout_rate, max_len=5000, reverse=False):\n        \"\"\"Construct an PositionalEncoding object.\"\"\"\n        super(PositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.reverse = reverse\n        self.xscale = math.sqrt(self.d_model)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.pe = None\n        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n        #self._register_load_state_dict_pre_hook(_pre_hook)\n\n    def extend_pe(self, x):\n        \"\"\"Reset the positional encodings.\"\"\"\n        if self.pe is not None:\n            if self.pe.size(1) >= x.size(1):\n                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n                return\n        pe = torch.zeros(x.size(1), self.d_model)\n        if self.reverse:\n            position = torch.arange(\n                x.size(1) - 1, -1, -1.0, dtype=torch.float32\n            ).unsqueeze(1)\n        else:\n            position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n            * -(math.log(10000.0) / self.d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.pe = pe.to(device=x.device, dtype=x.dtype)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Add positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n        \"\"\"\n        self.extend_pe(x)\n        x = x * self.xscale + self.pe[:, : x.size(1)]\n        return self.dropout(x)","metadata":{"id":"AvHCuslcoUZC","execution":{"iopub.status.busy":"2024-04-17T01:56:17.820937Z","iopub.execute_input":"2024-04-17T01:56:17.821342Z","iopub.status.idle":"2024-04-17T01:56:17.837084Z","shell.execute_reply.started":"2024-04-17T01:56:17.821308Z","shell.execute_reply":"2024-04-17T01:56:17.836039Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## DecoderLayer (Decoder del transformer)","metadata":{"id":"iE1DYbe_pAeg"}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    \"\"\"Single decoder layer module.\n    :param int size: input dim\n    :param espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\n        self_attn: self attention module\n    :param espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\n        src_attn: source attention module\n    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n        PositionwiseFeedForward feed_forward: feed forward layer module\n    :param float dropout_rate: dropout rate\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    \"\"\"\n\n    def __init__(\n        self,\n        size,\n        self_attn,\n        src_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        \"\"\"Construct an DecoderLayer object.\"\"\"\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.norm1 = LayerNorm(size)\n        self.norm2 = LayerNorm(size)\n        self.norm3 = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear1 = nn.Linear(size + size, size)\n            self.concat_linear2 = nn.Linear(size + size, size)\n\n    def forward(self, tgt, tgt_mask, memory, memory_mask, cache=None):\n        \"\"\"Compute decoded features.\n        Args:\n            tgt (torch.Tensor):\n                decoded previous target features (batch, max_time_out, size)\n            tgt_mask (torch.Tensor): mask for x (batch, max_time_out)\n            memory (torch.Tensor): encoded source features (batch, max_time_in, size)\n            memory_mask (torch.Tensor): mask for memory (batch, max_time_in)\n            cache (torch.Tensor): cached output (batch, max_time_out-1, size)\n        \"\"\"\n        residual = tgt\n        if self.normalize_before:\n            tgt = self.norm1(tgt)\n\n        if cache is None:\n            tgt_q = tgt\n            tgt_q_mask = tgt_mask\n        else:\n            # compute only the last frame query keeping dim: max_time_out -> 1\n            assert cache.shape == (\n                tgt.shape[0],\n                tgt.shape[1] - 1,\n                self.size,\n            ), f\"{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}\"\n            tgt_q = tgt[:, -1:, :]\n            residual = residual[:, -1:, :]\n            tgt_q_mask = None\n            if tgt_mask is not None:\n                tgt_q_mask = tgt_mask[:, -1:, :]\n\n        if self.concat_after:\n            tgt_concat = torch.cat(\n                (tgt_q, self.self_attn(tgt_q, tgt, tgt, tgt_q_mask)), dim=-1\n            )\n            x = residual + self.concat_linear1(tgt_concat)\n        else:\n            x = residual + self.dropout(self.self_attn(tgt_q, tgt, tgt, tgt_q_mask))\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        if self.concat_after:\n            x_concat = torch.cat(\n                (x, self.src_attn(x, memory, memory, memory_mask)), dim=-1\n            )\n            x = residual + self.concat_linear2(x_concat)\n        else:\n            x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))\n        if not self.normalize_before:\n            x = self.norm2(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm3(x)\n        x = residual + self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm3(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        return x, tgt_mask, memory, memory_mask","metadata":{"id":"Wuztt-WOpDdo","execution":{"iopub.status.busy":"2024-04-17T01:56:19.410947Z","iopub.execute_input":"2024-04-17T01:56:19.411353Z","iopub.status.idle":"2024-04-17T01:56:19.430335Z","shell.execute_reply.started":"2024-04-17T01:56:19.411318Z","shell.execute_reply":"2024-04-17T01:56:19.429219Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Mask","metadata":{"id":"dQAWTi24pfBa"}},{"cell_type":"code","source":"from distutils.version import LooseVersion\n\nis_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(\"1.2.0\")\n# LooseVersion('1.2.0') == LooseVersion(torch.__version__) can't include e.g. 1.2.0+aaa\nis_torch_1_2 = (\n    LooseVersion(\"1.3\") > LooseVersion(torch.__version__) >= LooseVersion(\"1.2\")\n)\ndatatype = torch.bool if is_torch_1_2_plus else torch.uint8\n\ndef subsequent_mask(size, device=\"cpu\", dtype=datatype):\n    \"\"\"Create mask for subsequent steps (1, size, size).\n\n    :param int size: size of mask\n    :param str device: \"cpu\" or \"cuda\" or torch.Tensor.device\n    :param torch.dtype dtype: result dtype\n    :rtype: torch.Tensor\n    >>> subsequent_mask(3)\n    [[1, 0, 0],\n     [1, 1, 0],\n     [1, 1, 1]]\n    \"\"\"\n    if is_torch_1_2 and dtype == torch.bool:\n        # torch=1.2 doesn't support tril for bool tensor\n        ret = torch.ones(size, size, device=device, dtype=torch.uint8)\n        return torch.tril(ret, out=ret).type(dtype)\n    else:\n        ret = torch.ones(size, size, device=device, dtype=dtype)\n        return torch.tril(ret, out=ret)\n\n\ndef target_mask(ys_in_pad, ignore_id):\n    \"\"\"Create mask for decoder self-attention.\n\n    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n    :param int ignore_id: index of padding\n    :param torch.dtype dtype: result dtype\n    :rtype: torch.Tensor\n    \"\"\"\n    ys_mask = ys_in_pad != ignore_id\n    m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)\n    return ys_mask.unsqueeze(-2) & m","metadata":{"id":"9nLsXl0cpgix","outputId":"2e9618aa-506c-4edf-fcc3-6e9fa0d74455","execution":{"iopub.status.busy":"2024-04-17T01:56:20.995577Z","iopub.execute_input":"2024-04-17T01:56:20.996001Z","iopub.status.idle":"2024-04-17T01:56:21.008901Z","shell.execute_reply.started":"2024-04-17T01:56:20.995968Z","shell.execute_reply":"2024-04-17T01:56:21.007748Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3006193323.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  is_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(\"1.2.0\")\n/tmp/ipykernel_34/3006193323.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  LooseVersion(\"1.3\") > LooseVersion(torch.__version__) >= LooseVersion(\"1.2\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Decoder","metadata":{"id":"Cf9JstLvmyhc"}},{"cell_type":"code","source":"\"\"\"Decoder definition.\"\"\"\n\n# from typing import Any, List, Tuple\n\n# import torch\n\n# from espnet.nets.pytorch_backend.nets_utils import rename_state_dict\n# from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n# from espnet.nets.pytorch_backend.transformer.decoder_layer import DecoderLayer\n# from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n# from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n# from espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\n# from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n#     PositionwiseFeedForward,  # noqa: H301\n# )\n# from espnet.nets.pytorch_backend.transformer.repeat import repeat\n# from espnet.nets.scorer_interface import BatchScorerInterface\n\n\n# def _pre_hook(\n#     state_dict,\n#     prefix,\n#     local_metadata,\n#     strict,\n#     missing_keys,\n#     unexpected_keys,\n#     error_msgs,\n# ):\n#     # https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563\n#     rename_state_dict(prefix + \"output_norm.\", prefix + \"after_norm.\", state_dict)\n\n\nclass Decoder(BatchScorerInterface, torch.nn.Module):\n    \"\"\"Transfomer decoder module.\n\n    :param int odim: output dim\n    :param int attention_dim: dimention of attention\n    :param int attention_heads: the number of heads of multi head attention\n    :param int linear_units: the number of units of position-wise feed forward\n    :param int num_blocks: the number of decoder blocks\n    :param float dropout_rate: dropout rate\n    :param float attention_dropout_rate: dropout rate for attention\n    :param str or torch.nn.Module input_layer: input layer type\n    :param bool use_output_layer: whether to use output layer\n    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    \"\"\"\n\n    def __init__(\n        self,\n        odim,\n        attention_dim=256,\n        attention_heads=4,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        self_attention_dropout_rate=0.0,\n        src_attention_dropout_rate=0.0,\n        input_layer=\"embed\",\n        use_output_layer=True,\n        pos_enc_class=PositionalEncoding,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        \"\"\"Construct an Decoder object.\"\"\"\n        torch.nn.Module.__init__(self)\n        #self._register_load_state_dict_pre_hook(_pre_hook)\n        if input_layer == \"embed\":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(odim, attention_dim),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        # elif input_layer == \"linear\":\n        #     self.embed = torch.nn.Sequential(\n        #         torch.nn.Linear(odim, attention_dim),\n        #         torch.nn.LayerNorm(attention_dim),\n        #         torch.nn.Dropout(dropout_rate),\n        #         torch.nn.ReLU(),\n        #         pos_enc_class(attention_dim, positional_dropout_rate),\n        #     )\n        # elif isinstance(input_layer, torch.nn.Module):\n        #     self.embed = torch.nn.Sequential(\n        #         input_layer, pos_enc_class(attention_dim, positional_dropout_rate)\n        #     )\n        else:\n            raise NotImplementedError(\"only `embed` or torch.nn.Module is supported.\")\n\n        self.normalize_before = normalize_before\n        self.decoders = repeat(\n            num_blocks,\n            lambda: DecoderLayer(\n                attention_dim,\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, self_attention_dropout_rate\n                ),\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, src_attention_dropout_rate\n                ),\n                PositionwiseFeedForward(attention_dim, linear_units, dropout_rate),\n                dropout_rate,\n                normalize_before,\n                concat_after,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n        if use_output_layer:\n            self.output_layer = torch.nn.Linear(attention_dim, odim)\n        else:\n            self.output_layer = None\n\n    def forward(self, tgt, tgt_mask, memory, memory_mask):\n        \"\"\"Forward decoder.\n        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n                                 if input_layer == \"embed\"\n                                 input tensor (batch, maxlen_out, #mels)\n                                 in the other cases\n        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n                                      dtype=torch.uint8 in PyTorch 1.2-\n                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n        :param torch.Tensor memory_mask: encoded memory mask,  (batch, maxlen_in)\n                                         dtype=torch.uint8 in PyTorch 1.2-\n                                         dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :return x: decoded token score before softmax (batch, maxlen_out, token)\n                   if use_output_layer is True,\n                   final block outputs (batch, maxlen_out, attention_dim)\n                   in the other cases\n        :rtype: torch.Tensor\n        :return tgt_mask: score mask before softmax (batch, maxlen_out)\n        :rtype: torch.Tensor\n        \"\"\"\n        x = self.embed(tgt)\n        x, tgt_mask, memory, memory_mask = self.decoders(\n            x, tgt_mask, memory, memory_mask\n        )\n        if self.normalize_before:\n            x = self.after_norm(x)\n        if self.output_layer is not None:\n            x = self.output_layer(x)\n        return x, tgt_mask\n\n    def forward_one_step(self, tgt, tgt_mask, memory, memory_mask=None, cache=None):\n        \"\"\"Forward one step.\n        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n                                      dtype=torch.uint8 in PyTorch 1.2-\n                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n        :param List[torch.Tensor] cache:\n            cached output list of (batch, max_time_out-1, size)\n        :return y, cache: NN output value and cache per `self.decoders`.\n            `y.shape` is (batch, maxlen_out, token)\n        :rtype: Tuple[torch.Tensor, List[torch.Tensor]]\n        \"\"\"\n        x = self.embed(tgt)\n        if cache is None:\n            cache = [None] * len(self.decoders)\n        new_cache = []\n        for c, decoder in zip(cache, self.decoders):\n            x, tgt_mask, memory, memory_mask = decoder(\n                x, tgt_mask, memory, memory_mask, cache=c\n            )\n            new_cache.append(x)\n\n        if self.normalize_before:\n            y = self.after_norm(x[:, -1])\n        else:\n            y = x[:, -1]\n        if self.output_layer is not None:\n            y = torch.log_softmax(self.output_layer(y), dim=-1)\n\n        return y, new_cache\n\n    # beam search API (see ScorerInterface)\n    def score(self, ys, state, x):\n        \"\"\"Score.\"\"\"\n        ys_mask = subsequent_mask(len(ys), device=x.device).unsqueeze(0)\n        logp, state = self.forward_one_step(\n            ys.unsqueeze(0), ys_mask, x.unsqueeze(0), cache=state\n        )\n        return logp.squeeze(0), state\n\n    # batch beam search API (see BatchScorerInterface)\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        \"\"\"Score new token batch (required).\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n        \"\"\"\n        # merge states\n        n_batch = len(ys)\n        n_layers = len(self.decoders)\n        if states[0] is None:\n            batch_state = None\n        else:\n            # transpose state of [batch, layer] into [layer, batch]\n            batch_state = [\n                torch.stack([states[b][l] for b in range(n_batch)])\n                for l in range(n_layers)\n            ]\n\n        # batch decoding\n        ys_mask = subsequent_mask(ys.size(-1), device=xs.device).unsqueeze(0)\n        logp, states = self.forward_one_step(ys, ys_mask, xs, cache=batch_state)\n\n        # transpose state of [layer, batch] into [batch, layer]\n        state_list = [[states[l][b] for l in range(n_layers)] for b in range(n_batch)]\n        return logp, state_list","metadata":{"id":"lCkqlATum0cr","execution":{"iopub.status.busy":"2024-04-17T01:56:22.740025Z","iopub.execute_input":"2024-04-17T01:56:22.740408Z","iopub.status.idle":"2024-04-17T01:56:22.778446Z","shell.execute_reply.started":"2024-04-17T01:56:22.740362Z","shell.execute_reply":"2024-04-17T01:56:22.776967Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{"id":"D6w-KSvPtOO3"}},{"cell_type":"code","source":"def to_device(m, x):\n    \"\"\"Send tensor into the device of the module.\n\n    Args:\n        m (torch.nn.Module): Torch module.\n        x (Tensor): Torch tensor.\n\n    Returns:\n        Tensor: Torch tensor located in the same place as torch module.\n\n    \"\"\"\n    if isinstance(m, torch.nn.Module):\n        device = next(m.parameters()).device\n    elif isinstance(m, torch.Tensor):\n        device = m.device\n    else:\n        raise TypeError(\n            \"Expected torch.nn.Module or torch.tensor, \" f\"bot got: {type(m)}\"\n        )\n    return x.to(device)\n\ndef pad_list(xs, pad_value):\n    \"\"\"Perform padding for the list of tensors.\n\n    Args:\n        xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)].\n        pad_value (float): Value for padding.\n\n    Returns:\n        Tensor: Padded tensor (B, Tmax, `*`).\n\n    Examples:\n        >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)]\n        >>> x\n        [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]\n        >>> pad_list(x, 0)\n        tensor([[1., 1., 1., 1.],\n                [1., 1., 0., 0.],\n                [1., 0., 0., 0.]])\n\n    \"\"\"\n    n_batch = len(xs)\n    max_len = max(x.size(0) for x in xs)\n    pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)\n\n    for i in range(n_batch):\n        pad[i, : xs[i].size(0)] = xs[i]\n\n    return pad\n\n\ndef make_pad_mask(lengths, xs=None, length_dim=-1, maxlen=None):\n    \"\"\"Make mask tensor containing indices of padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        Tensor: Mask tensor containing indices of padded part.\n                dtype=torch.uint8 in PyTorch 1.2-\n                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_pad_mask(lengths)\n        masks = [[0, 0, 0, 0 ,0],\n                 [0, 0, 0, 1, 1],\n                 [0, 0, 1, 1, 1]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0],\n                 [0, 0, 0, 0]],\n                [[0, 0, 0, 1],\n                 [0, 0, 0, 1]],\n                [[0, 0, 1, 1],\n                 [0, 0, 1, 1]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_pad_mask(lengths, xs, 1)\n        tensor([[[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n        >>> make_pad_mask(lengths, xs, 2)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n    \"\"\"\n    if length_dim == 0:\n        raise ValueError(\"length_dim cannot be 0: {}\".format(length_dim))\n\n    if not isinstance(lengths, list):\n        lengths = lengths.tolist()\n    bs = int(len(lengths))\n    if maxlen is None:\n        if xs is None:\n            maxlen = int(max(lengths))\n        else:\n            maxlen = xs.size(length_dim)\n    else:\n        assert xs is None\n        assert maxlen >= int(max(lengths))\n\n    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n    mask = seq_range_expand >= seq_length_expand\n\n    if xs is not None:\n        assert xs.size(0) == bs, (xs.size(0), bs)\n\n        if length_dim < 0:\n            length_dim = xs.dim() + length_dim\n        # ind = (:, None, ..., None, :, , None, ..., None)\n        ind = tuple(\n            slice(None) if i in (0, length_dim) else None for i in range(xs.dim())\n        )\n        mask = mask[ind].expand_as(xs).to(xs.device)\n    return mask\n\ndef make_non_pad_mask(lengths, xs=None, length_dim=-1):\n    \"\"\"Make mask tensor containing indices of non-padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        ByteTensor: mask tensor containing indices of padded part.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[1, 1, 1, 1 ,1],\n                 [1, 1, 1, 0, 0],\n                 [1, 1, 0, 0, 0]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1],\n                 [1, 1, 1, 1]],\n                [[1, 1, 1, 0],\n                 [1, 1, 1, 0]],\n                [[1, 1, 0, 0],\n                 [1, 1, 0, 0]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_non_pad_mask(lengths, xs, 1)\n        tensor([[[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)\n        >>> make_non_pad_mask(lengths, xs, 2)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n    \"\"\"\n    return ~make_pad_mask(lengths, xs, length_dim)\n\n\ndef th_accuracy(pad_outputs, pad_targets, ignore_label):\n    \"\"\"Calculate accuracy.\n\n    Args:\n        pad_outputs (Tensor): Prediction tensors (B * Lmax, D).\n        pad_targets (LongTensor): Target label tensors (B, Lmax, D).\n        ignore_label (int): Ignore label id.\n\n    Returns:\n        float: Accuracy value (0.0 - 1.0).\n\n    \"\"\"\n    pad_pred = pad_outputs.view(\n        pad_targets.size(0), pad_targets.size(1), pad_outputs.size(1)\n    ).argmax(2)\n    mask = pad_targets != ignore_label\n    numerator = torch.sum(\n        pad_pred.masked_select(mask) == pad_targets.masked_select(mask)\n    )\n    denominator = torch.sum(mask)\n    return float(numerator) / float(denominator)\n","metadata":{"id":"ESjiQFI8tQr9","execution":{"iopub.status.busy":"2024-04-17T01:56:24.301185Z","iopub.execute_input":"2024-04-17T01:56:24.301624Z","iopub.status.idle":"2024-04-17T01:56:24.326800Z","shell.execute_reply.started":"2024-04-17T01:56:24.301586Z","shell.execute_reply":"2024-04-17T01:56:24.325725Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## add_sos_eos","metadata":{"id":"nnas4FCNuK-3"}},{"cell_type":"code","source":"def add_sos_eos(ys_pad, sos, eos, ignore_id):\n    \"\"\"Add <sos> and <eos> labels.\n\n    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n    :param int sos: index of <sos>\n    :param int eos: index of <eeos>\n    :param int ignore_id: index of padding\n    :return: padded tensor (B, Lmax)\n    :rtype: torch.Tensor\n    :return: padded tensor (B, Lmax)\n    :rtype: torch.Tensor\n    \"\"\"\n    #from espnet.nets.pytorch_backend.nets_utils import pad_list\n\n    _sos = ys_pad.new([sos])\n    _eos = ys_pad.new([eos])\n    ys = [y[y != ignore_id] for y in ys_pad]  # parse padded ys\n    ys_in = [torch.cat([_sos, y], dim=0) for y in ys]\n    ys_out = [torch.cat([y, _eos], dim=0) for y in ys]\n    return pad_list(ys_in, eos), pad_list(ys_out, ignore_id)","metadata":{"id":"bhKNalMJuMWB","execution":{"iopub.status.busy":"2024-04-17T01:56:25.843598Z","iopub.execute_input":"2024-04-17T01:56:25.844439Z","iopub.status.idle":"2024-04-17T01:56:25.851659Z","shell.execute_reply.started":"2024-04-17T01:56:25.844392Z","shell.execute_reply":"2024-04-17T01:56:25.850554Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## LabelSmoothingLoss","metadata":{"id":"w-Y9I92RqrVT"}},{"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    \"\"\"Label-smoothing loss.\n\n    :param int size: the number of class\n    :param int padding_idx: ignored class id\n    :param float smoothing: smoothing rate (0.0 means the conventional CE)\n    :param bool normalize_length: normalize loss by sequence length if True\n    :param torch.nn.Module criterion: loss function to be smoothed\n    \"\"\"\n\n    def __init__(\n        self,\n        size,\n        padding_idx,\n        smoothing,\n        normalize_length=False,\n        criterion=nn.KLDivLoss(reduction=\"none\"),\n    ):\n        \"\"\"Construct an LabelSmoothingLoss object.\"\"\"\n        super(LabelSmoothingLoss, self).__init__()\n        self.criterion = criterion\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n        self.normalize_length = normalize_length\n\n    def forward(self, x, target):\n        \"\"\"Compute loss between x and target.\n\n        :param torch.Tensor x: prediction (batch, seqlen, class)\n        :param torch.Tensor target:\n            target signal masked with self.padding_id (batch, seqlen)\n        :return: scalar float value\n        :rtype torch.Tensor\n        \"\"\"\n        assert x.size(2) == self.size\n        batch_size = x.size(0)\n        x = x.view(-1, self.size)\n        target = target.view(-1)\n        with torch.no_grad():\n            true_dist = x.clone()\n            true_dist.fill_(self.smoothing / (self.size - 1))\n            ignore = target == self.padding_idx  # (B,)\n            total = len(target) - ignore.sum().item()\n            target = target.masked_fill(ignore, 0)  # avoid -1 index\n            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n        kl = self.criterion(torch.log_softmax(x, dim=1), true_dist)\n        denom = total if self.normalize_length else batch_size\n        return kl.masked_fill(ignore.unsqueeze(1), 0).sum() / denom","metadata":{"id":"HJCMkwOZq11b","execution":{"iopub.status.busy":"2024-04-17T01:56:27.475779Z","iopub.execute_input":"2024-04-17T01:56:27.476515Z","iopub.status.idle":"2024-04-17T01:56:27.488678Z","shell.execute_reply.started":"2024-04-17T01:56:27.476466Z","shell.execute_reply":"2024-04-17T01:56:27.487428Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## CTC","metadata":{"id":"P4_0WJ1RrLa0"}},{"cell_type":"code","source":"class CTC(torch.nn.Module):\n    \"\"\"CTC module\n\n    :param int odim: dimension of outputs\n    :param int eprojs: number of encoder projection units\n    :param float dropout_rate: dropout rate (0.0 ~ 1.0)\n    :param str ctc_type: builtin or warpctc\n    :param bool reduce: reduce the CTC loss into a scalar\n    \"\"\"\n\n    def __init__(self, odim, eprojs, dropout_rate, ctc_type=\"builtin\", reduce=True):\n        super().__init__()\n        self.dropout_rate = dropout_rate\n        self.loss = None\n        self.ctc_lo = torch.nn.Linear(eprojs, odim)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        self.probs = None  # for visualization\n\n        # In case of Pytorch >= 1.7.0, CTC will be always builtin\n        self.ctc_type = (\n            ctc_type\n            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")\n            else \"builtin\"\n        )\n\n        if ctc_type != self.ctc_type:\n            logging.debug(f\"CTC was set to {self.ctc_type} due to PyTorch version.\")\n\n        if self.ctc_type == \"builtin\":\n            reduction_type = \"sum\" if reduce else \"none\"\n            self.ctc_loss = torch.nn.CTCLoss(\n                reduction=reduction_type, zero_infinity=True\n            )\n        # elif self.ctc_type == \"cudnnctc\":\n        #     reduction_type = \"sum\" if reduce else \"none\"\n        #     self.ctc_loss = torch.nn.CTCLoss(reduction=reduction_type)\n        # elif self.ctc_type == \"warpctc\":\n        #     import warpctc_pytorch as warp_ctc\n\n        #     self.ctc_loss = warp_ctc.CTCLoss(size_average=True, reduce=reduce)\n        # elif self.ctc_type == \"gtnctc\":\n        #     from espnet.nets.pytorch_backend.gtn_ctc import GTNCTCLossFunction\n\n        #     self.ctc_loss = GTNCTCLossFunction.apply\n        else:\n            raise ValueError(\n                'ctc_type must be \"builtin\" or \"warpctc\": {}'.format(self.ctc_type)\n            )\n\n        self.ignore_id = -1\n        self.reduce = reduce\n\n    def loss_fn(self, th_pred, th_target, th_ilen, th_olen):\n        if self.ctc_type in [\"builtin\", \"cudnnctc\"]:\n            th_pred = th_pred.log_softmax(2)\n            # Use the deterministic CuDNN implementation of CTC loss to avoid\n            #  [issue#17798](https://github.com/pytorch/pytorch/issues/17798)\n            with torch.backends.cudnn.flags(deterministic=True):\n                loss = self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n            # Batch-size average\n            loss = loss / th_pred.size(1)\n            return loss\n        elif self.ctc_type == \"warpctc\":\n            return self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n        elif self.ctc_type == \"gtnctc\":\n            targets = [t.tolist() for t in th_target]\n            log_probs = torch.nn.functional.log_softmax(th_pred, dim=2)\n            return self.ctc_loss(log_probs, targets, th_ilen, 0, \"none\")\n        else:\n            raise NotImplementedError\n\n    def forward(self, hs_pad, hlens, ys_pad):\n        \"\"\"CTC forward\n\n        :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D)\n        :param torch.Tensor hlens: batch of lengths of hidden state sequences (B)\n        :param torch.Tensor ys_pad:\n            batch of padded character id sequence tensor (B, Lmax)\n        :return: ctc loss value\n        :rtype: torch.Tensor\n        \"\"\"\n        # TODO(kan-bayashi): need to make more smart way\n        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n\n        # zero padding for hs\n        ys_hat = self.ctc_lo(self.dropout(hs_pad))\n        if self.ctc_type != \"gtnctc\":\n            ys_hat = ys_hat.transpose(0, 1)\n\n        if self.ctc_type == \"builtin\":\n            olens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))\n            hlens = hlens.long()\n            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix\n            self.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)\n        else:\n            self.loss = None\n            hlens = torch.from_numpy(np.fromiter(hlens, dtype=np.int32))\n            olens = torch.from_numpy(\n                np.fromiter((x.size(0) for x in ys), dtype=np.int32)\n            )\n            # zero padding for ys\n            ys_true = torch.cat(ys).cpu().int()  # batch x olen\n            # get ctc loss\n            # expected shape of seqLength x batchSize x alphabet_size\n            dtype = ys_hat.dtype\n            if self.ctc_type == \"warpctc\" or dtype == torch.float16:\n                # warpctc only supports float32\n                # torch.ctc does not support float16 (#1751)\n                ys_hat = ys_hat.to(dtype=torch.float32)\n            if self.ctc_type == \"cudnnctc\":\n                # use GPU when using the cuDNN implementation\n                ys_true = to_device(hs_pad, ys_true)\n            if self.ctc_type == \"gtnctc\":\n                # keep as list for gtn\n                ys_true = ys\n            self.loss = to_device(\n                hs_pad, self.loss_fn(ys_hat, ys_true, hlens, olens)\n            ).to(dtype=dtype)\n\n        # get length info\n        \"\"\"\n        logging.debug(\n            self.__class__.__name__\n            + \" input lengths:  \"\n            + \"\".join(str(hlens).split(\"\\n\"))\n        )\n        logging.debug(\n            self.__class__.__name__\n            + \" output lengths: \"\n            + \"\".join(str(olens).split(\"\\n\"))\n        )\n        \"\"\"\n        if self.reduce:\n            # NOTE: sum() is needed to keep consistency\n            # since warpctc return as tensor w/ shape (1,)\n            # but builtin return as tensor w/o shape (scalar).\n            self.loss = self.loss.sum()\n            # logging.debug(\"ctc loss:\" + str(float(self.loss)))\n\n        return self.loss, ys_hat\n\n    def softmax(self, hs_pad):\n        \"\"\"softmax of frame activations\n\n        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: log softmax applied 3d tensor (B, Tmax, odim)\n        :rtype: torch.Tensor\n        \"\"\"\n        self.probs = F.softmax(self.ctc_lo(hs_pad), dim=-1)\n        return self.probs\n\n    def log_softmax(self, hs_pad):\n        \"\"\"log_softmax of frame activations\n\n        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: log softmax applied 3d tensor (B, Tmax, odim)\n        :rtype: torch.Tensor\n        \"\"\"\n        return F.log_softmax(self.ctc_lo(hs_pad), dim=-1)\n\n    def argmax(self, hs_pad):\n        \"\"\"argmax of frame activations\n\n        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: argmax applied 2d tensor (B, Tmax)\n        :rtype: torch.Tensor\n        \"\"\"\n        return torch.argmax(self.ctc_lo(hs_pad), dim=-1)\n\n    def forced_align(self, h, y, blank_id=0):\n        \"\"\"forced alignment.\n\n        :param torch.Tensor h: hidden state sequence, 2d tensor (T, D)\n        :param torch.Tensor y: id sequence tensor 1d tensor (L)\n        :param int y: blank symbol index\n        :return: best alignment results\n        :rtype: list\n        \"\"\"\n\n        def interpolate_blank(label, blank_id=0):\n            \"\"\"Insert blank token between every two label token.\"\"\"\n            label = np.expand_dims(label, 1)\n            blanks = np.zeros((label.shape[0], 1), dtype=np.int64) + blank_id\n            label = np.concatenate([blanks, label], axis=1)\n            label = label.reshape(-1)\n            label = np.append(label, label[0])\n            return label\n\n        lpz = self.log_softmax(h)\n        lpz = lpz.squeeze(0)\n\n        y_int = interpolate_blank(y, blank_id)\n\n        logdelta = np.zeros((lpz.size(0), len(y_int))) - 100000000000.0  # log of zero\n        state_path = (\n            np.zeros((lpz.size(0), len(y_int)), dtype=np.int16) - 1\n        )  # state path\n\n        logdelta[0, 0] = lpz[0][y_int[0]]\n        logdelta[0, 1] = lpz[0][y_int[1]]\n\n        for t in six.moves.range(1, lpz.size(0)):\n            for s in six.moves.range(len(y_int)):\n                if y_int[s] == blank_id or s < 2 or y_int[s] == y_int[s - 2]:\n                    candidates = np.array([logdelta[t - 1, s], logdelta[t - 1, s - 1]])\n                    prev_state = [s, s - 1]\n                else:\n                    candidates = np.array(\n                        [\n                            logdelta[t - 1, s],\n                            logdelta[t - 1, s - 1],\n                            logdelta[t - 1, s - 2],\n                        ]\n                    )\n                    prev_state = [s, s - 1, s - 2]\n                logdelta[t, s] = np.max(candidates) + lpz[t][y_int[s]]\n                state_path[t, s] = prev_state[np.argmax(candidates)]\n\n        state_seq = -1 * np.ones((lpz.size(0), 1), dtype=np.int16)\n\n        candidates = np.array(\n            [logdelta[-1, len(y_int) - 1], logdelta[-1, len(y_int) - 2]]\n        )\n        prev_state = [len(y_int) - 1, len(y_int) - 2]\n        state_seq[-1] = prev_state[np.argmax(candidates)]\n        for t in six.moves.range(lpz.size(0) - 2, -1, -1):\n            state_seq[t] = state_path[t + 1, state_seq[t + 1, 0]]\n\n        output_state_seq = []\n        for t in six.moves.range(0, lpz.size(0)):\n            output_state_seq.append(y_int[state_seq[t, 0]])\n\n        return output_state_seq\n\n    def forced_align_batch(self, hs_pad, ys_pad, ilens, blank_id=0):\n        \"\"\"forced alignment with batch processing.\n\n        :param torch.Tensor hs_pad: hidden state sequence, 3d tensor (T, B, D)\n        :param torch.Tensor ys_pad: id sequence tensor 2d tensor (B, L)\n        :param torch.Tensor ilens: Input length of each utterance (B,)\n        :param int blank_id: blank symbol index\n        :return: best alignment results\n        :rtype: list of numpy.array\n        \"\"\"\n\n        def interpolate_blank(label, olens_int):\n            \"\"\"Insert blank token between every two label token.\"\"\"\n            lab_len = label.shape[1] * 2 + 1\n            label_out = np.full((label.shape[0], lab_len), blank_id, dtype=np.int64)\n            label_out[:, 1::2] = label\n            for b in range(label.shape[0]):\n                label_out[b, olens_int[b] * 2 + 1 :] = self.ignore_id\n            return label_out\n\n        neginf = float(\"-inf\")  # log of zero\n        # lpz = self.log_softmax(hs_pad).cpu().detach().numpy()\n        # hs_pad = hs_pad.transpose(1,0)\n        lpz = F.log_softmax(hs_pad, dim=-1).cpu().detach().numpy()\n        ilens = ilens.cpu().detach().numpy()\n\n        ys_pad = ys_pad.cpu().detach().numpy()\n        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n        olens = np.array([len(s) for s in ys])\n        olens_int = olens * 2 + 1\n        ys_int = interpolate_blank(ys_pad, olens_int)\n\n        Tmax, B, _ = lpz.shape\n        Lmax = ys_int.shape[-1]\n        logdelta = np.full((Tmax, B, Lmax), neginf, dtype=lpz.dtype)\n        state_path = -np.ones(logdelta.shape, dtype=np.int16)  # state path\n\n        b_indx = np.arange(B, dtype=np.int64)\n        t_0 = np.zeros(B, dtype=np.int64)\n        logdelta[0, :, 0] = lpz[t_0, b_indx, ys_int[:, 0]]\n        logdelta[0, :, 1] = lpz[t_0, b_indx, ys_int[:, 1]]\n\n        s_indx_mat = np.arange(Lmax)[None, :].repeat(B, 0)\n        notignore_mat = ys_int != self.ignore_id\n        same_lab_mat = np.zeros((B, Lmax), dtype=np.bool)\n        same_lab_mat[:, 3::2] = ys_int[:, 3::2] == ys_int[:, 1:-2:2]\n        Lmin = olens_int.min()\n        for t in range(1, Tmax):\n            s_start = max(0, Lmin - (Tmax - t) * 2)\n            s_end = min(Lmax, t * 2 + 2)\n            candidates = np.full((B, Lmax, 3), neginf, dtype=logdelta.dtype)\n            candidates[:, :, 0] = logdelta[t - 1, :, :]\n            candidates[:, 1:, 1] = logdelta[t - 1, :, :-1]\n            candidates[:, 3::2, 2] = logdelta[t - 1, :, 1:-2:2]\n            candidates[same_lab_mat, 2] = neginf\n            candidates_ = candidates[:, s_start:s_end, :]\n            idx = candidates_.argmax(-1)\n            b_i, s_i = np.ogrid[:B, : idx.shape[-1]]\n            nignore = notignore_mat[:, s_start:s_end]\n            logdelta[t, :, s_start:s_end][nignore] = (\n                candidates_[b_i, s_i, idx][nignore]\n                + lpz[t, b_i, ys_int[:, s_start:s_end]][nignore]\n            )\n            s = s_indx_mat[:, s_start:s_end]\n            state_path[t, :, s_start:s_end][nignore] = (s - idx)[nignore]\n\n        alignments = []\n        prev_states = logdelta[\n            ilens[:, None] - 1,\n            b_indx[:, None],\n            np.stack([olens_int - 2, olens_int - 1], -1),\n        ].argmax(-1)\n        for b in range(B):\n            T, L = ilens[b], olens_int[b]\n            prev_state = prev_states[b] + L - 2\n            ali = np.empty(T, dtype=ys_int.dtype)\n            ali[T - 1] = ys_int[b, prev_state]\n            for t in range(T - 2, -1, -1):\n                prev_state = state_path[t + 1, b, prev_state]\n                ali[t] = ys_int[b, prev_state]\n            alignments.append(ali)\n\n        return alignments","metadata":{"id":"tVaK6pp-rNUq","execution":{"iopub.status.busy":"2024-04-17T01:56:29.161283Z","iopub.execute_input":"2024-04-17T01:56:29.161875Z","iopub.status.idle":"2024-04-17T01:56:29.221555Z","shell.execute_reply.started":"2024-04-17T01:56:29.161841Z","shell.execute_reply":"2024-04-17T01:56:29.220444Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## E2E AV","metadata":{"id":"6-ciO8kw0Wda"}},{"cell_type":"code","source":"\"\"\"Transformer speech recognition model (pytorch).\"\"\"\n\n# import logging\n# import numpy\n# import torch\n\n# from espnet.nets.pytorch_backend.ctc import CTC\n# from espnet.nets.pytorch_backend.nets_utils import (\n#     make_non_pad_mask,\n#     th_accuracy,\n# )\n# from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n# from espnet.nets.pytorch_backend.transformer.decoder import Decoder\n# from espnet.nets.pytorch_backend.transformer.encoder import Encoder\n# from espnet.nets.pytorch_backend.transformer.label_smoothing_loss import LabelSmoothingLoss\n# from espnet.nets.pytorch_backend.transformer.mask import target_mask\n# from espnet.nets.pytorch_backend.nets_utils import MLPHead\n\n\nclass E2E(torch.nn.Module):\n    def __init__(self, odim, args, ignore_id=-1):\n        torch.nn.Module.__init__(self)\n\n        self.encoder = Encoder(\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=args.transformer_input_layer,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            attention_dropout_rate=args.transformer_attn_dropout_rate,\n            encoder_attn_layer_type=args.transformer_encoder_attn_layer_type,\n            macaron_style=args.macaron_style,\n            use_cnn_module=args.use_cnn_module,\n            cnn_module_kernel=args.cnn_module_kernel,\n            zero_triu=getattr(args, \"zero_triu\", False),\n            a_upsample_ratio=args.a_upsample_ratio,\n            relu_type=getattr(args, \"relu_type\", \"swish\"),\n        )\n\n        self.aux_encoder = Encoder(\n            attention_dim=args.aux_adim,\n            attention_heads=args.aux_aheads,\n            linear_units=args.aux_eunits,\n            num_blocks=args.aux_elayers,\n            input_layer=args.aux_transformer_input_layer,\n            dropout_rate=args.aux_dropout_rate,\n            positional_dropout_rate=args.aux_dropout_rate,\n            attention_dropout_rate=args.aux_transformer_attn_dropout_rate,\n            encoder_attn_layer_type=args.aux_transformer_encoder_attn_layer_type,\n            macaron_style=args.aux_macaron_style,\n            use_cnn_module=args.aux_use_cnn_module,\n            cnn_module_kernel=args.aux_cnn_module_kernel,\n            zero_triu=getattr(args, \"aux_zero_triu\", False),\n            a_upsample_ratio=args.aux_a_upsample_ratio,\n            relu_type=getattr(args, \"aux_relu_type\", \"swish\"),\n        )\n\n        # self.transformer_input_layer = args.transformer_input_layer\n        # self.a_upsample_ratio = args.a_upsample_ratio\n\n        self.fusion = MLPHead(\n            idim=args.adim + args.aux_adim,\n            hdim=args.fusion_hdim,\n            odim=args.adim,\n            norm=args.fusion_norm,\n        )\n\n        self.proj_decoder = None\n        if args.adim != args.ddim:\n            self.proj_decoder = torch.nn.Linear(args.adim, args.ddim)\n\n        if args.mtlalpha < 1:\n            self.decoder = Decoder(\n                odim=odim,\n                attention_dim=args.ddim,\n                attention_heads=args.dheads,\n                linear_units=args.dunits,\n                num_blocks=args.dlayers,\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n                src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n            )\n        else:\n            self.decoder = None\n\n        self.blank = 0\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.odim = odim\n        self.ignore_id = ignore_id\n\n        # self.lsm_weight = a\n        self.criterion = LabelSmoothingLoss(\n            self.odim,\n            self.ignore_id,\n            args.lsm_weight,\n            args.transformer_length_normalized_loss,\n        )\n\n        self.adim = args.adim\n        self.mtlalpha = args.mtlalpha\n        if args.mtlalpha > 0.0:\n            self.ctc = CTC(\n                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n            )\n        else:\n            self.ctc = None\n\n    def forward(self, video, audio, video_lengths, audio_lengths, label):\n        video_padding_mask = make_non_pad_mask(video_lengths).to(video.device).unsqueeze(-2)\n        video_feat, _ = self.encoder(video, video_padding_mask)\n\n        audio_lengths = torch.div(audio_lengths, 640, rounding_mode=\"trunc\")\n        audio_padding_mask = make_non_pad_mask(audio_lengths).to(video.device).unsqueeze(-2)\n        audio_feat, _ = self.aux_encoder(audio, audio_padding_mask)\n\n        x = self.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n\n        # ctc loss\n        loss_ctc, ys_hat = self.ctc(x, video_lengths, label)\n\n        if self.proj_decoder:\n            x = self.proj_decoder(x)\n\n        # decoder loss\n        ys_in_pad, ys_out_pad = add_sos_eos(label, self.sos, self.eos, self.ignore_id)\n        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n        pred_pad, _ = self.decoder(ys_in_pad, ys_mask, x, video_padding_mask)\n        loss_att = self.criterion(pred_pad, ys_out_pad)\n        loss = self.mtlalpha * loss_ctc + (1 - self.mtlalpha) * loss_att\n\n        acc = th_accuracy(\n            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n        )\n\n        return loss, loss_ctc, loss_att, acc","metadata":{"id":"RSYDWjNC0V0o","execution":{"iopub.status.busy":"2024-04-17T01:56:30.723322Z","iopub.execute_input":"2024-04-17T01:56:30.723730Z","iopub.status.idle":"2024-04-17T01:56:30.747795Z","shell.execute_reply.started":"2024-04-17T01:56:30.723698Z","shell.execute_reply":"2024-04-17T01:56:30.746545Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## E2E","metadata":{"id":"tYvrFGxC06jI"}},{"cell_type":"code","source":"\"\"\"Transformer speech recognition model (pytorch).\"\"\"\n\n# import logging\n# import numpy\n# import torch\n\n# from espnet.nets.pytorch_backend.ctc import CTC\n# from espnet.nets.pytorch_backend.nets_utils import (\n#     make_non_pad_mask,\n#     th_accuracy,\n# )\n# from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n# from espnet.nets.pytorch_backend.transformer.decoder import Decoder\n# from espnet.nets.pytorch_backend.transformer.encoder import Encoder\n# from espnet.nets.pytorch_backend.transformer.label_smoothing_loss import LabelSmoothingLoss\n# from espnet.nets.pytorch_backend.transformer.mask import target_mask\n\n\nclass E2E(torch.nn.Module):\n    def __init__(self, odim, args, ignore_id=-1):\n        torch.nn.Module.__init__(self)\n\n        self.encoder = Encoder(\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=args.transformer_input_layer,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            attention_dropout_rate=args.transformer_attn_dropout_rate,\n            encoder_attn_layer_type=args.transformer_encoder_attn_layer_type,\n            macaron_style=args.macaron_style,\n            use_cnn_module=args.use_cnn_module,\n            cnn_module_kernel=args.cnn_module_kernel,\n            zero_triu=getattr(args, \"zero_triu\", False),\n            a_upsample_ratio=args.a_upsample_ratio,\n            relu_type=getattr(args, \"relu_type\", \"swish\"),\n        )\n\n        self.transformer_input_layer = args.transformer_input_layer\n        self.a_upsample_ratio = args.a_upsample_ratio\n\n        self.proj_decoder = None\n        if args.adim != args.ddim:\n            self.proj_decoder = torch.nn.Linear(args.adim, args.ddim)\n\n        if args.mtlalpha < 1:\n            self.decoder = Decoder(\n                odim=odim,\n                attention_dim=args.ddim,\n                attention_heads=args.dheads,\n                linear_units=args.dunits,\n                num_blocks=args.dlayers,\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n                src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n            )\n        else:\n            self.decoder = None\n        self.blank = 0\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.odim = odim\n        self.ignore_id = ignore_id\n\n        # self.lsm_weight = a\n        self.criterion = LabelSmoothingLoss(\n            self.odim,\n            self.ignore_id,\n            args.lsm_weight,\n            args.transformer_length_normalized_loss,\n        )\n\n        self.adim = args.adim\n        self.mtlalpha = args.mtlalpha\n        if args.mtlalpha > 0.0:\n            self.ctc = CTC(\n                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n            )\n        else:\n            self.ctc = None\n\n    def forward(self, x, lengths, label):\n        if self.transformer_input_layer == \"conv1d\":\n            lengths = torch.div(lengths, 640, rounding_mode=\"trunc\")\n        padding_mask = make_non_pad_mask(lengths).to(x.device).unsqueeze(-2)\n\n        x, _ = self.encoder(x, padding_mask)\n\n        # ctc loss\n        loss_ctc, ys_hat = self.ctc(x, lengths, label)\n\n        if self.proj_decoder:\n            x = self.proj_decoder(x)\n\n        # decoder loss\n        ys_in_pad, ys_out_pad = add_sos_eos(label, self.sos, self.eos, self.ignore_id)\n        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n        pred_pad, _ = self.decoder(ys_in_pad, ys_mask, x, padding_mask)\n        loss_att = self.criterion(pred_pad, ys_out_pad)\n        loss = self.mtlalpha * loss_ctc + (1 - self.mtlalpha) * loss_att\n\n        acc = th_accuracy(\n            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n        )\n\n        return loss, loss_ctc, loss_att, acc","metadata":{"id":"aRhg5iO508ZX","execution":{"iopub.status.busy":"2024-04-17T01:56:32.277019Z","iopub.execute_input":"2024-04-17T01:56:32.277598Z","iopub.status.idle":"2024-04-17T01:56:32.297312Z","shell.execute_reply.started":"2024-04-17T01:56:32.277542Z","shell.execute_reply":"2024-04-17T01:56:32.296133Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Cosine","metadata":{"id":"3nQEmdFqNcjT"}},{"cell_type":"code","source":"class WarmupCosineScheduler(_LRScheduler):\n    def __init__(self, optimizer, warmup_epochs, num_epochs, iter_per_epoch):\n        self.base_lrs = {\n            param_group[\"name\"]: param_group[\"lr\"]\n            for param_group in optimizer.param_groups\n        }\n        self.warmup_iter = warmup_epochs * iter_per_epoch\n        self.total_iter = num_epochs * iter_per_epoch\n        self.optimizer = optimizer\n        self.iter = 0\n        self.current_lr = 0\n\n        self.init_lr()  # so that at first step we have the correct step size\n\n    def get_lr(self, base_lr):\n        if self.iter < self.warmup_iter:\n            return base_lr * self.iter / self.warmup_iter\n        else:\n            decay_iter = self.total_iter - self.warmup_iter\n            return (\n                0.5\n                * base_lr\n                * (1 + np.cos(np.pi * (self.iter - self.warmup_iter) / decay_iter))\n            )\n\n    def update_param_groups(self):\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = self.get_lr(self.base_lrs[param_group[\"name\"]])\n\n    def step(self):\n        self.update_param_groups()\n        self.iter += 1\n\n    def init_lr(self):\n        self.update_param_groups()","metadata":{"id":"SdB5dFVWNfH9","execution":{"iopub.status.busy":"2024-04-17T01:56:33.938087Z","iopub.execute_input":"2024-04-17T01:56:33.939300Z","iopub.status.idle":"2024-04-17T01:56:33.950543Z","shell.execute_reply.started":"2024-04-17T01:56:33.939246Z","shell.execute_reply":"2024-04-17T01:56:33.949437Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## CTCPrefixScorer","metadata":{"id":"5bvueiRMObJw"}},{"cell_type":"code","source":"class CTCPrefixScoreTH(object):\n    \"\"\"Batch processing of CTCPrefixScore\n\n    which is based on Algorithm 2 in WATANABE et al.\n    \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\"\n    but extended to efficiently compute the label probablities for multiple\n    hypotheses simultaneously\n    See also Seki et al. \"Vectorized Beam Search for CTC-Attention-Based\n    Speech Recognition,\" In INTERSPEECH (pp. 3825-3829), 2019.\n    \"\"\"\n\n    def __init__(self, x, xlens, blank, eos, margin=0):\n        \"\"\"Construct CTC prefix scorer\n\n        :param torch.Tensor x: input label posterior sequences (B, T, O)\n        :param torch.Tensor xlens: input lengths (B,)\n        :param int blank: blank label id\n        :param int eos: end-of-sequence id\n        :param int margin: margin parameter for windowing (0 means no windowing)\n        \"\"\"\n        # In the comment lines,\n        # we assume T: input_length, B: batch size, W: beam width, O: output dim.\n        self.logzero = -10000000000.0\n        self.blank = blank\n        self.eos = eos\n        self.batch = x.size(0)\n        self.input_length = x.size(1)\n        self.odim = x.size(2)\n        self.dtype = x.dtype\n        self.device = (\n            torch.device(\"cuda:%d\" % x.get_device())\n            if x.is_cuda\n            else torch.device(\"cpu\")\n        )\n        # Pad the rest of posteriors in the batch\n        # TODO(takaaki-hori): need a better way without for-loops\n        for i, l in enumerate(xlens):\n            if l < self.input_length:\n                x[i, l:, :] = self.logzero\n                x[i, l:, blank] = 0\n        # Reshape input x\n        xn = x.transpose(0, 1)  # (B, T, O) -> (T, B, O)\n        xb = xn[:, :, self.blank].unsqueeze(2).expand(-1, -1, self.odim)\n        self.x = torch.stack([xn, xb])  # (2, T, B, O)\n        self.end_frames = torch.as_tensor(xlens) - 1\n\n        # Setup CTC windowing\n        self.margin = margin\n        if margin > 0:\n            self.frame_ids = torch.arange(\n                self.input_length, dtype=self.dtype, device=self.device\n            )\n        # Base indices for index conversion\n        self.idx_bh = None\n        self.idx_b = torch.arange(self.batch, device=self.device)\n        self.idx_bo = (self.idx_b * self.odim).unsqueeze(1)\n\n    def __call__(self, y, state, scoring_ids=None, att_w=None):\n        \"\"\"Compute CTC prefix scores for next labels\n\n        :param list y: prefix label sequences\n        :param tuple state: previous CTC state\n        :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O)\n        :param torch.Tensor att_w: attention weights to decide CTC window\n        :return new_state, ctc_local_scores (BW, O)\n        \"\"\"\n        output_length = len(y[0]) - 1  # ignore sos\n        last_ids = [yi[-1] for yi in y]  # last output label ids\n        n_bh = len(last_ids)  # batch * hyps\n        n_hyps = n_bh // self.batch  # assuming each utterance has the same # of hyps\n        self.scoring_num = scoring_ids.size(-1) if scoring_ids is not None else 0\n        # prepare state info\n        if state is None:\n            r_prev = torch.full(\n                (self.input_length, 2, self.batch, n_hyps),\n                self.logzero,\n                dtype=self.dtype,\n                device=self.device,\n            )\n            r_prev[:, 1] = torch.cumsum(self.x[0, :, :, self.blank], 0).unsqueeze(2)\n            r_prev = r_prev.view(-1, 2, n_bh)\n            s_prev = 0.0\n            f_min_prev = 0\n            f_max_prev = 1\n        else:\n            r_prev, s_prev, f_min_prev, f_max_prev = state\n\n        # select input dimensions for scoring\n        if self.scoring_num > 0:\n            scoring_idmap = torch.full(\n                (n_bh, self.odim), -1, dtype=torch.long, device=self.device\n            )\n            snum = self.scoring_num\n            if self.idx_bh is None or n_bh > len(self.idx_bh):\n                self.idx_bh = torch.arange(n_bh, device=self.device).view(-1, 1)\n            scoring_idmap[self.idx_bh[:n_bh], scoring_ids] = torch.arange(\n                snum, device=self.device\n            )\n            scoring_idx = (\n                scoring_ids + self.idx_bo.repeat(1, n_hyps).view(-1, 1)\n            ).view(-1)\n            x_ = torch.index_select(\n                self.x.view(2, -1, self.batch * self.odim), 2, scoring_idx\n            ).view(2, -1, n_bh, snum)\n        else:\n            scoring_ids = None\n            scoring_idmap = None\n            snum = self.odim\n            x_ = self.x.unsqueeze(3).repeat(1, 1, 1, n_hyps, 1).view(2, -1, n_bh, snum)\n\n        # new CTC forward probs are prepared as a (T x 2 x BW x S) tensor\n        # that corresponds to r_t^n(h) and r_t^b(h) in a batch.\n        r = torch.full(\n            (self.input_length, 2, n_bh, snum),\n            self.logzero,\n            dtype=self.dtype,\n            device=self.device,\n        )\n        if output_length == 0:\n            r[0, 0] = x_[0, 0]\n\n        r_sum = torch.logsumexp(r_prev, 1)\n        log_phi = r_sum.unsqueeze(2).repeat(1, 1, snum)\n        if scoring_ids is not None:\n            for idx in range(n_bh):\n                pos = scoring_idmap[idx, last_ids[idx]]\n                if pos >= 0:\n                    log_phi[:, idx, pos] = r_prev[:, 1, idx]\n        else:\n            for idx in range(n_bh):\n                log_phi[:, idx, last_ids[idx]] = r_prev[:, 1, idx]\n\n        # decide start and end frames based on attention weights\n        if att_w is not None and self.margin > 0:\n            f_arg = torch.matmul(att_w, self.frame_ids)\n            f_min = max(int(f_arg.min().cpu()), f_min_prev)\n            f_max = max(int(f_arg.max().cpu()), f_max_prev)\n            start = min(f_max_prev, max(f_min - self.margin, output_length, 1))\n            end = min(f_max + self.margin, self.input_length)\n        else:\n            f_min = f_max = 0\n            start = max(output_length, 1)\n            end = self.input_length\n\n        # compute forward probabilities log(r_t^n(h)) and log(r_t^b(h))\n        for t in range(start, end):\n            rp = r[t - 1]\n            rr = torch.stack([rp[0], log_phi[t - 1], rp[0], rp[1]]).view(\n                2, 2, n_bh, snum\n            )\n            r[t] = torch.logsumexp(rr, 1) + x_[:, t]\n\n        # compute log prefix probabilities log(psi)\n        log_phi_x = torch.cat((log_phi[0].unsqueeze(0), log_phi[:-1]), dim=0) + x_[0]\n        if scoring_ids is not None:\n            log_psi = torch.full(\n                (n_bh, self.odim), self.logzero, dtype=self.dtype, device=self.device\n            )\n            log_psi_ = torch.logsumexp(\n                torch.cat((log_phi_x[start:end], r[start - 1, 0].unsqueeze(0)), dim=0),\n                dim=0,\n            )\n            for si in range(n_bh):\n                log_psi[si, scoring_ids[si]] = log_psi_[si]\n        else:\n            log_psi = torch.logsumexp(\n                torch.cat((log_phi_x[start:end], r[start - 1, 0].unsqueeze(0)), dim=0),\n                dim=0,\n            )\n\n        for si in range(n_bh):\n            log_psi[si, self.eos] = r_sum[self.end_frames[si // n_hyps], si]\n\n        # exclude blank probs\n        log_psi[:, self.blank] = self.logzero\n\n        return (log_psi - s_prev), (r, log_psi, f_min, f_max, scoring_idmap)\n\n    def index_select_state(self, state, best_ids):\n        \"\"\"Select CTC states according to best ids\n\n        :param state    : CTC state\n        :param best_ids : index numbers selected by beam pruning (B, W)\n        :return selected_state\n        \"\"\"\n        r, s, f_min, f_max, scoring_idmap = state\n        # convert ids to BHO space\n        n_bh = len(s)\n        n_hyps = n_bh // self.batch\n        vidx = (best_ids + (self.idx_b * (n_hyps * self.odim)).view(-1, 1)).view(-1)\n        # select hypothesis scores\n        s_new = torch.index_select(s.view(-1), 0, vidx)\n        s_new = s_new.view(-1, 1).repeat(1, self.odim).view(n_bh, self.odim)\n        # convert ids to BHS space (S: scoring_num)\n        if scoring_idmap is not None:\n            snum = self.scoring_num\n            hyp_idx = (best_ids // self.odim + (self.idx_b * n_hyps).view(-1, 1)).view(\n                -1\n            )\n            label_ids = torch.fmod(best_ids, self.odim).view(-1)\n            score_idx = scoring_idmap[hyp_idx, label_ids]\n            score_idx[score_idx == -1] = 0\n            vidx = score_idx + hyp_idx * snum\n        else:\n            snum = self.odim\n        # select forward probabilities\n        r_new = torch.index_select(r.view(-1, 2, n_bh * snum), 2, vidx).view(\n            -1, 2, n_bh\n        )\n        return r_new, s_new, f_min, f_max\n\n    def extend_prob(self, x):\n        \"\"\"Extend CTC prob.\n\n        :param torch.Tensor x: input label posterior sequences (B, T, O)\n        \"\"\"\n\n        if self.x.shape[1] < x.shape[1]:  # self.x (2,T,B,O); x (B,T,O)\n            # Pad the rest of posteriors in the batch\n            # TODO(takaaki-hori): need a better way without for-loops\n            xlens = [x.size(1)]\n            for i, l in enumerate(xlens):\n                if l < self.input_length:\n                    x[i, l:, :] = self.logzero\n                    x[i, l:, self.blank] = 0\n            tmp_x = self.x\n            xn = x.transpose(0, 1)  # (B, T, O) -> (T, B, O)\n            xb = xn[:, :, self.blank].unsqueeze(2).expand(-1, -1, self.odim)\n            self.x = torch.stack([xn, xb])  # (2, T, B, O)\n            self.x[:, : tmp_x.shape[1], :, :] = tmp_x\n            self.input_length = x.size(1)\n            self.end_frames = torch.as_tensor(xlens) - 1\n\n    def extend_state(self, state):\n        \"\"\"Compute CTC prefix state.\n\n\n        :param state    : CTC state\n        :return ctc_state\n        \"\"\"\n\n        if state is None:\n            # nothing to do\n            return state\n        else:\n            r_prev, s_prev, f_min_prev, f_max_prev = state\n\n            r_prev_new = torch.full(\n                (self.input_length, 2),\n                self.logzero,\n                dtype=self.dtype,\n                device=self.device,\n            )\n            start = max(r_prev.shape[0], 1)\n            r_prev_new[0:start] = r_prev\n            for t in six.moves.range(start, self.input_length):\n                r_prev_new[t, 1] = r_prev_new[t - 1, 1] + self.x[0, t, :, self.blank]\n\n            return (r_prev_new, s_prev, f_min_prev, f_max_prev)\n\n\nclass CTCPrefixScore(object):\n    \"\"\"Compute CTC label sequence scores\n\n    which is based on Algorithm 2 in WATANABE et al.\n    \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\"\n    but extended to efficiently compute the probablities of multiple labels\n    simultaneously\n    \"\"\"\n\n    def __init__(self, x, blank, eos, xp):\n        self.xp = xp\n        self.logzero = -10000000000.0\n        self.blank = blank\n        self.eos = eos\n        self.input_length = len(x)\n        self.x = x\n\n    def initial_state(self):\n        \"\"\"Obtain an initial CTC state\n\n        :return: CTC state\n        \"\"\"\n        # initial CTC state is made of a frame x 2 tensor that corresponds to\n        # r_t^n(<sos>) and r_t^b(<sos>), where 0 and 1 of axis=1 represent\n        # superscripts n and b (non-blank and blank), respectively.\n        r = self.xp.full((self.input_length, 2), self.logzero, dtype=np.float32)\n        r[0, 1] = self.x[0, self.blank]\n        for i in six.moves.range(1, self.input_length):\n            r[i, 1] = r[i - 1, 1] + self.x[i, self.blank]\n        return r\n\n    def __call__(self, y, cs, r_prev):\n        \"\"\"Compute CTC prefix scores for next labels\n\n        :param y     : prefix label sequence\n        :param cs    : array of next labels\n        :param r_prev: previous CTC state\n        :return ctc_scores, ctc_states\n        \"\"\"\n        # initialize CTC states\n        output_length = len(y) - 1  # ignore sos\n        # new CTC states are prepared as a frame x (n or b) x n_labels tensor\n        # that corresponds to r_t^n(h) and r_t^b(h).\n        r = self.xp.ndarray((self.input_length, 2, len(cs)), dtype=np.float32)\n        xs = self.x[:, cs]\n        if output_length == 0:\n            r[0, 0] = xs[0]\n            r[0, 1] = self.logzero\n        else:\n            r[output_length - 1] = self.logzero\n\n        # prepare forward probabilities for the last label\n        r_sum = self.xp.logaddexp(\n            r_prev[:, 0], r_prev[:, 1]\n        )  # log(r_t^n(g) + r_t^b(g))\n        last = y[-1]\n        if output_length > 0 and last in cs:\n            log_phi = self.xp.ndarray((self.input_length, len(cs)), dtype=np.float32)\n            for i in six.moves.range(len(cs)):\n                log_phi[:, i] = r_sum if cs[i] != last else r_prev[:, 1]\n        else:\n            log_phi = r_sum\n\n        # compute forward probabilities log(r_t^n(h)), log(r_t^b(h)),\n        # and log prefix probabilities log(psi)\n        start = max(output_length, 1)\n        log_psi = r[start - 1, 0]\n        for t in six.moves.range(start, self.input_length):\n            r[t, 0] = self.xp.logaddexp(r[t - 1, 0], log_phi[t - 1]) + xs[t]\n            r[t, 1] = (\n                self.xp.logaddexp(r[t - 1, 0], r[t - 1, 1]) + self.x[t, self.blank]\n            )\n            log_psi = self.xp.logaddexp(log_psi, log_phi[t - 1] + xs[t])\n\n        # get P(...eos|X) that ends with the prefix itself\n        eos_pos = self.xp.where(cs == self.eos)[0]\n        if len(eos_pos) > 0:\n            log_psi[eos_pos] = r_sum[-1]  # log(r_T^n(g) + r_T^b(g))\n\n        # exclude blank probs\n        blank_pos = self.xp.where(cs == self.blank)[0]\n        if len(blank_pos) > 0:\n            log_psi[blank_pos] = self.logzero\n\n        # return the log prefix probability and CTC states, where the label axis\n        # of the CTC states is moved to the first axis to slice it easily\n        return log_psi, self.xp.rollaxis(r, 2)","metadata":{"id":"pyvw0k0AQCTS","execution":{"iopub.status.busy":"2024-04-17T01:56:35.423539Z","iopub.execute_input":"2024-04-17T01:56:35.423947Z","iopub.status.idle":"2024-04-17T01:56:35.884573Z","shell.execute_reply.started":"2024-04-17T01:56:35.423912Z","shell.execute_reply":"2024-04-17T01:56:35.883433Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class CTCPrefixScorer(BatchPartialScorerInterface):\n    \"\"\"Decoder interface wrapper for CTCPrefixScore.\"\"\"\n\n    def __init__(self, ctc: torch.nn.Module, eos: int):\n        \"\"\"Initialize class.\n\n        Args:\n            ctc (torch.nn.Module): The CTC implementation.\n                For example, :class:`espnet.nets.pytorch_backend.ctc.CTC`\n            eos (int): The end-of-sequence id.\n\n        \"\"\"\n        self.ctc = ctc\n        self.eos = eos\n        self.impl = None\n\n    def init_state(self, x: torch.Tensor):\n        \"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"\n        logp = self.ctc.log_softmax(x.unsqueeze(0)).detach().squeeze(0).cpu().numpy()\n        # TODO(karita): use CTCPrefixScoreTH\n        self.impl = CTCPrefixScore(logp, 0, self.eos, np)\n        return 0, self.impl.initial_state()\n\n    def select_state(self, state, i, new_id=None):\n        \"\"\"Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n            new_id (int): New label id to select a state if necessary\n\n        Returns:\n            state: pruned state\n\n        \"\"\"\n        if type(state) == tuple:\n            if len(state) == 2:  # for CTCPrefixScore\n                sc, st = state\n                return sc[i], st[i]\n            else:  # for CTCPrefixScoreTH (need new_id > 0)\n                r, log_psi, f_min, f_max, scoring_idmap = state\n                s = log_psi[i, new_id].expand(log_psi.size(1))\n                if scoring_idmap is not None:\n                    return r[:, :, i, scoring_idmap[i, new_id]], s, f_min, f_max\n                else:\n                    return r[:, :, i, new_id], s, f_min, f_max\n        return None if state is None else state[i]\n\n    def score_partial(self, y, ids, state, x):\n        \"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            next_tokens (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"\n        prev_score, state = state\n        presub_score, new_st = self.impl(y.cpu(), ids.cpu(), state)\n        tscore = torch.as_tensor(\n            presub_score - prev_score, device=x.device, dtype=x.dtype\n        )\n        return tscore, (presub_score, new_st)\n\n    def batch_init_state(self, x: torch.Tensor):\n        \"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"\n        logp = self.ctc.log_softmax(x.unsqueeze(0))  # assuming batch_size = 1\n        xlen = torch.tensor([logp.size(1)])\n        self.impl = CTCPrefixScoreTH(logp, xlen, 0, self.eos)\n        return None\n\n    def batch_score_partial(self, y, ids, state, x):\n        \"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            ids (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"\n        batch_state = (\n            (\n                torch.stack([s[0] for s in state], dim=2),\n                torch.stack([s[1] for s in state]),\n                state[0][2],\n                state[0][3],\n            )\n            if state[0] is not None\n            else None\n        )\n        return self.impl(y, batch_state, ids)\n\n    def extend_prob(self, x: torch.Tensor):\n        \"\"\"Extend probs for decoding.\n\n        This extension is for streaming decoding\n        as in Eq (14) in https://arxiv.org/abs/2006.14941\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        \"\"\"\n        logp = self.ctc.log_softmax(x.unsqueeze(0))\n        self.impl.extend_prob(logp)\n\n    def extend_state(self, state):\n        \"\"\"Extend state for decoding.\n\n        This extension is for streaming decoding\n        as in Eq (14) in https://arxiv.org/abs/2006.14941\n\n        Args:\n            state: The states of hyps\n\n        Returns: exteded state\n\n        \"\"\"\n        new_state = []\n        for s in state:\n            new_state.append(self.impl.extend_state(s))\n\n        return new_state","metadata":{"id":"V0LFm_JLOcD4","execution":{"iopub.status.busy":"2024-04-17T01:56:36.239275Z","iopub.execute_input":"2024-04-17T01:56:36.240102Z","iopub.status.idle":"2024-04-17T01:56:36.261745Z","shell.execute_reply.started":"2024-04-17T01:56:36.240065Z","shell.execute_reply":"2024-04-17T01:56:36.260535Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## LengthBonus","metadata":{"id":"gZ980wJWQULi"}},{"cell_type":"code","source":"class LengthBonus(BatchScorerInterface):\n    \"\"\"Length bonus in beam search.\"\"\"\n\n    def __init__(self, n_vocab: int):\n        \"\"\"Initialize class.\n\n        Args:\n            n_vocab (int): The number of tokens in vocabulary for beam search\n\n        \"\"\"\n        self.n = n_vocab\n\n    def score(self, y, state, x):\n        \"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and None\n\n        \"\"\"\n        return torch.tensor([1.0], device=x.device, dtype=x.dtype).expand(self.n), None\n\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        \"\"\"Score new token batch.\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        \"\"\"\n        return (\n            torch.tensor([1.0], device=xs.device, dtype=xs.dtype).expand(\n                ys.shape[0], self.n\n            ),\n            None,\n        )","metadata":{"id":"qE_RPRxTQVED","execution":{"iopub.status.busy":"2024-04-17T01:56:37.816764Z","iopub.execute_input":"2024-04-17T01:56:37.817648Z","iopub.status.idle":"2024-04-17T01:56:37.826840Z","shell.execute_reply.started":"2024-04-17T01:56:37.817611Z","shell.execute_reply":"2024-04-17T01:56:37.825782Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## BeamSearch","metadata":{"id":"0nHmNDnWQxcu"}},{"cell_type":"code","source":"def end_detect(ended_hyps, i, M=3, D_end=np.log(1 * np.exp(-10))):\n    \"\"\"End detection.\n\n    described in Eq. (50) of S. Watanabe et al\n    \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\"\n\n    :param ended_hyps:\n    :param i:\n    :param M:\n    :param D_end:\n    :return:\n    \"\"\"\n    if len(ended_hyps) == 0:\n        return False\n    count = 0\n    best_hyp = sorted(ended_hyps, key=lambda x: x[\"score\"], reverse=True)[0]\n    for m in range(M):\n        # get ended_hyps with their length is i - m\n        hyp_length = i - m\n        hyps_same_length = [x for x in ended_hyps if len(x[\"yseq\"]) == hyp_length]\n        if len(hyps_same_length) > 0:\n            best_hyp_same_length = sorted(\n                hyps_same_length, key=lambda x: x[\"score\"], reverse=True\n            )[0]\n            if best_hyp_same_length[\"score\"] - best_hyp[\"score\"] < D_end:\n                count += 1\n\n    if count == M:\n        return True\n    else:\n        return False\n\n\nclass Hypothesis(NamedTuple):\n    \"\"\"Hypothesis data type.\"\"\"\n\n    yseq: torch.Tensor\n    score: Union[float, torch.Tensor] = 0\n    scores: Dict[str, Union[float, torch.Tensor]] = dict()\n    states: Dict[str, Any] = dict()\n\n    def asdict(self) -> dict:\n        \"\"\"Convert data to JSON-friendly dict.\"\"\"\n        return self._replace(\n            yseq=self.yseq.tolist(),\n            score=float(self.score),\n            scores={k: float(v) for k, v in self.scores.items()},\n        )._asdict()\n\n\nclass BeamSearch(torch.nn.Module):\n    \"\"\"Beam search implementation.\"\"\"\n\n    def __init__(\n        self,\n        scorers: Dict[str, ScorerInterface],\n        weights: Dict[str, float],\n        beam_size: int,\n        vocab_size: int,\n        sos: int,\n        eos: int,\n        token_list: List[str] = None,\n        pre_beam_ratio: float = 1.5,\n        pre_beam_score_key: str = None,\n    ):\n        \"\"\"Initialize beam search.\n\n        Args:\n            scorers (dict[str, ScorerInterface]): Dict of decoder modules\n                e.g., Decoder, CTCPrefixScorer, LM\n                The scorer will be ignored if it is `None`\n            weights (dict[str, float]): Dict of weights for each scorers\n                The scorer will be ignored if its weight is 0\n            beam_size (int): The number of hypotheses kept during search\n            vocab_size (int): The number of vocabulary\n            sos (int): Start of sequence id\n            eos (int): End of sequence id\n            token_list (list[str]): List of tokens for debug log\n            pre_beam_score_key (str): key of scores to perform pre-beam search\n            pre_beam_ratio (float): beam size in the pre-beam search\n                will be `int(pre_beam_ratio * beam_size)`\n\n        \"\"\"\n        super().__init__()\n        # set scorers\n        self.weights = weights\n        self.scorers = dict()\n        self.full_scorers = dict()\n        self.part_scorers = dict()\n        # this module dict is required for recursive cast\n        # `self.to(device, dtype)` in `recog.py`\n        self.nn_dict = torch.nn.ModuleDict()\n        for k, v in scorers.items():\n            w = weights.get(k, 0)\n            if w == 0 or v is None:\n                continue\n            assert isinstance(\n                v, ScorerInterface\n            ), f\"{k} ({type(v)}) does not implement ScorerInterface\"\n            self.scorers[k] = v\n            if isinstance(v, PartialScorerInterface):\n                self.part_scorers[k] = v\n            else:\n                self.full_scorers[k] = v\n            if isinstance(v, torch.nn.Module):\n                self.nn_dict[k] = v\n\n        # set configurations\n        self.sos = sos\n        self.eos = eos\n        self.token_list = token_list\n        self.pre_beam_size = int(pre_beam_ratio * beam_size)\n        self.beam_size = beam_size\n        self.n_vocab = vocab_size\n        if (\n            pre_beam_score_key is not None\n            and pre_beam_score_key != \"full\"\n            and pre_beam_score_key not in self.full_scorers\n        ):\n            raise KeyError(f\"{pre_beam_score_key} is not found in {self.full_scorers}\")\n        self.pre_beam_score_key = pre_beam_score_key\n        self.do_pre_beam = (\n            self.pre_beam_score_key is not None\n            and self.pre_beam_size < self.n_vocab\n            and len(self.part_scorers) > 0\n        )\n\n    def init_hyp(self, x: torch.Tensor) -> List[Hypothesis]:\n        \"\"\"Get an initial hypothesis data.\n\n        Args:\n            x (torch.Tensor): The encoder output feature\n\n        Returns:\n            Hypothesis: The initial hypothesis.\n\n        \"\"\"\n        init_states = dict()\n        init_scores = dict()\n        for k, d in self.scorers.items():\n            init_states[k] = d.init_state(x)\n            init_scores[k] = 0.0\n        return [\n            Hypothesis(\n                score=0.0,\n                scores=init_scores,\n                states=init_states,\n                yseq=torch.tensor([self.sos], device=x.device),\n            )\n        ]\n\n    @staticmethod\n    def append_token(xs: torch.Tensor, x: int) -> torch.Tensor:\n        \"\"\"Append new token to prefix tokens.\n\n        Args:\n            xs (torch.Tensor): The prefix token\n            x (int): The new token to append\n\n        Returns:\n            torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device\n\n        \"\"\"\n        x = torch.tensor([x], dtype=xs.dtype, device=xs.device)\n        return torch.cat((xs, x))\n\n    def score_full(\n        self, hyp: Hypothesis, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        \"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"\n        scores = dict()\n        states = dict()\n        for k, d in self.full_scorers.items():\n            scores[k], states[k] = d.score(hyp.yseq, hyp.states[k], x)\n        return scores, states\n\n    def score_partial(\n        self, hyp: Hypothesis, ids: torch.Tensor, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        \"\"\"Score new hypothesis by `self.part_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            ids (torch.Tensor): 1D tensor of new partial tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.part_scorers`\n                and tensor score values of shape: `(len(ids),)`,\n                and state dict that has string keys\n                and state values of `self.part_scorers`\n\n        \"\"\"\n        scores = dict()\n        states = dict()\n        for k, d in self.part_scorers.items():\n            scores[k], states[k] = d.score_partial(hyp.yseq, ids, hyp.states[k], x)\n        return scores, states\n\n    def beam(\n        self, weighted_scores: torch.Tensor, ids: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute topk full token ids and partial token ids.\n\n        Args:\n            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n            Its shape is `(self.n_vocab,)`.\n            ids (torch.Tensor): The partial token ids to compute topk\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]:\n                The topk full token ids and partial token ids.\n                Their shapes are `(self.beam_size,)`\n\n        \"\"\"\n        # no pre beam performed\n        if weighted_scores.size(0) == ids.size(0):\n            top_ids = weighted_scores.topk(self.beam_size)[1]\n            return top_ids, top_ids\n\n        # mask pruned in pre-beam not to select in topk\n        tmp = weighted_scores[ids]\n        weighted_scores[:] = -float(\"inf\")\n        weighted_scores[ids] = tmp\n        top_ids = weighted_scores.topk(self.beam_size)[1]\n        local_ids = weighted_scores[ids].topk(self.beam_size)[1]\n        return top_ids, local_ids\n\n    @staticmethod\n    def merge_scores(\n        prev_scores: Dict[str, float],\n        next_full_scores: Dict[str, torch.Tensor],\n        full_idx: int,\n        next_part_scores: Dict[str, torch.Tensor],\n        part_idx: int,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Merge scores for new hypothesis.\n\n        Args:\n            prev_scores (Dict[str, float]):\n                The previous hypothesis scores by `self.scorers`\n            next_full_scores (Dict[str, torch.Tensor]): scores by `self.full_scorers`\n            full_idx (int): The next token id for `next_full_scores`\n            next_part_scores (Dict[str, torch.Tensor]):\n                scores of partial tokens by `self.part_scorers`\n            part_idx (int): The new token id for `next_part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are scalar tensors by the scorers.\n\n        \"\"\"\n        new_scores = dict()\n        for k, v in next_full_scores.items():\n            new_scores[k] = prev_scores[k] + v[full_idx]\n        for k, v in next_part_scores.items():\n            new_scores[k] = prev_scores[k] + v[part_idx]\n        return new_scores\n\n    def merge_states(self, states: Any, part_states: Any, part_idx: int) -> Any:\n        \"\"\"Merge states for new hypothesis.\n\n        Args:\n            states: states of `self.full_scorers`\n            part_states: states of `self.part_scorers`\n            part_idx (int): The new token id for `part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are states of the scorers.\n\n        \"\"\"\n        new_states = dict()\n        for k, v in states.items():\n            new_states[k] = v\n        for k, d in self.part_scorers.items():\n            new_states[k] = d.select_state(part_states[k], part_idx)\n        return new_states\n\n    def search(\n        self, running_hyps: List[Hypothesis], x: torch.Tensor\n    ) -> List[Hypothesis]:\n        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n\n        Args:\n            running_hyps (List[Hypothesis]): Running hypotheses on beam\n            x (torch.Tensor): Encoded speech feature (T, D)\n\n        Returns:\n            List[Hypotheses]: Best sorted hypotheses\n\n        \"\"\"\n        best_hyps = []\n        part_ids = torch.arange(self.n_vocab, device=x.device)  # no pre-beam\n        for hyp in running_hyps:\n            # scoring\n            weighted_scores = torch.zeros(self.n_vocab, dtype=x.dtype, device=x.device)\n            scores, states = self.score_full(hyp, x)\n            for k in self.full_scorers:\n                weighted_scores += self.weights[k] * scores[k]\n            # partial scoring\n            if self.do_pre_beam:\n                pre_beam_scores = (\n                    weighted_scores\n                    if self.pre_beam_score_key == \"full\"\n                    else scores[self.pre_beam_score_key]\n                )\n                part_ids = torch.topk(pre_beam_scores, self.pre_beam_size)[1]\n            part_scores, part_states = self.score_partial(hyp, part_ids, x)\n            for k in self.part_scorers:\n                weighted_scores[part_ids] += self.weights[k] * part_scores[k]\n            # add previous hyp score\n            weighted_scores += hyp.score\n\n            # update hyps\n            for j, part_j in zip(*self.beam(weighted_scores, part_ids)):\n                # will be (2 x beam at most)\n                best_hyps.append(\n                    Hypothesis(\n                        score=weighted_scores[j],\n                        yseq=self.append_token(hyp.yseq, j),\n                        scores=self.merge_scores(\n                            hyp.scores, scores, j, part_scores, part_j\n                        ),\n                        states=self.merge_states(states, part_states, part_j),\n                    )\n                )\n\n            # sort and prune 2 x beam -> beam\n            best_hyps = sorted(best_hyps, key=lambda x: x.score, reverse=True)[\n                : min(len(best_hyps), self.beam_size)\n            ]\n        return best_hyps\n\n    def forward(\n        self, x: torch.Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0\n    ) -> List[Hypothesis]:\n        \"\"\"Perform beam search.\n\n        Args:\n            x (torch.Tensor): Encoded speech feature (T, D)\n            maxlenratio (float): Input length ratio to obtain max output length.\n                If maxlenratio=0.0 (default), it uses a end-detect function\n                to automatically find maximum hypothesis lengths\n                If maxlenratio<0.0, its absolute value is interpreted\n                as a constant max output length.\n            minlenratio (float): Input length ratio to obtain min output length.\n\n        Returns:\n            list[Hypothesis]: N-best decoding results\n\n        \"\"\"\n        # set length bounds\n        if maxlenratio == 0:\n            maxlen = x.shape[0]\n        elif maxlenratio < 0:\n            maxlen = -1 * int(maxlenratio)\n        else:\n            maxlen = max(1, int(maxlenratio * x.size(0)))\n        minlen = int(minlenratio * x.size(0))\n        logging.debug(\"decoder input length: \" + str(x.shape[0]))\n        logging.debug(\"max output length: \" + str(maxlen))\n        logging.debug(\"min output length: \" + str(minlen))\n\n        # main loop of prefix search\n        running_hyps = self.init_hyp(x)\n        ended_hyps = []\n        for i in range(maxlen):\n            logging.debug(\"position \" + str(i))\n            best = self.search(running_hyps, x)\n            # post process of one iteration\n            running_hyps = self.post_process(i, maxlen, maxlenratio, best, ended_hyps)\n            # end detection\n            if maxlenratio == 0.0 and end_detect([h.asdict() for h in ended_hyps], i):\n                logging.debug(f\"end detected at {i}\")\n                break\n            if len(running_hyps) == 0:\n                logging.debug(\"no hypothesis. Finish decoding.\")\n                break\n            else:\n                logging.debug(f\"remained hypotheses: {len(running_hyps)}\")\n\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x.score, reverse=True)\n        # check the number of hypotheses reaching to eos\n        if len(nbest_hyps) == 0:\n            logging.warning(\n                \"there is no N-best results, perform recognition \"\n                \"again with smaller minlenratio.\"\n            )\n            return (\n                []\n                if minlenratio < 0.1\n                else self.forward(x, maxlenratio, max(0.0, minlenratio - 0.1))\n            )\n\n        # report the best result\n        best = nbest_hyps[0]\n        for k, v in best.scores.items():\n            logging.debug(\n                f\"{v:6.2f} * {self.weights[k]:3} = {v * self.weights[k]:6.2f} for {k}\"\n            )\n        logging.debug(f\"total log probability: {best.score:.2f}\")\n        logging.debug(f\"normalized log probability: {best.score / len(best.yseq):.2f}\")\n        logging.debug(f\"total number of ended hypotheses: {len(nbest_hyps)}\")\n        if self.token_list is not None:\n            logging.debug(\n                \"best hypo: \"\n                + \"\".join([self.token_list[x] for x in best.yseq[1:-1]])\n                + \"\\n\"\n            )\n        return nbest_hyps\n\n    def post_process(\n        self,\n        i: int,\n        maxlen: int,\n        maxlenratio: float,\n        running_hyps: List[Hypothesis],\n        ended_hyps: List[Hypothesis],\n    ) -> List[Hypothesis]:\n        \"\"\"Perform post-processing of beam search iterations.\n\n        Args:\n            i (int): The length of hypothesis tokens.\n            maxlen (int): The maximum length of tokens in beam search.\n            maxlenratio (int): The maximum length ratio in beam search.\n            running_hyps (List[Hypothesis]): The running hypotheses in beam search.\n            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n\n        Returns:\n            List[Hypothesis]: The new running hypotheses.\n\n        \"\"\"\n        logging.debug(f\"the number of running hypotheses: {len(running_hyps)}\")\n        if self.token_list is not None:\n            logging.debug(\n                \"best hypo: \"\n                + \"\".join([self.token_list[x] for x in running_hyps[0].yseq[1:]])\n            )\n        # add eos in the final loop to avoid that there are no ended hyps\n        if i == maxlen - 1:\n            logging.debug(\"adding <eos> in the last position in the loop\")\n            running_hyps = [\n                h._replace(yseq=self.append_token(h.yseq, self.eos))\n                for h in running_hyps\n            ]\n\n        # add ended hypotheses to a final list, and removed them from current hypotheses\n        # (this will be a problem, number of hyps < beam)\n        remained_hyps = []\n        for hyp in running_hyps:\n            if hyp.yseq[-1] == self.eos:\n                # e.g., Word LM needs to add final <eos> score\n                for k, d in chain(self.full_scorers.items(), self.part_scorers.items()):\n                    s = d.final_score(hyp.states[k])\n                    hyp.scores[k] += s\n                    hyp = hyp._replace(score=hyp.score + self.weights[k] * s)\n                ended_hyps.append(hyp)\n            else:\n                remained_hyps.append(hyp)\n        return remained_hyps","metadata":{"id":"OJ9U-Ge2QyfD","execution":{"iopub.status.busy":"2024-04-17T01:56:39.323398Z","iopub.execute_input":"2024-04-17T01:56:39.324240Z","iopub.status.idle":"2024-04-17T01:56:39.411864Z","shell.execute_reply.started":"2024-04-17T01:56:39.324190Z","shell.execute_reply":"2024-04-17T01:56:39.407450Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## BatchBeamSearch","metadata":{"id":"9KU4cv09Qrkz"}},{"cell_type":"code","source":"class BatchHypothesis(NamedTuple):\n    \"\"\"Batchfied/Vectorized hypothesis data type.\"\"\"\n\n    yseq: torch.Tensor = torch.tensor([])  # (batch, maxlen)\n    score: torch.Tensor = torch.tensor([])  # (batch,)\n    length: torch.Tensor = torch.tensor([])  # (batch,)\n    scores: Dict[str, torch.Tensor] = dict()  # values: (batch,)\n    states: Dict[str, Dict] = dict()\n\n    def __len__(self) -> int:\n        \"\"\"Return a batch size.\"\"\"\n        return len(self.length)\n\n\nclass BatchBeamSearch(BeamSearch):\n    \"\"\"Batch beam search implementation.\"\"\"\n\n    def batchfy(self, hyps: List[Hypothesis]) -> BatchHypothesis:\n        \"\"\"Convert list to batch.\"\"\"\n        if len(hyps) == 0:\n            return BatchHypothesis()\n        yseq = pad_sequence(\n            [h.yseq for h in hyps], batch_first=True, padding_value=self.eos\n        )\n        return BatchHypothesis(\n            yseq=yseq,\n            length=torch.tensor(\n                [len(h.yseq) for h in hyps], dtype=torch.int64, device=yseq.device\n            ),\n            score=torch.tensor([h.score for h in hyps]).to(yseq.device),\n            scores={\n                k: torch.tensor([h.scores[k] for h in hyps], device=yseq.device)\n                for k in self.scorers\n            },\n            states={k: [h.states[k] for h in hyps] for k in self.scorers},\n        )\n\n    def _batch_select(self, hyps: BatchHypothesis, ids: List[int]) -> BatchHypothesis:\n        return BatchHypothesis(\n            yseq=hyps.yseq[ids],\n            score=hyps.score[ids],\n            length=hyps.length[ids],\n            scores={k: v[ids] for k, v in hyps.scores.items()},\n            states={\n                k: [self.scorers[k].select_state(v, i) for i in ids]\n                for k, v in hyps.states.items()\n            },\n        )\n\n    def _select(self, hyps: BatchHypothesis, i: int) -> Hypothesis:\n        return Hypothesis(\n            yseq=hyps.yseq[i, : hyps.length[i]],\n            score=hyps.score[i],\n            scores={k: v[i] for k, v in hyps.scores.items()},\n            states={\n                k: self.scorers[k].select_state(v, i) for k, v in hyps.states.items()\n            },\n        )\n\n    def unbatchfy(self, batch_hyps: BatchHypothesis) -> List[Hypothesis]:\n        \"\"\"Revert batch to list.\"\"\"\n        return [\n            Hypothesis(\n                yseq=batch_hyps.yseq[i][: batch_hyps.length[i]],\n                score=batch_hyps.score[i],\n                scores={k: batch_hyps.scores[k][i] for k in self.scorers},\n                states={\n                    k: v.select_state(batch_hyps.states[k], i)\n                    for k, v in self.scorers.items()\n                },\n            )\n            for i in range(len(batch_hyps.length))\n        ]\n\n    def batch_beam(\n        self, weighted_scores: torch.Tensor, ids: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Batch-compute topk full token ids and partial token ids.\n\n        Args:\n            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n                Its shape is `(n_beam, self.vocab_size)`.\n            ids (torch.Tensor): The partial token ids to compute topk.\n                Its shape is `(n_beam, self.pre_beam_size)`.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n                The topk full (prev_hyp, new_token) ids\n                and partial (prev_hyp, new_token) ids.\n                Their shapes are all `(self.beam_size,)`\n\n        \"\"\"\n        top_ids = weighted_scores.view(-1).topk(self.beam_size)[1]\n        # Because of the flatten above, `top_ids` is organized as:\n        # [hyp1 * V + token1, hyp2 * V + token2, ..., hypK * V + tokenK],\n        # where V is `self.n_vocab` and K is `self.beam_size`\n        prev_hyp_ids = torch.div(top_ids, self.n_vocab, rounding_mode=\"trunc\")\n        new_token_ids = top_ids % self.n_vocab\n        return prev_hyp_ids, new_token_ids, prev_hyp_ids, new_token_ids\n\n    def init_hyp(self, x: torch.Tensor) -> BatchHypothesis:\n        \"\"\"Get an initial hypothesis data.\n\n        Args:\n            x (torch.Tensor): The encoder output feature\n\n        Returns:\n            Hypothesis: The initial hypothesis.\n\n        \"\"\"\n        init_states = dict()\n        init_scores = dict()\n        for k, d in self.scorers.items():\n            init_states[k] = d.batch_init_state(x)\n            init_scores[k] = 0.0\n        return self.batchfy(\n            [\n                Hypothesis(\n                    score=0.0,\n                    scores=init_scores,\n                    states=init_states,\n                    yseq=torch.tensor([self.sos], device=x.device),\n                )\n            ]\n        )\n\n    def score_full(\n        self, hyp: BatchHypothesis, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        \"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"\n        scores = dict()\n        states = dict()\n        for k, d in self.full_scorers.items():\n            scores[k], states[k] = d.batch_score(hyp.yseq, hyp.states[k], x)\n        return scores, states\n\n    def score_partial(\n        self, hyp: BatchHypothesis, ids: torch.Tensor, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        \"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            ids (torch.Tensor): 2D tensor of new partial tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"\n        scores = dict()\n        states = dict()\n        for k, d in self.part_scorers.items():\n            scores[k], states[k] = d.batch_score_partial(\n                hyp.yseq, ids, hyp.states[k], x\n            )\n        return scores, states\n\n    def merge_states(self, states: Any, part_states: Any, part_idx: int) -> Any:\n        \"\"\"Merge states for new hypothesis.\n\n        Args:\n            states: states of `self.full_scorers`\n            part_states: states of `self.part_scorers`\n            part_idx (int): The new token id for `part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are states of the scorers.\n\n        \"\"\"\n        new_states = dict()\n        for k, v in states.items():\n            new_states[k] = v\n        for k, v in part_states.items():\n            new_states[k] = v\n        return new_states\n\n    def search(self, running_hyps: BatchHypothesis, x: torch.Tensor) -> BatchHypothesis:\n        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n\n        Args:\n            running_hyps (BatchHypothesis): Running hypotheses on beam\n            x (torch.Tensor): Encoded speech feature (T, D)\n\n        Returns:\n            BatchHypothesis: Best sorted hypotheses\n\n        \"\"\"\n        n_batch = len(running_hyps)\n        part_ids = None  # no pre-beam\n        # batch scoring\n        weighted_scores = torch.zeros(\n            n_batch, self.n_vocab, dtype=x.dtype, device=x.device\n        )\n        scores, states = self.score_full(running_hyps, x.expand(n_batch, *x.shape))\n        for k in self.full_scorers:\n            weighted_scores += self.weights[k] * scores[k]\n        # partial scoring\n        if self.do_pre_beam:\n            pre_beam_scores = (\n                weighted_scores\n                if self.pre_beam_score_key == \"full\"\n                else scores[self.pre_beam_score_key]\n            )\n            part_ids = torch.topk(pre_beam_scores, self.pre_beam_size, dim=-1)[1]\n        # NOTE(takaaki-hori): Unlike BeamSearch, we assume that score_partial returns\n        # full-size score matrices, which has non-zero scores for part_ids and zeros\n        # for others.\n        part_scores, part_states = self.score_partial(running_hyps, part_ids, x)\n        for k in self.part_scorers:\n            weighted_scores += self.weights[k] * part_scores[k]\n        # add previous hyp scores\n        weighted_scores += running_hyps.score.to(\n            dtype=x.dtype, device=x.device\n        ).unsqueeze(1)\n\n        # TODO(karita): do not use list. use batch instead\n        # see also https://github.com/espnet/espnet/pull/1402#discussion_r354561029\n        # update hyps\n        best_hyps = []\n        prev_hyps = self.unbatchfy(running_hyps)\n        for (\n            full_prev_hyp_id,\n            full_new_token_id,\n            part_prev_hyp_id,\n            part_new_token_id,\n        ) in zip(*self.batch_beam(weighted_scores, part_ids)):\n            prev_hyp = prev_hyps[full_prev_hyp_id]\n            best_hyps.append(\n                Hypothesis(\n                    score=weighted_scores[full_prev_hyp_id, full_new_token_id],\n                    yseq=self.append_token(prev_hyp.yseq, full_new_token_id),\n                    scores=self.merge_scores(\n                        prev_hyp.scores,\n                        {k: v[full_prev_hyp_id] for k, v in scores.items()},\n                        full_new_token_id,\n                        {k: v[part_prev_hyp_id] for k, v in part_scores.items()},\n                        part_new_token_id,\n                    ),\n                    states=self.merge_states(\n                        {\n                            k: self.full_scorers[k].select_state(v, full_prev_hyp_id)\n                            for k, v in states.items()\n                        },\n                        {\n                            k: self.part_scorers[k].select_state(\n                                v, part_prev_hyp_id, part_new_token_id\n                            )\n                            for k, v in part_states.items()\n                        },\n                        part_new_token_id,\n                    ),\n                )\n            )\n        return self.batchfy(best_hyps)\n\n    def post_process(\n        self,\n        i: int,\n        maxlen: int,\n        maxlenratio: float,\n        running_hyps: BatchHypothesis,\n        ended_hyps: List[Hypothesis],\n    ) -> BatchHypothesis:\n        \"\"\"Perform post-processing of beam search iterations.\n\n        Args:\n            i (int): The length of hypothesis tokens.\n            maxlen (int): The maximum length of tokens in beam search.\n            maxlenratio (int): The maximum length ratio in beam search.\n            running_hyps (BatchHypothesis): The running hypotheses in beam search.\n            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n\n        Returns:\n            BatchHypothesis: The new running hypotheses.\n\n        \"\"\"\n        n_batch = running_hyps.yseq.shape[0]\n        logging.debug(f\"the number of running hypothes: {n_batch}\")\n        if self.token_list is not None:\n            logging.debug(\n                \"best hypo: \"\n                + \"\".join(\n                    [\n                        self.token_list[x]\n                        for x in running_hyps.yseq[0, 1 : running_hyps.length[0]]\n                    ]\n                )\n            )\n        # add eos in the final loop to avoid that there are no ended hyps\n        if i == maxlen - 1:\n            logging.debug(\"adding <eos> in the last position in the loop\")\n            yseq_eos = torch.cat(\n                (\n                    running_hyps.yseq,\n                    torch.full(\n                        (n_batch, 1),\n                        self.eos,\n                        device=running_hyps.yseq.device,\n                        dtype=torch.int64,\n                    ),\n                ),\n                1,\n            )\n            running_hyps.yseq.resize_as_(yseq_eos)\n            running_hyps.yseq[:] = yseq_eos\n            running_hyps.length[:] = yseq_eos.shape[1]\n\n        # add ended hypotheses to a final list, and removed them from current hypotheses\n        # (this will be a probmlem, number of hyps < beam)\n        is_eos = (\n            running_hyps.yseq[torch.arange(n_batch), running_hyps.length - 1]\n            == self.eos\n        )\n        for b in torch.nonzero(is_eos, as_tuple=False).view(-1):\n            hyp = self._select(running_hyps, b)\n            ended_hyps.append(hyp)\n        remained_ids = torch.nonzero(is_eos == 0, as_tuple=False).view(-1)\n        return self._batch_select(running_hyps, remained_ids)","metadata":{"id":"e4xkn6M3QsgW","execution":{"iopub.status.busy":"2024-04-17T01:56:40.781722Z","iopub.execute_input":"2024-04-17T01:56:40.782130Z","iopub.status.idle":"2024-04-17T01:56:40.834841Z","shell.execute_reply.started":"2024-04-17T01:56:40.782096Z","shell.execute_reply":"2024-04-17T01:56:40.833786Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"## ensemble","metadata":{"id":"kJJTggAkS5Pe"}},{"cell_type":"code","source":"def average_checkpoints(last):\n    avg = None\n    for path in last:\n        states = torch.load(path, map_location=lambda storage, loc: storage)[\n            \"state_dict\"\n        ]\n        states = {k[6:]: v for k, v in states.items() if k.startswith(\"model.\")}\n        if avg is None:\n            avg = states\n        else:\n            for k in avg.keys():\n                avg[k] += states[k]\n    # average\n    for k in avg.keys():\n        if avg[k] is not None:\n            if avg[k].is_floating_point():\n                avg[k] /= len(last)\n            else:\n                avg[k] //= len(last)\n    return avg\n\n\ndef ensemble(args):\n    last = [\n        os.path.join(args.exp_dir, args.exp_name, f\"epoch={n}.ckpt\")\n        for n in range(\n            args.trainer.max_epochs - 10,\n            args.trainer.max_epochs,\n        )\n    ]\n    model_path = os.path.join(\n        args.exp_dir, args.exp_name, f\"model_avg_10.pth\"\n    )\n    torch.save(average_checkpoints(last), model_path)\n    return model_path","metadata":{"id":"HLnnKKLOS6NK","execution":{"iopub.status.busy":"2024-04-17T01:56:42.108914Z","iopub.execute_input":"2024-04-17T01:56:42.109330Z","iopub.status.idle":"2024-04-17T01:56:42.119774Z","shell.execute_reply.started":"2024-04-17T01:56:42.109297Z","shell.execute_reply":"2024-04-17T01:56:42.118672Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## AVSR multimodal(Audio-video)","metadata":{"id":"TXA40fu0zMBS"}},{"cell_type":"code","source":"# import torch\n# import torchaudio\n# from cosine import WarmupCosineScheduler\n# from datamodule.transforms import TextTransform\n\n# from pytorch_lightning import LightningModule\n# from espnet.nets.batch_beam_search import BatchBeamSearch\n# from espnet.nets.pytorch_backend.e2e_asr_conformer_av import E2E\n# from espnet.nets.scorers.length_bonus import LengthBonus\n# from espnet.nets.scorers.ctc import CTCPrefixScorer\n\n\ndef compute_word_level_distance(seq1, seq2):\n    return torchaudio.functional.edit_distance(seq1.lower().split(), seq2.lower().split())\n\n\nclass ModelModule(LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.save_hyperparameters(cfg)\n        self.cfg = cfg\n        self.backbone_args = self.cfg.model.audiovisual_backbone\n\n        self.text_transform = TextTransform()\n        self.token_list = self.text_transform.token_list\n        self.model = E2E(len(self.token_list), self.backbone_args)\n\n        # -- initialise\n        if self.cfg.pretrained_model_path:\n            ckpt = torch.load(self.cfg.pretrained_model_path, map_location=lambda storage, loc: storage)\n            if self.cfg.transfer_frontend:\n                tmp_ckpt = {k: v for k, v in ckpt[\"model_state_dict\"].items() if k.startswith(\"trunk.\") or k.startswith(\"frontend3D.\")}\n                self.model.encoder.frontend.load_state_dict(tmp_ckpt)\n            elif self.cfg.transfer_encoder:\n                tmp_ckpt = {k.replace(\"encoder.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"encoder.\")}\n                self.model.encoder.load_state_dict(tmp_ckpt, strict=True)\n            else:\n                self.model.load_state_dict(ckpt)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW([{\"name\": \"model\", \"params\": self.model.parameters(), \"lr\": self.cfg.optimizer.lr}], weight_decay=self.cfg.optimizer.weight_decay, betas=(0.9, 0.98))\n        scheduler = WarmupCosineScheduler(optimizer, self.cfg.optimizer.warmup_epochs, self.cfg.trainer.max_epochs, len(self.trainer.datamodule.train_dataloader()))\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]\n\n    def forward(self, video, audio):\n        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n        video_feat, _ = self.model.encoder(video.unsqueeze(0).to(self.device), None)\n        audio_feat, _ = self.model.aux_encoder(audio.unsqueeze(0).to(self.device), None)\n        audiovisual_feat = self.model.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n\n        audiovisual_feat = audiovisual_feat.squeeze(0)\n\n        nbest_hyps = self.beam_search(audiovisual_feat)\n        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n        return predicted\n\n    def training_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(self, sample, sample_idx):\n        video_feat, _ = self.model.encoder(sample[\"video\"].unsqueeze(0).to(self.device), None)\n        audio_feat, _ = self.model.aux_encoder(sample[\"audio\"].unsqueeze(0).to(self.device), None)\n        audiovisual_feat = self.model.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n        audiovisual_feat = audiovisual_feat.squeeze(0)\n\n        nbest_hyps = self.beam_search(audiovisual_feat)\n        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n\n        token_id = sample[\"target\"]\n        actual = self.text_transform.post_process(token_id)\n\n        self.total_edit_distance += compute_word_level_distance(actual, predicted)\n        self.total_length += len(actual.split())\n        return\n\n    def _step(self, batch, batch_idx, step_type):\n        loss, loss_ctc, loss_att, acc = self.model(batch[\"videos\"], batch[\"audios\"], batch[\"video_lengths\"],\n                                                   batch[\"audio_lengths\"], batch[\"targets\"])\n        batch_size = len(batch[\"videos\"])\n\n        if step_type == \"train\":\n            self.log(\"loss\", loss, on_step=True, on_epoch=True, batch_size=batch_size)\n            self.log(\"loss_ctc\", loss_ctc, on_step=False, on_epoch=True, batch_size=batch_size)\n            self.log(\"loss_att\", loss_att, on_step=False, on_epoch=True, batch_size=batch_size)\n            self.log(\"decoder_acc\", acc, on_step=True, on_epoch=True, batch_size=batch_size)\n        else:\n            self.log(\"loss_val\", loss, batch_size=batch_size)\n            self.log(\"loss_ctc_val\", loss_ctc, batch_size=batch_size)\n            self.log(\"loss_att_val\", loss_att, batch_size=batch_size)\n            self.log(\"decoder_acc_val\", acc, batch_size=batch_size)\n\n        if step_type == \"train\":\n            self.log(\"monitoring_step\", torch.tensor(self.global_step, dtype=torch.float32))\n\n        return loss\n\n    def on_train_epoch_start(self):\n        sampler = self.trainer.train_dataloader.loaders.batch_sampler\n        if hasattr(sampler, \"set_epoch\"):\n            sampler.set_epoch(self.current_epoch)\n        return super().on_train_epoch_start()\n\n    def on_test_epoch_start(self):\n        self.total_length = 0\n        self.total_edit_distance = 0\n        self.text_transform = TextTransform()\n        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n\n    def on_test_epoch_end(self):\n        self.log(\"wer\", self.total_edit_distance / self.total_length)\n\n\ndef get_beam_search_decoder(model, token_list, ctc_weight=0.1, beam_size=40):\n    scorers = {\n        \"decoder\": model.decoder,\n        \"ctc\": CTCPrefixScorer(model.ctc, model.eos),\n        \"length_bonus\": LengthBonus(len(token_list)),\n        \"lm\": None\n    }\n\n    weights = {\n        \"decoder\": 1.0 - ctc_weight,\n        \"ctc\": ctc_weight,\n        \"lm\": 0.0,\n        \"length_bonus\": 0.0,\n    }\n\n    return BatchBeamSearch(\n        beam_size=beam_size,\n        vocab_size=len(token_list),\n        weights=weights,\n        scorers=scorers,\n        sos=model.sos,\n        eos=model.eos,\n        token_list=token_list,\n        pre_beam_score_key=None if ctc_weight == 1.0 else \"decoder\",\n    )","metadata":{"id":"hYkyP2H4zcZE","execution":{"iopub.status.busy":"2024-04-17T01:56:43.603605Z","iopub.execute_input":"2024-04-17T01:56:43.603980Z","iopub.status.idle":"2024-04-17T01:56:43.640065Z","shell.execute_reply.started":"2024-04-17T01:56:43.603951Z","shell.execute_reply":"2024-04-17T01:56:43.638800Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## AVSR Base","metadata":{"id":"zEdCfDfmzabz"}},{"cell_type":"code","source":"# import torch\n# import torchaudio\n# from cosine import WarmupCosineScheduler\n# from datamodule.transforms import TextTransform\n\n# from pytorch_lightning import LightningModule\n# from espnet.nets.batch_beam_search import BatchBeamSearch\n# from espnet.nets.pytorch_backend.e2e_asr_conformer import E2E\n# from espnet.nets.scorers.length_bonus import LengthBonus\n# from espnet.nets.scorers.ctc import CTCPrefixScorer\n\n\ndef compute_word_level_distance(seq1, seq2):\n    return torchaudio.functional.edit_distance(seq1.lower().split(), seq2.lower().split())\n\n\nclass ModelModule(LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.save_hyperparameters(cfg)\n        self.cfg = cfg\n        if self.cfg.data.modality == \"audio\":\n            self.backbone_args = self.cfg.model.audio_backbone\n        elif self.cfg.data.modality == \"video\":\n            self.backbone_args = self.cfg.model.visual_backbone\n\n        self.text_transform = TextTransform()\n        self.token_list = self.text_transform.token_list\n        self.model = E2E(len(self.token_list), self.backbone_args)\n        # -- initialise\n        if self.cfg.pretrained_model_path:\n            ckpt = torch.load(self.cfg.pretrained_model_path, map_location=lambda storage, loc: storage)\n            if self.cfg.transfer_frontend:\n                tmp_ckpt = {k: v for k, v in ckpt.items() if k.startswith(\"trunk.\") or k.startswith(\"frontend3D.\")}\n                self.model.encoder.frontend.load_state_dict(ckpt)\n            elif self.cfg.transfer_encoder:\n                tmp_ckpt = {k.replace(\"encoder.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"encoder.\")}\n                self.model.encoder.load_state_dict(tmp_ckpt, strict=True)\n                print('Transfer learning')\n            else:\n                self.model.load_state_dict(ckpt)\n        else:\n            print('Entrenamiento de cero')\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW([{\"name\": \"model\", \"params\": self.model.parameters(), \"lr\": self.cfg.optimizer.lr}], weight_decay=self.cfg.optimizer.weight_decay, betas=(0.9, 0.98))\n        scheduler = WarmupCosineScheduler(optimizer, self.cfg.optimizer.warmup_epochs, self.cfg.trainer.max_epochs, len(self.trainer.datamodule.train_dataloader()))\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]\n\n    def forward(self, sample):\n        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n        enc_feat, _ = self.model.encoder(sample.unsqueeze(0).to(self.device), None)\n        enc_feat = enc_feat.squeeze(0)\n\n        nbest_hyps = self.beam_search(enc_feat)\n        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n        return predicted\n\n    def training_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(self, sample, sample_idx):\n        enc_feat, _ = self.model.encoder(sample[\"input\"].unsqueeze(0).to(self.device), None)\n        enc_feat = enc_feat.squeeze(0)\n\n        nbest_hyps = self.beam_search(enc_feat)\n        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n\n        token_id = sample[\"target\"]\n        actual = self.text_transform.post_process(token_id)\n\n        self.total_edit_distance += compute_word_level_distance(actual, predicted)\n        self.total_length += len(actual.split())\n        return\n\n    def _step(self, batch, batch_idx, step_type):\n        loss, loss_ctc, loss_att, acc = self.model(batch[\"inputs\"], batch[\"input_lengths\"], batch[\"targets\"])\n        batch_size = len(batch[\"inputs\"])\n\n        if step_type == \"train\":\n            self.log(\"loss\", loss, on_step=True, on_epoch=True, batch_size=batch_size)\n            self.log(\"loss_ctc\", loss_ctc, on_step=False, on_epoch=True, batch_size=batch_size)\n            self.log(\"loss_att\", loss_att, on_step=False, on_epoch=True, batch_size=batch_size)\n            self.log(\"decoder_acc\", acc, on_step=True, on_epoch=True, batch_size=batch_size)\n        else:\n            self.log(\"loss_val\", loss, batch_size=batch_size)\n            self.log(\"loss_ctc_val\", loss_ctc, batch_size=batch_size)\n            self.log(\"loss_att_val\", loss_att, batch_size=batch_size)\n            self.log(\"decoder_acc_val\", acc, batch_size=batch_size)\n\n        if step_type == \"train\":\n            self.log(\"monitoring_step\", torch.tensor(self.global_step, dtype=torch.float32))\n\n        return loss\n\n    def on_train_epoch_start(self):\n        #sampler = self.trainer.train_dataloader.loaders.batch_sampler\n        sampler = self.trainer.train_dataloader.batch_sampler\n        if hasattr(sampler, \"set_epoch\"):\n            sampler.set_epoch(self.current_epoch)\n        return super().on_train_epoch_start()\n\n    def on_test_epoch_start(self):\n        self.total_length = 0\n        self.total_edit_distance = 0\n        self.text_transform = TextTransform()\n        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n\n    def on_test_epoch_end(self):\n        self.log(\"wer\", self.total_edit_distance / self.total_length)\n\n\ndef get_beam_search_decoder(model, token_list, ctc_weight=0.1, beam_size=40):\n    scorers = {\n        \"decoder\": model.decoder,\n        \"ctc\": CTCPrefixScorer(model.ctc, model.eos),\n        \"length_bonus\": LengthBonus(len(token_list)),\n        \"lm\": None\n    }\n\n    weights = {\n        \"decoder\": 1.0 - ctc_weight,\n        \"ctc\": ctc_weight,\n        \"lm\": 0.0,\n        \"length_bonus\": 0.0,\n    }\n\n    return BatchBeamSearch(\n        beam_size=beam_size,\n        vocab_size=len(token_list),\n        weights=weights,\n        scorers=scorers,\n        sos=model.sos,\n        eos=model.eos,\n        token_list=token_list,\n        pre_beam_score_key=None if ctc_weight == 1.0 else \"decoder\",\n    )","metadata":{"id":"Li8yCFQ4zmxS","execution":{"iopub.status.busy":"2024-04-17T01:56:45.480480Z","iopub.execute_input":"2024-04-17T01:56:45.480843Z","iopub.status.idle":"2024-04-17T01:56:45.515065Z","shell.execute_reply.started":"2024-04-17T01:56:45.480814Z","shell.execute_reply":"2024-04-17T01:56:45.513912Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Prueba ModuleModel","metadata":{"id":"gVcdlgZ5umU1"}},{"cell_type":"code","source":"modelmodule = ModelModule(cfg)","metadata":{"id":"Vss4d9VJuujz","outputId":"57590734-ab73-456b-f8da-b784435030d2","execution":{"iopub.status.busy":"2024-04-16T18:52:03.578912Z","iopub.execute_input":"2024-04-16T18:52:03.579665Z","iopub.status.idle":"2024-04-16T18:52:03.633594Z","shell.execute_reply.started":"2024-04-16T18:52:03.579611Z","shell.execute_reply":"2024-04-16T18:52:03.632275Z"},"trusted":true},"execution_count":53,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m modelmodule \u001b[38;5;241m=\u001b[39m ModelModule(\u001b[43mcfg\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"],"ename":"NameError","evalue":"name 'cfg' is not defined","output_type":"error"}]},{"cell_type":"code","source":"print(\"Número de parámetros:\", sum(p.numel() for p in list(modelmodule.parameters())))","metadata":{"execution":{"iopub.status.busy":"2024-04-16T02:00:43.733094Z","iopub.execute_input":"2024-04-16T02:00:43.733989Z","iopub.status.idle":"2024-04-16T02:00:43.741821Z","shell.execute_reply.started":"2024-04-16T02:00:43.733951Z","shell.execute_reply":"2024-04-16T02:00:43.740959Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Número de parámetros: 56350770\n","output_type":"stream"}]},{"cell_type":"code","source":"modelmodule.cuda()","metadata":{"id":"s3mEHaZswlJW","outputId":"30e3bbc9-c047-444e-afe0-1a2eeb128953","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelmodule.device","metadata":{"id":"Cl6LPN4dzmBf","outputId":"36f7747d-14d8-4241-9f7a-4e2de78de6ca","execution":{"iopub.status.busy":"2024-04-15T15:48:17.020064Z","iopub.execute_input":"2024-04-15T15:48:17.020461Z","iopub.status.idle":"2024-04-15T15:48:17.026362Z","shell.execute_reply.started":"2024-04-15T15:48:17.020428Z","shell.execute_reply":"2024-04-15T15:48:17.025410Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"modelmodule.cpu()","metadata":{"id":"Axiyc2r-0Hwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.memory_allocated() / (1024 ** 2)","metadata":{"id":"qNM20dUR0FKZ","outputId":"f2b150e9-1171-40dc-cac6-a6aedd76f97e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuraciones con Hydra","metadata":{"id":"IzIaMo14x8me"}},{"cell_type":"code","source":"import hydra\nfrom omegaconf import OmegaConf\nimport pickle\n\n@hydra.main(config_path=\"/content/auto_avsr/configs\", config_name=\"config\")\ndef my_app(cfg):\n    #print(OmegaConf.to_yaml(cfg))\n    cfg = dict(cfg)\n    with open(\"cfg.pickle\", \"wb\") as f:\n      pickle.dump(cfg, f)\n\nif __name__ == \"__main__\":\n    my_app()","metadata":{"id":"sxQHAxxtbrMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python main.py","metadata":{"id":"FSVbpUpHcKf_","outputId":"a3dbb547-fd2a-4c6f-80b4-5d0a2d5af39e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{"id":"CERfWWMgWOcr"}},{"cell_type":"code","source":"cfg = DictConfig({'exp_dir': None, 'exp_name': None, 'gpus': None, 'slurm_job_id': None, 'file_path': None,\n                  'ckpt_path': None, 'pretrained_model_path': None, 'transfer_frontend': None,\n                  'transfer_encoder': None,\n                  'data': {'modality': 'video', 'use_audio_normalise': False, 'max_frames': 1800,\n                           'max_frames_val': 500,\n                           'dataset': {'root_dir': None, 'label_dir': 'labels',\n                                       'train_file': 'lrs3_train_transcript_lengths_seg24s.csv',\n                                       'val_file': 'lrs3_test_transcript_lengths_seg24s.csv',\n                                       'test_file': 'lrs3_test_transcript_lengths_seg24s.csv'}},\n                  'model': {'visual_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072,\n                                                'elayers': 12, 'transformer_input_layer': 'conv3d',\n                                                'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1,\n                                                'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                'macaron_style': True, 'use_cnn_module': True,\n                                                'cnn_module_kernel': 31, 'zero_triu': False,\n                                                'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': 768,\n                                                'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                'ctc_type': 'builtin', 'rel_pos_type': 'latest'},\n                            'audio_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072, 'elayers': 12,\n                                               'transformer_input_layer': 'conv1d', 'dropout_rate': 0.1,\n                                               'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                               'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                               'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                               'ddim': 768, 'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n                                               'transformer_length_normalized_loss': False, 'mtlalpha': 0.1, 'ctc_type': 'builtin',\n                                               'rel_pos_type': 'latest'},\n                            'audiovisual_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072, 'elayers': 12,\n                                                     'transformer_input_layer': 'conv3d', 'dropout_rate': 0.1,\n                                                     'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                     'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                                     'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                                     'ddim': 768, 'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                     'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                     'ctc_type': 'builtin', 'rel_pos_type': 'latest', 'aux_adim': 768,\n                                                     'aux_aheads': 12, 'aux_eunits': 3072, 'aux_elayers': 12,\n                                                     'aux_transformer_input_layer': 'conv1d', 'aux_dropout_rate': 0.1,\n                                                     'aux_transformer_attn_dropout_rate': 0.1,\n                                                     'aux_transformer_encoder_attn_layer_type': 'rel_mha', 'aux_macaron_style': True,\n                                                     'aux_use_cnn_module': True, 'aux_cnn_module_kernel': 31, 'aux_zero_triu': False,\n                                                     'aux_a_upsample_ratio': 1, 'aux_relu_type': 'swish', 'aux_dunits': 3072,\n                                                     'aux_dlayers': 6, 'aux_lsm_weight': 0.1,\n                                                     'aux_transformer_length_normalized_loss': False, 'aux_mtlalpha': 0.1,\n                                                     'aux_ctc_type': 'builtin', 'aux_rel_pos_type': 'latest', 'fusion_hdim': 8192,\n                                                     'fusion_norm': 'batchnorm'}},\n                  'optimizer': {'name': 'adamw', 'lr': 0.001, 'warmup_epochs': 5, 'weight_decay': 0.03},\n                  'trainer': {'precision': 32, 'max_epochs': 75, 'num_nodes': 1, 'gpus': -1, 'sync_batchnorm': True,\n                              'default_root_dir': '${exp_dir}', 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1,\n                              'gradient_clip_val': 5.0, 'replace_sampler_ddp': False, 'resume_from_checkpoint': None},\n                  'decode': {'name': 'default', 'snr_target': 999999}})","metadata":{"id":"SFHMxS-nA3Ck","execution":{"iopub.status.busy":"2024-04-17T02:04:07.902077Z","iopub.execute_input":"2024-04-17T02:04:07.902552Z","iopub.status.idle":"2024-04-17T02:04:07.937570Z","shell.execute_reply.started":"2024-04-17T02:04:07.902514Z","shell.execute_reply":"2024-04-17T02:04:07.936663Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"!mkdir model_vsr","metadata":{"execution":{"iopub.status.busy":"2024-04-17T01:58:36.138097Z","iopub.execute_input":"2024-04-17T01:58:36.139028Z","iopub.status.idle":"2024-04-17T01:58:37.230890Z","shell.execute_reply.started":"2024-04-17T01:58:36.138994Z","shell.execute_reply":"2024-04-17T01:58:37.229744Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory 'model_vsr': File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"cfg = DictConfig({'exp_dir': '/kaggle/working/model_vsr/', 'exp_name': 'model', 'gpus': None, 'slurm_job_id': None, 'file_path': None,\n                  'ckpt_path': '/kaggle/working/', \n                  'pretrained_model_path': '/kaggle/input/cmumoseas_v_es_wer44.5/pytorch/model/1/model.pth', \n                  'transfer_frontend': None,\n                  'transfer_encoder': True,\n                  'data': {'modality': 'video', 'use_audio_normalise': False, 'max_frames': 1800,\n                           'max_frames_val': 1800,\n                           'dataset': {'root_dir': '/kaggle/input/vphbusfx/VPHB_USFX_preprocessing', 'label_dir': 'labels',\n                                       'train_file': 'VPHBUSFX_unigram100_train.csv',\n                                       'val_file': 'VPHBUSFX_unigram100_val.csv'}},\n                  'model': {'visual_backbone': {'adim': 256, 'aheads': 4, 'eunits': 2048,\n                                                'elayers': 12, 'transformer_input_layer': 'conv3d',\n                                                'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1,\n                                                'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                'macaron_style': 1, 'use_cnn_module': 1,\n                                                'cnn_module_kernel': 31, 'zero_triu': 0,\n                                                'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': 256,\n                                                'dheads': 4, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                'ctc_type': 'builtin', 'rel_pos_type': 'latest'},\n                            'audio_backbone': {'adim': 256, 'aheads': 12, 'eunits': 2048, 'elayers': 12,\n                                               'transformer_input_layer': 'conv1d', 'dropout_rate': 0.1,\n                                               'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                               'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                               'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                               'ddim': 256, 'dheads': 30, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                               'transformer_length_normalized_loss': False, 'mtlalpha': 0.1, 'ctc_type': 'builtin',\n                                               'rel_pos_type': 'latest'},\n                            'audiovisual_backbone': {'adim': 256, 'aheads': 12, 'eunits': 2048, 'elayers': 12,\n                                                     'transformer_input_layer': 'conv3d', 'dropout_rate': 0.1,\n                                                     'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                     'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                                     'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                                     'ddim': 256, 'dheads': 12, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                     'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                     'ctc_type': 'builtin', 'rel_pos_type': 'latest', 'aux_adim': 256,\n                                                     'aux_aheads': 12, 'aux_eunits': 2048, 'aux_elayers': 12,\n                                                     'aux_transformer_input_layer': 'conv1d', 'aux_dropout_rate': 0.1,\n                                                     'aux_transformer_attn_dropout_rate': 0.1,\n                                                     'aux_transformer_encoder_attn_layer_type': 'rel_mha', 'aux_macaron_style': True,\n                                                     'aux_use_cnn_module': True, 'aux_cnn_module_kernel': 31, 'aux_zero_triu': False,\n                                                     'aux_a_upsample_ratio': 1, 'aux_relu_type': 'swish', 'aux_dunits': 2048,\n                                                     'aux_dlayers': 6, 'aux_lsm_weight': 0.1,\n                                                     'aux_transformer_length_normalized_loss': False, 'aux_mtlalpha': 0.1,\n                                                     'aux_ctc_type': 'builtin', 'aux_rel_pos_type': 'latest', 'fusion_hdim': 8192,\n                                                     'fusion_norm': 'batchnorm'}},\n                  'optimizer': {'name': 'adamw', 'lr': 0.001, 'warmup_epochs': 5, 'weight_decay': 0.03},\n                  'trainer': {'precision': 16, 'max_epochs': 20, 'num_nodes': 1, 'sync_batchnorm': True,\n                              'default_root_dir': '${exp_dir}', 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1,\n                              'gradient_clip_val': 5.0},\n                  'decode': {'name': 'default', 'snr_target': 999999}})","metadata":{"id":"TcX8RNfWGe6U","execution":{"iopub.status.busy":"2024-04-17T02:06:21.327625Z","iopub.execute_input":"2024-04-17T02:06:21.328722Z","iopub.status.idle":"2024-04-17T02:06:21.363330Z","shell.execute_reply.started":"2024-04-17T02:06:21.328685Z","shell.execute_reply":"2024-04-17T02:06:21.362199Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"seed_everything(42, workers=True)\ncfg.gpus = torch.cuda.device_count()\n\ncheckpoint = ModelCheckpoint(\n    monitor=\"monitoring_step\",\n    mode=\"max\",\n    dirpath=os.path.join(cfg.exp_dir, cfg.exp_name) if cfg.exp_dir else None,\n    save_last=True,\n    filename=\"{epoch}\",\n    save_top_k=10,\n)\nlr_monitor = LearningRateMonitor(logging_interval=\"step\")\ncallbacks = [checkpoint, lr_monitor]\n\n# Set modules and trainer\n# if cfg.data.modality in [\"audio\", \"visual\"]:\n#     from lightning import ModelModule\n# elif cfg.data.modality == \"audiovisual\":\n#     from lightning_av import ModelModule\ndatamodule = DataModule(cfg)\nmodelmodule = ModelModule(cfg)\ncomet_logger = CometLogger(\n    api_key=\"g7gTJs7ovTZcgJKboOt03uQPI\",\n    project_name=\"VSR\",\n    experiment_name='model-unigram100'\n)\ntrainer = Trainer(\n    **cfg.trainer,\n    #logger=WandbLogger(name=cfg.exp_name, project=\"auto_avsr\"),\n    callbacks=callbacks,\n    #strategy=DDPPlugin(find_unused_parameters=False)\n    logger=comet_logger\n)\nprint('Cantidad de memoria ocupada:',torch.cuda.memory_allocated() / (1024 ** 2))\ntrainer.fit(model=modelmodule, datamodule=datamodule)\nensemble(cfg)","metadata":{"id":"BEacjNzBY2NW","outputId":"296f21e9-fec4-4252-e0c4-5be1f476e831","execution":{"iopub.status.busy":"2024-04-17T02:06:33.872456Z","iopub.execute_input":"2024-04-17T02:06:33.873374Z","iopub.status.idle":"2024-04-17T02:52:59.859320Z","shell.execute_reply.started":"2024-04-17T02:06:33.873337Z","shell.execute_reply":"2024-04-17T02:52:59.858228Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Numero de gpus 1\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/3492487820.py:22: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")\n","output_type":"stream"},{"name":"stdout","text":"Transfer learning\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning_fabric/connector.py:563: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","output_type":"stream"},{"name":"stdout","text":"Cantidad de memoria ocupada: 0.0\n","output_type":"stream"},{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/daniel314/vsr/3db7797836fe40bba656c12b880c38e6\n\n","output_type":"stream"},{"name":"stdout","text":"num_train:270\nnum_train:270\nnum_val:30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd92a2df0434fe3bc3c41b402cdf57b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : model-unigram100\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/daniel314/vsr/3db7797836fe40bba656c12b880c38e6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decoder_acc_epoch [20] : (0.045115791261196136, 0.28181192278862)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decoder_acc_step [20]  : (0.02777777798473835, 0.2799378037452698)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decoder_acc_val [20]   : (0.10951092094182968, 0.21822497248649597)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [124]             : (58.68365478515625, 731.3529052734375)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_att [20]          : (262.20208740234375, 440.7483215332031)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_att_val [20]      : (305.7026062011719, 388.4641418457031)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_ctc [20]          : (412.4201354980469, 890.888671875)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_ctc_val [20]      : (431.08758544921875, 498.58447265625)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_epoch [20]        : (277.2238464355469, 485.76226806640625)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_step [20]         : (118.83389282226562, 599.481201171875)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_val [20]          : (318.71826171875, 399.4761657714844)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr-AdamW/model [20]    : (7.136959534174592e-06, 0.0009941551980335653)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     monitoring_step [20]   : (49.0, 999.0)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : model-unigram100\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ckpt_path                                                          : /kaggle/working/\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/dataset/label_dir                                             : labels\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/dataset/root_dir                                              : /kaggle/input/vphbusfx/VPHB_USFX_preprocessing\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/dataset/train_file                                            : VPHBUSFX_unigram100_train.csv\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/dataset/val_file                                              : VPHBUSFX_unigram100_val.csv\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/max_frames                                                    : 1800\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/max_frames_val                                                : 1800\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/modality                                                      : video\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/use_audio_normalise                                           : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decode/name                                                        : default\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decode/snr_target                                                  : 999999\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exp_dir                                                            : /kaggle/working/model_vsr/\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exp_name                                                           : model\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     file_path                                                          : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gpus                                                               : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/a_upsample_ratio                              : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/adim                                          : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/aheads                                        : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/cnn_module_kernel                             : 31\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/ctc_type                                      : builtin\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/ddim                                          : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/dheads                                        : 30\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/dlayers                                       : 6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/dropout_rate                                  : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/dunits                                        : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/elayers                                       : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/eunits                                        : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/lsm_weight                                    : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/macaron_style                                 : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/mtlalpha                                      : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/rel_pos_type                                  : latest\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/relu_type                                     : swish\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/transformer_attn_dropout_rate                 : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/transformer_encoder_attn_layer_type           : rel_mha\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/transformer_input_layer                       : conv1d\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/transformer_length_normalized_loss            : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/use_cnn_module                                : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/zero_triu                                     : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/a_upsample_ratio                        : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/adim                                    : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aheads                                  : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_a_upsample_ratio                    : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_adim                                : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_aheads                              : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_cnn_module_kernel                   : 31\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_ctc_type                            : builtin\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_dlayers                             : 6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_dropout_rate                        : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_dunits                              : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_elayers                             : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_eunits                              : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_lsm_weight                          : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_macaron_style                       : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_mtlalpha                            : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_rel_pos_type                        : latest\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_relu_type                           : swish\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_transformer_attn_dropout_rate       : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_transformer_encoder_attn_layer_type : rel_mha\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_transformer_input_layer             : conv1d\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_transformer_length_normalized_loss  : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_use_cnn_module                      : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_zero_triu                           : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/cnn_module_kernel                       : 31\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/ctc_type                                : builtin\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/ddim                                    : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/dheads                                  : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/dlayers                                 : 6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/dropout_rate                            : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/dunits                                  : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/elayers                                 : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/eunits                                  : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/fusion_hdim                             : 8192\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/fusion_norm                             : batchnorm\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/lsm_weight                              : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/macaron_style                           : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/mtlalpha                                : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/rel_pos_type                            : latest\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/relu_type                               : swish\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/transformer_attn_dropout_rate           : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/transformer_encoder_attn_layer_type     : rel_mha\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/transformer_input_layer                 : conv3d\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/transformer_length_normalized_loss      : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/use_cnn_module                          : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/zero_triu                               : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/a_upsample_ratio                             : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/adim                                         : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/aheads                                       : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/cnn_module_kernel                            : 31\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/ctc_type                                     : builtin\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/ddim                                         : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/dheads                                       : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/dlayers                                      : 6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/dropout_rate                                 : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/dunits                                       : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/elayers                                      : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/eunits                                       : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/lsm_weight                                   : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/macaron_style                                : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/mtlalpha                                     : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/rel_pos_type                                 : latest\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/relu_type                                    : swish\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/transformer_attn_dropout_rate                : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/transformer_encoder_attn_layer_type          : rel_mha\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/transformer_input_layer                      : conv3d\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/transformer_length_normalized_loss           : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/use_cnn_module                               : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/zero_triu                                    : 0\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer/lr                                                       : 0.001\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer/name                                                     : adamw\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer/warmup_epochs                                            : 5\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer/weight_decay                                             : 0.03\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pretrained_model_path                                              : /kaggle/input/cmumoseas_v_es_wer44.5/pytorch/model/1/model.pth\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     slurm_job_id                                                       : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/accumulate_grad_batches                                    : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/default_root_dir                                           : /kaggle/working/model_vsr/\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/gradient_clip_val                                          : 5.0\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/max_epochs                                                 : 20\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/num_nodes                                                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/num_sanity_val_steps                                       : 0\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/precision                                                  : 16\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/sync_batchnorm                                             : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transfer_encoder                                                   : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transfer_frontend                                                  : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 159 metrics, params and output messages\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model_vsr/model/model_avg_10.pth'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"jsQNfgKdW06n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evalucion","metadata":{}},{"cell_type":"code","source":"cfg = DictConfig({'exp_dir': '/kaggle/working/model_vsr/', 'exp_name': 'model1', 'gpus': None, 'slurm_job_id': None, 'file_path': None,\n                  'ckpt_path': '/kaggle/working/', \n                  'pretrained_model_path': '/kaggle/input/vsr_catellano/pytorch/unigram50/1/model_avg_10_unigram50.pth', \n                  'transfer_frontend': None,\n                  'transfer_encoder': None,\n                  'data': {'modality': 'video', 'use_audio_normalise': False, 'max_frames': 1800,\n                           'max_frames_val': 1800,\n                           'dataset': {'root_dir': '/kaggle/input/vphbusfx/VPHB_USFX_preprocessing', 'label_dir': 'labels',\n                                       'train_file': 'VPHBUSFX_unigram50_train.csv',\n                                       'val_file': 'VPHBUSFX_unigram50_val.csv',\n                                       'test_file': 'VPHBUSFX_unigram50_val.csv'}},\n                  'model': {'visual_backbone': {'adim': 256, 'aheads': 4, 'eunits': 2048,\n                                                'elayers': 12, 'transformer_input_layer': 'conv3d',\n                                                'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1,\n                                                'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                'macaron_style': 1, 'use_cnn_module': 1,\n                                                'cnn_module_kernel': 31, 'zero_triu': 0,\n                                                'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': 256,\n                                                'dheads': 4, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                'ctc_type': 'builtin', 'rel_pos_type': 'latest'},\n                            'audio_backbone': {'adim': 256, 'aheads': 12, 'eunits': 2048, 'elayers': 12,\n                                               'transformer_input_layer': 'conv1d', 'dropout_rate': 0.1,\n                                               'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                               'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                               'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                               'ddim': 256, 'dheads': 30, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                               'transformer_length_normalized_loss': False, 'mtlalpha': 0.1, 'ctc_type': 'builtin',\n                                               'rel_pos_type': 'latest'},\n                            'audiovisual_backbone': {'adim': 256, 'aheads': 12, 'eunits': 2048, 'elayers': 12,\n                                                     'transformer_input_layer': 'conv3d', 'dropout_rate': 0.1,\n                                                     'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                     'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                                     'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                                     'ddim': 256, 'dheads': 12, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                     'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                     'ctc_type': 'builtin', 'rel_pos_type': 'latest', 'aux_adim': 256,\n                                                     'aux_aheads': 12, 'aux_eunits': 2048, 'aux_elayers': 12,\n                                                     'aux_transformer_input_layer': 'conv1d', 'aux_dropout_rate': 0.1,\n                                                     'aux_transformer_attn_dropout_rate': 0.1,\n                                                     'aux_transformer_encoder_attn_layer_type': 'rel_mha', 'aux_macaron_style': True,\n                                                     'aux_use_cnn_module': True, 'aux_cnn_module_kernel': 31, 'aux_zero_triu': False,\n                                                     'aux_a_upsample_ratio': 1, 'aux_relu_type': 'swish', 'aux_dunits': 2048,\n                                                     'aux_dlayers': 6, 'aux_lsm_weight': 0.1,\n                                                     'aux_transformer_length_normalized_loss': False, 'aux_mtlalpha': 0.1,\n                                                     'aux_ctc_type': 'builtin', 'aux_rel_pos_type': 'latest', 'fusion_hdim': 8192,\n                                                     'fusion_norm': 'batchnorm'}},\n                  'optimizer': {'name': 'adamw', 'lr': 0.001, 'warmup_epochs': 5, 'weight_decay': 0.03},\n                  'trainer': {'precision': 16, 'max_epochs': 20, 'num_nodes': 1, 'sync_batchnorm': True,\n                              'default_root_dir': '${exp_dir}', 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1,\n                              'gradient_clip_val': 5.0},\n                  'decode': {'name': 'default', 'snr_target': 999999}})","metadata":{"execution":{"iopub.status.busy":"2024-04-17T00:34:26.773789Z","iopub.execute_input":"2024-04-17T00:34:26.774763Z","iopub.status.idle":"2024-04-17T00:34:26.805392Z","shell.execute_reply.started":"2024-04-17T00:34:26.774715Z","shell.execute_reply":"2024-04-17T00:34:26.804317Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"modelmodule = ModelModule(cfg)\ndatamodule = DataModule(cfg)\ntrainer = Trainer(num_nodes=1)\n# Training and testing\nmodelmodule.model.load_state_dict(torch.load(cfg.pretrained_model_path, map_location=lambda storage, loc: storage))\ntrainer.test(model=modelmodule, datamodule=datamodule)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T00:34:28.435203Z","iopub.execute_input":"2024-04-17T00:34:28.435638Z","iopub.status.idle":"2024-04-17T00:43:03.857945Z","shell.execute_reply.started":"2024-04-17T00:34:28.435606Z","shell.execute_reply":"2024-04-17T00:43:03.856975Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3492487820.py:22: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")\n","output_type":"stream"},{"name":"stdout","text":"Numero de gpus 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c9656c1ce71451bbe67be85e5ca75a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m           wer           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4494706392288208    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">            wer            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4494706392288208     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"[{'wer': 1.4494706392288208}]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jiwer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}