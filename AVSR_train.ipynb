{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lrOIKAfWsW9R",
        "6LDIcdKAwaHC",
        "N_Y-Polmt8de",
        "PdZn0dVwu-dF",
        "cgj1aC53vk09",
        "EB1W0yKNwuDQ",
        "pqTbNgn_wUkN",
        "twaDpetGxIYG",
        "lTz0BtKMwMdz",
        "SDhdr-AvzGi6",
        "IEGOCDxVe4qB",
        "exinCbGXgAw4",
        "Oj8NjiA_f8Ny",
        "bM0t1pzbf9nX",
        "Y-xkRUYSfc5x",
        "CRX9rtXHfd2l",
        "azqaGmtUj3Dw",
        "gPL6F3hminyQ",
        "2pSgOj6Liukd",
        "7qQLfrWejOjd",
        "FiwOPaYzkiE7",
        "MwoN53F0kkFq",
        "s9rqrInBks8b",
        "JDY4Nxr7dbzg",
        "YvlnuzwKl8ya",
        "JAA0ah8jnioL",
        "apmMcHctoPrh",
        "iE1DYbe_pAeg",
        "dQAWTi24pfBa",
        "Cf9JstLvmyhc",
        "D6w-KSvPtOO3",
        "nnas4FCNuK-3",
        "w-Y9I92RqrVT",
        "P4_0WJ1RrLa0",
        "6-ciO8kw0Wda",
        "tYvrFGxC06jI",
        "3nQEmdFqNcjT",
        "5bvueiRMObJw",
        "gZ980wJWQULi",
        "0nHmNDnWQxcu",
        "9KU4cv09Qrkz",
        "kJJTggAkS5Pe",
        "TXA40fu0zMBS",
        "IzIaMo14x8me"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf"
      ],
      "metadata": {
        "id": "NQHlw5x6m29G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-nHfbP_HGZf",
        "outputId": "c5ecec7e-cd72-4845-aa5c-9c2546ade931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'auto_avsr'...\n",
            "remote: Enumerating objects: 268, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 268 (delta 34), reused 22 (delta 22), pack-reused 216\u001b[K\n",
            "Receiving objects: 100% (268/268), 31.46 MiB | 19.25 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mpc001/auto_avsr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq"
      ],
      "metadata": {
        "id": "ZCZMfAdNISz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fcfe82-af73-4b4b-9e15-9e5bdb640041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.12.25)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.2)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.25.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.10.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->fairseq)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->fairseq)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->fairseq)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->fairseq)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->fairseq)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->fairseq)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->fairseq)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->fairseq)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->fairseq)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->fairseq)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->fairseq)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291790 sha256=86be036fe0fdb961dd91173cda17557ea6e64a85484e11d54ffc081874951165\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=e1a500dd49f8c2aacc2f55d9ceda40ed797115e20a13e8233ada917fc7d5a75d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/pytorch/fairseq\n",
        "# %cd 'fairseq'\n",
        "# !pip install --editable ./"
      ],
      "metadata": {
        "id": "zYwM8B1YXYSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "qiIp4XKezxsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1942d12b-88d2-4a68-8ae6-b3e94e6d5f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.10.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.2 pytorch-lightning-2.2.1 torchmetrics-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p46Ifkno2UgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==1.5.10"
      ],
      "metadata": {
        "id": "LEyaaRzj2UdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jedi>=0.16\n",
        "!pip install setuptools>=65.5.1\n",
        "!pip install pytorch-lightning==1.5.10"
      ],
      "metadata": {
        "id": "pXn1RJgOXSUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9bdX-RJWhIZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pytorch-lightning==1.5.10\n",
        "#!pip install sentencepiece\n",
        "!pip install av\n",
        "!pip install gdown"
      ],
      "metadata": {
        "id": "8ldscqWJfg82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b246a6a-0e3b-4e4a-8750-88384c8010d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.0.0\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hydra-core --upgrade"
      ],
      "metadata": {
        "id": "6VyyOoCBfj4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show hydra-core"
      ],
      "metadata": {
        "id": "-grpGMgZbYdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descarga modelo prentrenado y dataset LRS3"
      ],
      "metadata": {
        "id": "lrOIKAfWsW9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "ZGjZ9GFpPbcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download(id='1G2-rEUNeGotJ9EtTIj0UzqbvCSbn6CJy', output='model.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "tvQcEa4nxZJJ",
        "outputId": "7daf7f3c-634e-4db2-e41b-e04b8ba4bc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1G2-rEUNeGotJ9EtTIj0UzqbvCSbn6CJy\n",
            "From (redirected): https://drive.google.com/uc?id=1G2-rEUNeGotJ9EtTIj0UzqbvCSbn6CJy&confirm=t&uuid=ab03050d-bf1a-4df7-848c-bb3c5c28ce50\n",
            "To: /content/model.zip\n",
            "100%|██████████| 9.81G/9.81G [01:47<00:00, 91.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o /content/model.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaS2AywRyE_o",
        "outputId": "0c995f09-158a-471c-ad23-60bacb3c5414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mSe truncaron las últimas líneas 5000 del resultado de transmisión.\u001b[0m\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5909052084016964405/00009.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6210299801668267679/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6210299801668267679/00006.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5682422121689376091/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682422121689376091/00002.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5540815761450059421/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00090.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00093.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00085.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00092.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00083.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5540815761450059421/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5951306831268451355/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5951306831268451355/00030.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6096542868370334952/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6096542868370334952/00014.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6265355699075927737/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6265355699075927737/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6189592475844010422/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6189592475844010422/00033.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6112781710219058868/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6112781710219058868/00037.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5991069638494833527/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5991069638494833527/00014.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6038591733638881165/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6038591733638881165/00018.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5966361550635111880/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966361550635111880/00008.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6104934804969353633/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6104934804969353633/00010.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6125390875206668804/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125390875206668804/00040.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5679743350586855665/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5679743350586855665/00036.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5955801085047056676/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5955801085047056676/00028.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6125715574734309899/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125715574734309899/00014.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5929987472604579145/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929987472604579145/00026.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5937931014618601501/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5937931014618601501/00016.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6097583968442246271/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6097583968442246271/00008.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6129434157419197933/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129434157419197933/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6281667984735645655/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6281667984735645655/00036.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6211443980955922166/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211443980955922166/00013.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5858194087774899826/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5858194087774899826/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5858194087774899826/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5858194087774899826/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5858194087774899826/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6124231234036812145/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124231234036812145/00006.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6290871670154253572/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290871670154253572/00010.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6229716060323243990/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6229716060323243990/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6229716060323243990/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6229716060323243990/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6229716060323243990/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6229716060323243990/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6229716060323243990/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6085363927491651922/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085363927491651922/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085363927491651922/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085363927491651922/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085363927491651922/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085363927491651922/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085363927491651922/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085363927491651922/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085363927491651922/00030.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6221498069899136887/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6221498069899136887/00028.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6118695880185727002/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6118695880185727002/00040.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6059574796362754902/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6059574796362754902/00013.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6240106445205774561/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6240106445205774561/00011.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5878623099718247676/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5878623099718247676/00003.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5949455270867143705/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5949455270867143705/00047.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6201791900951616599/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6201791900951616599/00015.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5625996559341423918/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5625996559341423918/00032.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6100593881523194452/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6100593881523194452/00016.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6113829252742555755/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6113829252742555755/00021.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6125383144265536003/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6125383144265536003/00053.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6093036886565962211/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093036886565962211/00054.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6083454385031848078/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083454385031848078/00028.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5952755094240730687/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5952755094240730687/00013.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5715020923465995537/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5715020923465995537/00025.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5653695232930083232/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653695232930083232/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6232677010777147805/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6232677010777147805/00002.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6169577069251241865/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6169577069251241865/00011.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5983004978403184340/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00086.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00085.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00084.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5983004978403184340/00078.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5710306337865199145/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5710306337865199145/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5566467024128680493/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5566467024128680493/00051.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6290574028920640730/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6290574028920640730/00021.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5984784383353860191/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5984784383353860191/00005.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6242704041426395698/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242704041426395698/00040.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6099836249292200550/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6099836249292200550/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6099836249292200550/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6099836249292200550/00002.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5917981321025328964/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5917981321025328964/00016.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6261629385319510182/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6261629385319510182/00016.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6144513358098645637/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6144513358098645637/00010.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6287225242920010832/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6287225242920010832/00027.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6213032689489126420/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00081.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00083.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6213032689489126420/00028.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5961823488190159446/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5961823488190159446/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6076836699422127254/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076836699422127254/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076836699422127254/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076836699422127254/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076836699422127254/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076836699422127254/00034.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5547541680235599469/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5547541680235599469/00004.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5882720498518649145/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5882720498518649145/00009.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5972909657773893974/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00088.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00086.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00092.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5972909657773893974/00030.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6060887767865882242/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6060887767865882242/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6060887767865882242/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6060887767865882242/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6060887767865882242/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6060887767865882242/00020.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6090559119932897267/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6090559119932897267/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6090559119932897267/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6090559119932897267/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6090559119932897267/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6090559119932897267/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6090559119932897267/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6090559119932897267/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6090559119932897267/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6105714341533489724/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6105714341533489724/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6105714341533489724/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6105714341533489724/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6105714341533489724/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6105714341533489724/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6105714341533489724/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6105714341533489724/00004.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5854800204617596265/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854800204617596265/00024.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6262646004078470529/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6262646004078470529/00038.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6077845587240023489/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077845587240023489/00049.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6130168596826811377/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6130168596826811377/00004.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5971768055466614577/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5971768055466614577/00035.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6083782950029991723/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6083782950029991723/00005.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6226320888675819792/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6226320888675819792/00008.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6135379251150230043/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6135379251150230043/00061.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6103168284920510398/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00095.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00083.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00094.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00093.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00086.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00081.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00088.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00090.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00089.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00096.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00091.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00098.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6103168284920510398/00049.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5939608628844355451/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5939608628844355451/00020.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5684930812086971130/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5684930812086971130/00034.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6074602457434810844/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6074602457434810844/00056.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5958437336103762271/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00081.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958437336103762271/00033.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5653730022165179587/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5653730022165179587/00009.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6138042560370547560/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6138042560370547560/00021.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5654132031104085226/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5654132031104085226/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5654132031104085226/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5654132031104085226/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6215931792283516867/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6215931792283516867/00061.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5682987768882257681/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5682987768882257681/00045.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5947571498211212414/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5947571498211212414/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5902967833345446762/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5902967833345446762/00037.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5869717055533282402/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5869717055533282402/00035.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5997030194108225012/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5997030194108225012/00014.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6066407659834703489/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6066407659834703489/00043.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6085267290727491911/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085267290727491911/00026.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6015518739827306440/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6015518739827306440/00014.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5648488444077134431/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5648488444077134431/00044.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5987041818165357049/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5987041818165357049/00030.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6223310975594781256/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6223310975594781256/00013.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5574337122201878384/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5574337122201878384/00018.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5993334804246742169/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5993334804246742169/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5993334804246742169/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5993334804246742169/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5993334804246742169/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5993334804246742169/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5993334804246742169/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5561271831687433454/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5561271831687433454/00011.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5929782602664560865/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929782602664560865/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929782602664560865/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929782602664560865/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929782602664560865/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929782602664560865/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5929782602664560865/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6017826425755458919/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6017826425755458919/00013.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6030427859801904207/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6030427859801904207/00024.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6122785548044912528/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6122785548044912528/00034.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6124656435799052753/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6124656435799052753/00037.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5958023730622736779/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958023730622736779/00008.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5542338756853221069/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00081.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542338756853221069/00004.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6220067845789506758/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6220067845789506758/00047.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6285027078788341775/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6285027078788341775/00015.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6093102599565591018/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6093102599565591018/00050.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6119132678359728652/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6119132678359728652/00017.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5860778799093633220/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5860778799093633220/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5860778799093633220/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5860778799093633220/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5860778799093633220/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5860778799093633220/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5860778799093633220/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5860778799093633220/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5860778799093633220/00011.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6077803067063793084/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077803067063793084/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6214872653348257156/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6214872653348257156/00002.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5990342930028348344/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990342930028348344/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6078272077492450549/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6078272077492450549/00014.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6231950302310690931/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6231950302310690931/00018.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5956187632103630886/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5956187632103630886/00018.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5932956153999576296/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5932956153999576296/00032.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5933559167407934721/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5933559167407934721/00019.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6025209474537289413/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6025209474537289413/00020.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5565365365017257953/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5565365365017257953/00018.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6258632357140358416/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6258632357140358416/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6258632357140358416/00009.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5562876001972492067/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00112.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00115.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00092.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00120.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00084.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00104.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00103.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00121.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00094.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00093.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00114.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00113.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00088.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00101.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00086.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00081.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00108.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00122.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00119.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00090.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00097.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00110.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00117.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00089.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00100.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00107.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00111.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00118.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00091.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00124.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5562876001972492067/00123.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6151706998822721186/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00088.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00084.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6151706998822721186/00083.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6234261853839799567/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00086.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00084.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00083.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00085.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6234261853839799567/00005.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5569064620349304030/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5569064620349304030/00031.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5921668979945676418/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5921668979945676418/00003.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5958785228324251846/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5958785228324251846/00019.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6058228324115475310/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6058228324115475310/00007.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6079364717172531830/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6079364717172531830/00060.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6242236319487862422/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6242236319487862422/00021.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5681963419182164492/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5681963419182164492/00002.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5970716647472558665/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970716647472558665/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970716647472558665/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970716647472558665/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970716647472558665/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970716647472558665/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970716647472558665/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970716647472558665/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970716647472558665/00001.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5998506803864589933/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5998506803864589933/00006.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5979233567750998189/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5979233567750998189/00062.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5666708983836943143/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5666708983836943143/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6195622610058056866/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6195622610058056866/00023.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6260462013208452361/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6260462013208452361/00018.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5973410880458038503/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5973410880458038503/00022.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5727686782021918545/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00093.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00094.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00113.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00114.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00121.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00126.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00128.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00130.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00105.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00102.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00085.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00129.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00120.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00115.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00112.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00095.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00092.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00083.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00084.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00103.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00104.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00109.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00089.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00100.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00124.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00111.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00098.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00118.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00091.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00096.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00101.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00106.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00081.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00108.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00086.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00119.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00090.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00110.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00099.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00125.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00122.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5727686782021918545/00073.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5542026942227535168/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5542026942227535168/00008.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6077934493063050698/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077934493063050698/00011.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5854865917617226996/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5854865917617226996/00047.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5970296599671005453/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5970296599671005453/00020.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6255224300590975746/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6255224300590975746/00004.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5957356292704936469/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5957356292704936469/00022.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6127207646372950530/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6127207646372950530/00018.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5966245586647891373/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5966245586647891373/00063.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5564240513082431391/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5564240513082431391/00028.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6257450811637224469/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6257450811637224469/00025.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5898290613960100635/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5898290613960100635/00025.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6225050437349598381/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6225050437349598381/00032.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6211490366602652865/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6211490366602652865/00005.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5748484302159323570/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5748484302159323570/00008.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5992171297606255672/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5992171297606255672/00003.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5677085195327362770/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5677085195327362770/00009.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6008549296526581801/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00081.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00083.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00084.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6008549296526581801/00043.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6246743458168222038/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00085.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00089.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00090.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00081.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6246743458168222038/00057.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5965124600053158030/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5965124600053158030/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5965124600053158030/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5965124600053158030/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5965124600053158030/00018.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6129805242593572339/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6129805242593572339/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5995874418408935143/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00093.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00094.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00114.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00077.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00066.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00115.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00112.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00092.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00083.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00103.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00104.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00109.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00089.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00098.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00091.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00118.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00101.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00088.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00106.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00086.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00097.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00090.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00119.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00099.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00110.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5995874418408935143/00073.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6224520867882064553/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6224520867882064553/00003.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5568322450000555157/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5568322450000555157/00004.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6076671772677960843/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076671772677960843/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076671772677960843/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076671772677960843/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076671772677960843/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076671772677960843/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076671772677960843/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6076671772677960843/00012.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5990025961441899397/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990025961441899397/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990025961441899397/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990025961441899397/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990025961441899397/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990025961441899397/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990025961441899397/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990025961441899397/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5990025961441899397/00007.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6085421909680566712/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00094.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00039.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00070.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00079.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00068.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00085.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00054.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00082.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00078.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00076.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00071.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00067.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00069.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00089.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00058.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00087.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00056.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00080.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00051.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00064.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00063.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00072.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00075.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00065.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00088.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00059.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00074.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00073.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6085421909680566712/00006.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6251927054197835730/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00057.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00055.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6251927054197835730/00003.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6027791608875644805/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6027791608875644805/00026.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5924637661340673854/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00060.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5924637661340673854/00021.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5582794771801168835/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5582794771801168835/00037.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5857069235840075244/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00047.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00035.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00062.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00048.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00046.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00037.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00053.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00061.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5857069235840075244/00029.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5587989964242415540/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00045.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00052.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00036.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00038.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00049.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00050.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5587989964242415540/00019.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6114583019503065736/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00041.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00042.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00029.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00044.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6114583019503065736/00004.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5988777414449021379/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00011.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5988777414449021379/00002.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/6077095685950143250/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00033.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/6077095685950143250/00007.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5658894290841894014/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00021.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00003.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00020.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00016.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00018.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00009.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00010.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00006.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00001.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5658894290841894014/00008.pkl  \n",
            "   creating: /content/LRS2_landmarks/pretrain/5571662216569927372/\n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00032.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00040.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00007.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00024.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00023.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00008.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00034.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00022.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00025.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00019.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00017.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00026.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00028.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00014.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00013.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00002.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00005.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00030.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00012.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00015.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00027.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00031.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00043.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00004.pkl  \n",
            "  inflating: /content/LRS2_landmarks/pretrain/5571662216569927372/00003.pkl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "px1FHSdyxZF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRTLdvQpxZDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download(id='1mLAuCnK2y7zbmiHlAXMqPSF_ApGqfbAD', output='model.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "JJKedkz0N-hW",
        "outputId": "1d3d5892-007d-4b7b-a84b-9f9ae8d1c529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1mLAuCnK2y7zbmiHlAXMqPSF_ApGqfbAD\n",
            "From (redirected): https://drive.google.com/uc?id=1mLAuCnK2y7zbmiHlAXMqPSF_ApGqfbAD&confirm=t&uuid=3af0a655-402b-4b28-9d79-f782a86f80e9\n",
            "To: /content/model.zip\n",
            "100%|██████████| 1.00G/1.00G [00:12<00:00, 83.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/auto_avsr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0pXBmEBUW0V",
        "outputId": "c52d752a-452a-4f00-f73e-577bb978baef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/auto_avsr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo.py  data.modality=['visual'] \\\n",
        "                pretrained_model_path=['/content/model.zip'] \\\n",
        "                file_path=['/content/clip.mp4']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b3zhh7oODmd",
        "outputId": "f8d668c5-743d-4855-a45d-292d1567d207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error executing job with overrides: ['data.modality=[visual]', 'pretrained_model_path=[/content/model.zip]', 'file_path=[/content/clip.mp4]']\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/auto_avsr/demo.py\", line 97, in main\n",
            "    pipeline = InferencePipeline(cfg)\n",
            "  File \"/content/auto_avsr/demo.py\", line 34, in __init__\n",
            "    self.modelmodule = ModelModule(cfg)\n",
            "UnboundLocalError: local variable 'ModelModule' referenced before assignment\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bxr6h_wGUAh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjQCh5vsscjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "GUufWC2jsdEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hydra\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DistributedSampler, RandomSampler\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning import seed_everything, Trainer\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "#from pytorch_lightning.plugins import DDPPlugin\n",
        "\n",
        "from fairseq.data import data_utils\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import sentencepiece\n",
        "from typing import Iterator, Optional\n",
        "from operator import itemgetter\n",
        "import warnings\n",
        "from typing import Any, List, Tuple, Dict, NamedTuple, Union\n",
        "import six\n",
        "from itertools import chain\n",
        "import copy"
      ],
      "metadata": {
        "id": "7-vB2bNLtIeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataModule (Dataset)"
      ],
      "metadata": {
        "id": "6LDIcdKAwaHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AVDataset"
      ],
      "metadata": {
        "id": "N_Y-Polmt8de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_or_pad(data, size, dim=0):\n",
        "    \"\"\"\n",
        "    Pads or trims the data along a dimension.\n",
        "    \"\"\"\n",
        "    if data.size(dim) < size:\n",
        "        padding = size - data.size(dim)\n",
        "        data = torch.nn.functional.pad(data, (0, 0, 0, padding), \"constant\")\n",
        "        size = data.size(dim)\n",
        "    elif data.size(dim) > size:\n",
        "        data = data[:size]\n",
        "    assert data.size(dim) == size\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_video(path):\n",
        "    \"\"\"\n",
        "    rtype: torch, T x C x H x W\n",
        "    \"\"\"\n",
        "    vid = torchvision.io.read_video(path, pts_unit=\"sec\", output_format=\"THWC\")[0]\n",
        "    vid = vid.permute((0, 3, 1, 2))\n",
        "    return vid\n",
        "\n",
        "\n",
        "def load_audio(path):\n",
        "    \"\"\"\n",
        "    rtype: torch, T x 1\n",
        "    \"\"\"\n",
        "    waveform, sample_rate = torchaudio.load(path[:-4] + \".wav\", normalize=True)\n",
        "    return waveform.transpose(1, 0)\n",
        "\n",
        "\n",
        "class AVDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir,\n",
        "        label_path,\n",
        "        subset,\n",
        "        modality,\n",
        "        audio_transform,\n",
        "        video_transform,\n",
        "        rate_ratio=640,\n",
        "    ):\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "\n",
        "        self.modality = modality\n",
        "        self.rate_ratio = rate_ratio\n",
        "\n",
        "        self.list = self.load_list(label_path)\n",
        "\n",
        "        self.audio_transform = audio_transform\n",
        "        self.video_transform = video_transform\n",
        "\n",
        "    def load_list(self, label_path):\n",
        "        paths_counts_labels = []\n",
        "        for path_count_label in open(label_path).read().splitlines()[1:]:\n",
        "            id, dataset_name, rel_path, input_length, token_id = path_count_label.split(\",\")\n",
        "            #print(input_length)\n",
        "            paths_counts_labels.append(\n",
        "                (\n",
        "                    dataset_name,\n",
        "                    rel_path,\n",
        "                    int(input_length),\n",
        "                    torch.tensor([int(_) for _ in token_id.split()]),\n",
        "                )\n",
        "            )\n",
        "        return paths_counts_labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dataset_name, rel_path, input_length, token_id = self.list[idx]\n",
        "        path = os.path.join(self.root_dir, dataset_name, rel_path)\n",
        "        if self.modality == \"video\":\n",
        "            video = load_video(path)\n",
        "            video = self.video_transform(video)\n",
        "            return {\"input\": video, \"target\": token_id}\n",
        "        elif self.modality == \"audio\":\n",
        "            audio = load_audio(path)\n",
        "            audio = self.audio_transform(audio)\n",
        "            return {\"input\": audio, \"target\": token_id}\n",
        "        elif self.modality == \"audiovisual\":\n",
        "            video = load_video(path)\n",
        "            audio = load_audio(path)\n",
        "            audio = cut_or_pad(audio, len(video) * self.rate_ratio)\n",
        "            video = self.video_transform(video)\n",
        "            audio = self.audio_transform(audio)\n",
        "            return {\"video\": video, \"audio\": audio, \"target\": token_id}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)"
      ],
      "metadata": {
        "id": "ZLIGNqVUt_i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tranformaciones"
      ],
      "metadata": {
        "id": "PdZn0dVwu-dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NOISE_FILENAME = os.path.join(\n",
        "    os.path.dirname(os.path.abspath('')),\n",
        "    \"content\",\n",
        "    \"auto_avsr\",\n",
        "    \"datamodule\",\n",
        "    \"babble_noise.wav\"\n",
        ")\n",
        "\n",
        "SP_MODEL_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"auto_avsr\",\n",
        "    \"spm\",\n",
        "    \"unigram\",\n",
        "    \"unigram40.model\",\n",
        ")\n",
        "\n",
        "DICT_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"auto_avsr\",\n",
        "    \"spm\",\n",
        "    \"unigram\",\n",
        "    \"unigram40_units.txt\",\n",
        ")\n",
        "\n",
        "\n",
        "class FunctionalModule(torch.nn.Module):\n",
        "    def __init__(self, functional):\n",
        "        super().__init__()\n",
        "        self.functional = functional\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.functional(input)\n",
        "\n",
        "\n",
        "class AdaptiveTimeMask(torch.nn.Module):\n",
        "    def __init__(self, window, stride):\n",
        "        super().__init__()\n",
        "        self.window = window\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [T, ...]\n",
        "        cloned = x.clone()\n",
        "        length = cloned.size(0)\n",
        "        n_mask = int((length + self.stride - 0.1) // self.stride)\n",
        "        ts = torch.randint(0, self.window, size=(n_mask, 2))\n",
        "        for t, t_end in ts:\n",
        "            if length - t <= 0:\n",
        "                continue\n",
        "            t_start = random.randrange(0, length - t)\n",
        "            if t_start == t_start + t:\n",
        "                continue\n",
        "            t_end += t_start\n",
        "            cloned[t_start:t_end] = 0\n",
        "        return cloned\n",
        "\n",
        "\n",
        "class AddNoise(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        noise_filename=NOISE_FILENAME,\n",
        "        snr_target=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.snr_levels = [snr_target] if snr_target else [-5, 0, 5, 10, 15, 20, 999999]\n",
        "        self.noise, sample_rate = torchaudio.load(noise_filename)\n",
        "        assert sample_rate == 16000\n",
        "\n",
        "    def forward(self, speech):\n",
        "        # speech: T x 1\n",
        "        # return: T x 1\n",
        "        speech = speech.t()\n",
        "        start_idx = random.randint(0, self.noise.shape[1] - speech.shape[1])\n",
        "        noise_segment = self.noise[:, start_idx : start_idx + speech.shape[1]]\n",
        "        snr_level = torch.tensor([random.choice(self.snr_levels)])\n",
        "        noisy_speech = torchaudio.functional.add_noise(speech, noise_segment, snr_level)\n",
        "        return noisy_speech.t()\n",
        "\n",
        "\n",
        "class VideoTransform:\n",
        "    def __init__(self, subset):\n",
        "        if subset == \"train\":\n",
        "            self.video_pipeline = torch.nn.Sequential(\n",
        "                FunctionalModule(lambda x: x / 255.0),\n",
        "                torchvision.transforms.RandomCrop(88),\n",
        "                torchvision.transforms.Grayscale(),\n",
        "                AdaptiveTimeMask(10, 25),\n",
        "                torchvision.transforms.Normalize(0.421, 0.165),\n",
        "            )\n",
        "        elif subset == \"val\" or subset == \"test\":\n",
        "            self.video_pipeline = torch.nn.Sequential(\n",
        "                FunctionalModule(lambda x: x / 255.0),\n",
        "                torchvision.transforms.CenterCrop(88),\n",
        "                torchvision.transforms.Grayscale(),\n",
        "                torchvision.transforms.Normalize(0.421, 0.165),\n",
        "            )\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        # sample: T x C x H x W\n",
        "        # rtype: T x 1 x H x W\n",
        "        return self.video_pipeline(sample)\n",
        "\n",
        "\n",
        "class AudioTransform:\n",
        "    def __init__(self, subset, snr_target=None):\n",
        "        if subset == \"train\":\n",
        "            self.audio_pipeline = torch.nn.Sequential(\n",
        "                AdaptiveTimeMask(6400, 16000),\n",
        "                AddNoise(),\n",
        "                FunctionalModule(\n",
        "                    lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=1e-8)\n",
        "                ),\n",
        "            )\n",
        "        elif subset == \"val\" or subset == \"test\":\n",
        "            self.audio_pipeline = torch.nn.Sequential(\n",
        "                AddNoise(snr_target=snr_target)\n",
        "                if snr_target is not None\n",
        "                else FunctionalModule(lambda x: x),\n",
        "                FunctionalModule(\n",
        "                    lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=1e-8)\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        # sample: T x 1\n",
        "        # rtype: T x 1\n",
        "        return self.audio_pipeline(sample)\n",
        "\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Mapping Dictionary Class for SentencePiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sp_model_path=SP_MODEL_PATH,\n",
        "        dict_path=DICT_PATH,\n",
        "    ):\n",
        "\n",
        "        # Load SentencePiece model\n",
        "        self.spm = sentencepiece.SentencePieceProcessor(model_file=sp_model_path)\n",
        "\n",
        "        # Load units and create dictionary\n",
        "        units = open(dict_path, encoding='utf8').read().splitlines()\n",
        "        self.hashmap = {unit.split()[0]: unit.split()[-1] for unit in units}\n",
        "        # 0 will be used for \"blank\" in CTC\n",
        "        self.token_list = [\"<blank>\"] + list(self.hashmap.keys()) + [\"<eos>\"]\n",
        "        self.ignore_id = -1\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = self.spm.EncodeAsPieces(text)\n",
        "        token_ids = [self.hashmap.get(token, self.hashmap[\"<unk>\"]) for token in tokens]\n",
        "        return torch.tensor(list(map(int, token_ids)))\n",
        "\n",
        "    def post_process(self, token_ids):\n",
        "        token_ids = token_ids[token_ids != -1]\n",
        "        text = self._ids_to_str(token_ids, self.token_list)\n",
        "        text = text.replace(\"\\u2581\", \" \").strip()\n",
        "        return text\n",
        "\n",
        "    def _ids_to_str(self, token_ids, char_list):\n",
        "        token_as_list = [char_list[idx] for idx in token_ids]\n",
        "        return \"\".join(token_as_list).replace(\"<space>\", \" \")"
      ],
      "metadata": {
        "id": "C_o27zBmvAek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EJqbLHgyvkRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ByFrameCountSampler"
      ],
      "metadata": {
        "id": "cgj1aC53vk09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ByFrameCountSampler(Sampler):\n",
        "    def __init__(self, dataset, max_frames_per_gpu, shuffle=True, seed=0):\n",
        "        self.dataset = dataset\n",
        "        self.max_frames_per_gpu = max_frames_per_gpu\n",
        "        self.sizes = [item[2] for item in self.dataset.list]\n",
        "\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.epoch = 0\n",
        "\n",
        "        batch_indices = data_utils.batch_by_size(\n",
        "            self._get_indices(), lambda i: self.sizes[i], max_tokens=max_frames_per_gpu\n",
        "        )\n",
        "        self.num_batches = len(batch_indices)\n",
        "\n",
        "    def _get_indices(self):\n",
        "        if self.shuffle:  # shuffles indices corresponding to equal lengths\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(self.seed + self.epoch)\n",
        "            order = [torch.randperm(len(self.dataset), generator=g).tolist()]\n",
        "        else:\n",
        "            order = [list(range(len(self.dataset)))]\n",
        "        order.append(self.sizes)\n",
        "        return np.lexsort(order)[::-1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch_indices = data_utils.batch_by_size(\n",
        "            self._get_indices(),\n",
        "            lambda i: self.sizes[i],\n",
        "            max_tokens=self.max_frames_per_gpu,\n",
        "        )\n",
        "        return iter(batch_indices)\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        self.epoch = epoch"
      ],
      "metadata": {
        "id": "zq2myP33vlTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DatasetFromSampler"
      ],
      "metadata": {
        "id": "EB1W0yKNwuDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetFromSampler(Dataset):\n",
        "    \"\"\"Dataset to create indexes from `Sampler`.\n",
        "    Args:\n",
        "        sampler: PyTorch sampler\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sampler: Sampler):\n",
        "        \"\"\"Initialisation for DatasetFromSampler.\"\"\"\n",
        "        self.sampler = sampler\n",
        "        self.sampler_list = None\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \"\"\"Gets element of the dataset.\n",
        "        Args:\n",
        "            index: index of the element in the dataset\n",
        "        Returns:\n",
        "            Single element by index\n",
        "        \"\"\"\n",
        "        if self.sampler_list is None:\n",
        "            self.sampler_list = list(self.sampler)\n",
        "        return self.sampler_list[index]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.sampler)\n"
      ],
      "metadata": {
        "id": "Mwmb5zDLwu_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistributedSamplerWrapper"
      ],
      "metadata": {
        "id": "pqTbNgn_wUkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistributedSamplerWrapper(DistributedSampler):\n",
        "    \"\"\"\n",
        "    Wrapper over `Sampler` for distributed training.\n",
        "    Allows you to use any sampler in distributed mode.\n",
        "    It is especially useful in conjunction with\n",
        "    `torch.nn.parallel.DistributedDataParallel`. In such case, each\n",
        "    process can pass a DistributedSamplerWrapper instance as a DataLoader\n",
        "    sampler, and load a subset of subsampled data of the original dataset\n",
        "    that is exclusive to it.\n",
        "    .. note::\n",
        "        Sampler is assumed to be of constant size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sampler,\n",
        "        num_replicas: Optional[int] = None,\n",
        "        rank: Optional[int] = None,\n",
        "        shuffle: bool = True,\n",
        "        drop_last: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sampler: Sampler used for subsampling\n",
        "            num_replicas (int, optional): Number of processes participating in\n",
        "                distributed training\n",
        "            rank (int, optional): Rank of the current process\n",
        "                within ``num_replicas``\n",
        "            shuffle (bool, optional): If true (default),\n",
        "                sampler will shuffle the indices\n",
        "        \"\"\"\n",
        "        super(DistributedSamplerWrapper, self).__init__(\n",
        "            DatasetFromSampler(sampler),\n",
        "            num_replicas=num_replicas,\n",
        "            rank=rank,\n",
        "            shuffle=shuffle,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        self.sampler = sampler\n",
        "\n",
        "    def __iter__(self) -> Iterator[int]:\n",
        "        \"\"\"Iterate over sampler.\n",
        "        Returns:\n",
        "            python iterator\n",
        "        \"\"\"\n",
        "        self.dataset = DatasetFromSampler(self.sampler)\n",
        "        indexes_of_indexes = super().__iter__()\n",
        "\n",
        "        subsampler_indexes = self.dataset\n",
        "        return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        super().set_epoch(epoch)\n",
        "        self.sampler.set_epoch(epoch)"
      ],
      "metadata": {
        "id": "IyT2ML3IwTx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RandomSamplerWrapper"
      ],
      "metadata": {
        "id": "twaDpetGxIYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomSamplerWrapper(RandomSampler):\n",
        "    def __init__(self, sampler):\n",
        "        super(RandomSamplerWrapper, self).__init__(DatasetFromSampler(sampler))\n",
        "        self.sampler = sampler\n",
        "\n",
        "    def __iter__(self) -> Iterator[int]:\n",
        "        \"\"\"Iterate over sampler.\n",
        "        Returns:\n",
        "            python iterator\n",
        "        \"\"\"\n",
        "        self.dataset = DatasetFromSampler(self.sampler)\n",
        "        indexes_of_indexes = super().__iter__()\n",
        "        subsampler_indexes = self.dataset\n",
        "        return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))"
      ],
      "metadata": {
        "id": "roBfR_dCxJbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataModule"
      ],
      "metadata": {
        "id": "lTz0BtKMwMdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(samples, pad_val=0.0):\n",
        "    lengths = [len(s) for s in samples]\n",
        "    max_size = max(lengths)\n",
        "    sample_shape = list(samples[0].shape[1:])\n",
        "    collated_batch = samples[0].new_zeros([len(samples), max_size] + sample_shape)\n",
        "    for i, sample in enumerate(samples):\n",
        "        diff = len(sample) - max_size\n",
        "        if diff == 0:\n",
        "            collated_batch[i] = sample\n",
        "        else:\n",
        "            collated_batch[i] = torch.cat(\n",
        "                [sample, sample.new_full([-diff] + sample_shape, pad_val)]\n",
        "            )\n",
        "    if len(samples[0].shape) == 1:\n",
        "        collated_batch = collated_batch.unsqueeze(1)  # targets\n",
        "    elif len(samples[0].shape) == 2:\n",
        "        pass  # collated_batch: [B, T, 1]\n",
        "    elif len(samples[0].shape) == 4:\n",
        "        pass  # collated_batch: [B, T, C, H, W]\n",
        "    return collated_batch, lengths\n",
        "\n",
        "def collate_pad(batch):\n",
        "    batch_out = {}\n",
        "    for data_type in batch[0].keys():\n",
        "        pad_val = -1 if data_type == \"target\" else 0.0\n",
        "        c_batch, sample_lengths = pad(\n",
        "            [s[data_type] for s in batch if s[data_type] is not None], pad_val\n",
        "        )\n",
        "        batch_out[data_type + \"s\"] = c_batch\n",
        "        batch_out[data_type + \"_lengths\"] = torch.tensor(sample_lengths)\n",
        "    return batch_out\n",
        "\n",
        "\n",
        "class DataModule(LightningDataModule):\n",
        "    def __init__(self, cfg=None):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.cfg.gpus = torch.cuda.device_count()\n",
        "        self.total_gpus = self.cfg.gpus * self.cfg.trainer.num_nodes\n",
        "        print('Numero de gpus',self.total_gpus)\n",
        "\n",
        "    def _dataloader(self, ds, sampler, collate_fn):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            ds,\n",
        "            num_workers=12,\n",
        "            pin_memory=True,\n",
        "            batch_sampler=sampler,\n",
        "            collate_fn=collate_fn,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        ds_args = self.cfg.data.dataset\n",
        "        train_ds = AVDataset(\n",
        "            root_dir=ds_args.root_dir,\n",
        "            label_path=os.path.join(\n",
        "                ds_args.root_dir, ds_args.label_dir, ds_args.train_file\n",
        "            ),\n",
        "            subset=\"train\",\n",
        "            modality=self.cfg.data.modality,\n",
        "            audio_transform=AudioTransform(\"train\"),\n",
        "            video_transform=VideoTransform(\"train\"),\n",
        "        )\n",
        "        sampler = ByFrameCountSampler(train_ds, self.cfg.data.max_frames)\n",
        "        if self.total_gpus > 1:\n",
        "            sampler = DistributedSamplerWrapper(sampler)\n",
        "        else:\n",
        "            sampler = RandomSamplerWrapper(sampler)\n",
        "        return self._dataloader(train_ds, sampler, collate_pad)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        ds_args = self.cfg.data.dataset\n",
        "        val_ds = AVDataset(\n",
        "            root_dir=ds_args.root_dir,\n",
        "            label_path=os.path.join(ds_args.root_dir, ds_args.label_dir, ds_args.val_file),\n",
        "            subset=\"val\",\n",
        "            modality=self.cfg.data.modality,\n",
        "            audio_transform=AudioTransform(\"val\"),\n",
        "            video_transform=VideoTransform(\"val\"),\n",
        "        )\n",
        "        sampler = ByFrameCountSampler(\n",
        "            val_ds, self.cfg.data.max_frames_val, shuffle=False\n",
        "        )\n",
        "        if self.total_gpus > 1:\n",
        "            sampler = DistributedSamplerWrapper(sampler, shuffle=False, drop_last=True)\n",
        "        return self._dataloader(val_ds, sampler, collate_pad)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        ds_args = self.cfg.data.dataset\n",
        "        dataset = AVDataset(\n",
        "            root_dir=ds_args.root_dir,\n",
        "            label_path=os.path.join(ds_args.root_dir, ds_args.label_dir, ds_args.test_file),\n",
        "            subset=\"test\",\n",
        "            modality=self.cfg.data.modality,\n",
        "            audio_transform=AudioTransform(\n",
        "                \"test\", snr_target=self.cfg.decode.snr_target\n",
        "            ),\n",
        "            video_transform=VideoTransform(\"test\"),\n",
        "        )\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=None)\n",
        "        return dataloader"
      ],
      "metadata": {
        "id": "F2mb_uPzs-g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ModelModule (Modelo)"
      ],
      "metadata": {
        "id": "SDhdr-AvzGi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RelPositionalEncoding"
      ],
      "metadata": {
        "id": "IEGOCDxVe4qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RelPositionalEncoding(torch.nn.Module):\n",
        "    \"\"\"Relative positional encoding module (new implementation).\n",
        "    Details can be found in https://github.com/espnet/espnet/pull/2816.\n",
        "    See : Appendix B in https://arxiv.org/abs/1901.02860\n",
        "    Args:\n",
        "        d_model (int): Embedding dimension.\n",
        "        dropout_rate (float): Dropout rate.\n",
        "        max_len (int): Maximum input length.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout_rate, max_len=5000):\n",
        "        \"\"\"Construct an PositionalEncoding object.\"\"\"\n",
        "        super(RelPositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.xscale = math.sqrt(self.d_model)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "        self.pe = None\n",
        "        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n",
        "\n",
        "    def extend_pe(self, x):\n",
        "        \"\"\"Reset the positional encodings.\"\"\"\n",
        "        if self.pe is not None:\n",
        "            # self.pe contains both positive and negative parts\n",
        "            # the length of self.pe is 2 * input_len - 1\n",
        "            if self.pe.size(1) >= x.size(1) * 2 - 1:\n",
        "                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n",
        "                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n",
        "                return\n",
        "        # Suppose `i` means to the position of query vecotr and `j` means the\n",
        "        # position of key vector. We use position relative positions when keys\n",
        "        # are to the left (i>j) and negative relative positions otherwise (i<j).\n",
        "        pe_positive = torch.zeros(x.size(1), self.d_model)\n",
        "        pe_negative = torch.zeros(x.size(1), self.d_model)\n",
        "        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n",
        "            * -(math.log(10000.0) / self.d_model)\n",
        "        )\n",
        "        pe_positive[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_positive[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)\n",
        "        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)\n",
        "\n",
        "        # Reserve the order of positive indices and concat both positive and\n",
        "        # negative indices. This is used to support the shifting trick\n",
        "        # as in https://arxiv.org/abs/1901.02860\n",
        "        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n",
        "        pe_negative = pe_negative[1:].unsqueeze(0)\n",
        "        pe = torch.cat([pe_positive, pe_negative], dim=1)\n",
        "        self.pe = pe.to(device=x.device, dtype=x.dtype)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"Add positional encoding.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (batch, time, `*`).\n",
        "        Returns:\n",
        "            torch.Tensor: Encoded tensor (batch, time, `*`).\n",
        "        \"\"\"\n",
        "        self.extend_pe(x)\n",
        "        x = x * self.xscale\n",
        "        pos_emb = self.pe[\n",
        "            :,\n",
        "            self.pe.size(1) // 2 - x.size(1) + 1 : self.pe.size(1) // 2 + x.size(1),\n",
        "        ]\n",
        "        return self.dropout(x), self.dropout(pos_emb)"
      ],
      "metadata": {
        "id": "whTGzHZNe51i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swish"
      ],
      "metadata": {
        "id": "exinCbGXgAw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(nn.Module):\n",
        "    \"\"\"Construct an Swish object.\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Return Swich activation function.\"\"\"\n",
        "        return x * torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "ZSQU5guKfWUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet 2D"
      ],
      "metadata": {
        "id": "Oj8NjiA_f8Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"conv3x3.\n",
        "\n",
        "    :param in_planes: int, number of channels in the input sequence.\n",
        "    :param out_planes: int,  number of channels produced by the convolution.\n",
        "    :param stride: int, size of the convolving kernel.\n",
        "    \"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def downsample_basic_block(inplanes, outplanes, stride):\n",
        "    \"\"\"downsample_basic_block.\n",
        "\n",
        "    :param inplanes: int, number of channels in the input sequence.\n",
        "    :param outplanes: int, number of channels produced by the convolution.\n",
        "    :param stride: int, size of the convolving kernel.\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            inplanes,\n",
        "            outplanes,\n",
        "            kernel_size=1,\n",
        "            stride=stride,\n",
        "            bias=False,\n",
        "        ),\n",
        "        nn.BatchNorm2d(outplanes),\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        relu_type=\"swish\",\n",
        "    ):\n",
        "        \"\"\"__init__.\n",
        "\n",
        "        :param inplanes: int, number of channels in the input sequence.\n",
        "        :param planes: int,  number of channels produced by the convolution.\n",
        "        :param stride: int, size of the convolving kernel.\n",
        "        :param downsample: boolean, if True, the temporal resolution is downsampled.\n",
        "        :param relu_type: str, type of activation function.\n",
        "        \"\"\"\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        assert relu_type in [\"relu\", \"prelu\", \"swish\"]\n",
        "\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        if relu_type == \"relu\":\n",
        "            self.relu1 = nn.ReLU(inplace=True)\n",
        "            self.relu2 = nn.ReLU(inplace=True)\n",
        "        elif relu_type == \"prelu\":\n",
        "            self.relu1 = nn.PReLU(num_parameters=planes)\n",
        "            self.relu2 = nn.PReLU(num_parameters=planes)\n",
        "        elif relu_type == \"swish\":\n",
        "            self.relu1 = Swish()\n",
        "            self.relu2 = Swish()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # --------\n",
        "\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        :param x: torch.Tensor, input tensor with input size (B, C, T, H, W).\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        relu_type=\"swish\",\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.relu_type = relu_type\n",
        "        self.downsample_block = downsample_basic_block\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        \"\"\"_make_layer.\n",
        "\n",
        "        :param block: torch.nn.Module, class of blocks.\n",
        "        :param planes: int,  number of channels produced by the convolution.\n",
        "        :param blocks: int, number of layers in a block.\n",
        "        :param stride: int, size of the convolving kernel.\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = self.downsample_block(\n",
        "                inplanes=self.inplanes,\n",
        "                outplanes=planes * block.expansion,\n",
        "                stride=stride,\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                relu_type=self.relu_type,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    relu_type=self.relu_type,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        :param x: torch.Tensor, input tensor with input size (B, C, T, H, W).\n",
        "        \"\"\"\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "E3WrvoRKf9U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet 1D"
      ],
      "metadata": {
        "id": "bM0t1pzbf9nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x31D(in_planes, out_planes, stride=1):\n",
        "    \"\"\"conv3x3.\n",
        "\n",
        "    :param in_planes: int, number of channels in the input sequence.\n",
        "    :param out_planes: int,  number of channels produced by the convolution.\n",
        "    :param stride: int, size of the convolving kernel.\n",
        "    \"\"\"\n",
        "    return nn.Conv1d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def downsample_basic_block1D(inplanes, outplanes, stride):\n",
        "    \"\"\"downsample_basic_block.\n",
        "\n",
        "    :param inplanes: int, number of channels in the input sequence.\n",
        "    :param outplanes: int, number of channels produced by the convolution.\n",
        "    :param stride: int, size of the convolving kernel.\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv1d(\n",
        "            inplanes,\n",
        "            outplanes,\n",
        "            kernel_size=1,\n",
        "            stride=stride,\n",
        "            bias=False,\n",
        "        ),\n",
        "        nn.BatchNorm1d(outplanes),\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock1D(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        relu_type=\"relu\",\n",
        "    ):\n",
        "        \"\"\"__init__.\n",
        "\n",
        "        :param inplanes: int, number of channels in the input sequence.\n",
        "        :param planes: int,  number of channels produced by the convolution.\n",
        "        :param stride: int, size of the convolving kernel.\n",
        "        :param downsample: boolean, if True, the temporal resolution is downsampled.\n",
        "        :param relu_type: str, type of activation function.\n",
        "        \"\"\"\n",
        "        super(BasicBlock1D, self).__init__()\n",
        "\n",
        "        assert relu_type in [\"relu\", \"prelu\", \"swish\"]\n",
        "\n",
        "        self.conv1 = conv3x31D(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "\n",
        "        # type of ReLU is an input option\n",
        "        if relu_type == \"relu\":\n",
        "            self.relu1 = nn.ReLU(inplace=True)\n",
        "            self.relu2 = nn.ReLU(inplace=True)\n",
        "        elif relu_type == \"prelu\":\n",
        "            self.relu1 = nn.PReLU(num_parameters=planes)\n",
        "            self.relu2 = nn.PReLU(num_parameters=planes)\n",
        "        elif relu_type == \"swish\":\n",
        "            self.relu1 = Swish()\n",
        "            self.relu2 = Swish()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # --------\n",
        "\n",
        "        self.conv2 = conv3x31D(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        :param x: torch.Tensor, input tensor with input size (B, C, T)\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet1D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        relu_type=\"swish\",\n",
        "        a_upsample_ratio=1,\n",
        "    ):\n",
        "        \"\"\"__init__.\n",
        "\n",
        "        :param block: torch.nn.Module, class of blocks.\n",
        "        :param layers: List, customised layers in each block.\n",
        "        :param relu_type: str, type of activation function.\n",
        "        :param a_upsample_ratio: int, The ratio related to the \\\n",
        "            temporal resolution of output features of the frontend. \\\n",
        "            a_upsample_ratio=1 produce features with a fps of 25.\n",
        "        \"\"\"\n",
        "        super(ResNet1D, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.relu_type = relu_type\n",
        "        self.downsample_block = downsample_basic_block1D\n",
        "        self.a_upsample_ratio = a_upsample_ratio\n",
        "\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=self.inplanes,\n",
        "            kernel_size=80,\n",
        "            stride=4,\n",
        "            padding=38,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(self.inplanes)\n",
        "\n",
        "        if relu_type == \"relu\":\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "        elif relu_type == \"prelu\":\n",
        "            self.relu = nn.PReLU(num_parameters=self.inplanes)\n",
        "        elif relu_type == \"swish\":\n",
        "            self.relu = Swish()\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool1d(\n",
        "            kernel_size=20 // self.a_upsample_ratio,\n",
        "            stride=20 // self.a_upsample_ratio,\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        \"\"\"_make_layer.\n",
        "\n",
        "        :param block: torch.nn.Module, class of blocks.\n",
        "        :param planes: int,  number of channels produced by the convolution.\n",
        "        :param blocks: int, number of layers in a block.\n",
        "        :param stride: int, size of the convolving kernel.\n",
        "        \"\"\"\n",
        "\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = self.downsample_block(\n",
        "                inplanes=self.inplanes,\n",
        "                outplanes=planes * block.expansion,\n",
        "                stride=stride,\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                relu_type=self.relu_type,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    relu_type=self.relu_type,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        :param x: torch.Tensor, input tensor with input size (B, C, T)\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6-OF93Qpf_ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv3dResNet"
      ],
      "metadata": {
        "id": "Y-xkRUYSfc5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def threeD_to_2D_tensor(x):\n",
        "    n_batch, n_channels, s_time, sx, sy = x.shape\n",
        "    x = x.transpose(1, 2)\n",
        "    return x.reshape(n_batch * s_time, n_channels, sx, sy)\n",
        "\n",
        "\n",
        "class Conv3dResNet(torch.nn.Module):\n",
        "    \"\"\"Conv3dResNet module\"\"\"\n",
        "\n",
        "    def __init__(self, backbone_type=\"resnet\", relu_type=\"swish\"):\n",
        "        \"\"\"__init__.\n",
        "\n",
        "        :param backbone_type: str, the type of a visual front-end.\n",
        "        :param relu_type: str, activation function used in an audio front-end.\n",
        "        \"\"\"\n",
        "        super(Conv3dResNet, self).__init__()\n",
        "        self.frontend_nout = 64\n",
        "        self.trunk = ResNet(BasicBlock, [2, 2, 2, 2], relu_type=relu_type)\n",
        "        self.frontend3D = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                1, self.frontend_nout, (5, 7, 7), (1, 2, 2), (2, 3, 3), bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(self.frontend_nout),\n",
        "            Swish(),\n",
        "            nn.MaxPool3d((1, 3, 3), (1, 2, 2), (0, 1, 1)),\n",
        "        )\n",
        "\n",
        "    def forward(self, xs_pad):\n",
        "        xs_pad = xs_pad.transpose(1, 2)  # [B, T, C, H, W] -> [B, C, T, H, W]\n",
        "\n",
        "        B, C, T, H, W = xs_pad.size()\n",
        "        xs_pad = self.frontend3D(xs_pad)\n",
        "        Tnew = xs_pad.shape[2]\n",
        "        xs_pad = threeD_to_2D_tensor(xs_pad)\n",
        "        xs_pad = self.trunk(xs_pad)\n",
        "        return xs_pad.view(B, Tnew, xs_pad.size(1))"
      ],
      "metadata": {
        "id": "jMIj_0NBfsEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv1dResNet"
      ],
      "metadata": {
        "id": "CRX9rtXHfd2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1dResNet(torch.nn.Module):\n",
        "    def __init__(self, relu_type=\"swish\", a_upsample_ratio=1):\n",
        "        super().__init__()\n",
        "        self.a_upsample_ratio = a_upsample_ratio\n",
        "        self.trunk = ResNet1D(\n",
        "            BasicBlock1D,\n",
        "            [2, 2, 2, 2],\n",
        "            relu_type=relu_type,\n",
        "            a_upsample_ratio=a_upsample_ratio,\n",
        "        )\n",
        "\n",
        "    def forward(self, xs_pad):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        :param xs_pad: torch.Tensor, batch of padded input sequences (B, Tmax, idim)\n",
        "        \"\"\"\n",
        "        B, T, C = xs_pad.size()\n",
        "        xs_pad = xs_pad[:, : T // 640 * 640, :]\n",
        "        xs_pad = xs_pad.transpose(1, 2)\n",
        "        xs_pad = self.trunk(xs_pad)\n",
        "        return xs_pad.transpose(1, 2)"
      ],
      "metadata": {
        "id": "a1d5lvfEhFg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadedAttention y RelPositionMultiHeadedAttention"
      ],
      "metadata": {
        "id": "azqaGmtUj3Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention layer.\n",
        "    Args:\n",
        "        n_head (int): The number of heads.\n",
        "        n_feat (int): The number of features.\n",
        "        dropout_rate (float): Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_head, n_feat, dropout_rate):\n",
        "        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert n_feat % n_head == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = n_feat // n_head\n",
        "        self.h = n_head\n",
        "        self.linear_q = nn.Linear(n_feat, n_feat)\n",
        "        self.linear_k = nn.Linear(n_feat, n_feat)\n",
        "        self.linear_v = nn.Linear(n_feat, n_feat)\n",
        "        self.linear_out = nn.Linear(n_feat, n_feat)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward_qkv(self, query, key, value):\n",
        "        \"\"\"Transform query, key and value.\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor (#batch, time1, size).\n",
        "            key (torch.Tensor): Key tensor (#batch, time2, size).\n",
        "            value (torch.Tensor): Value tensor (#batch, time2, size).\n",
        "        Returns:\n",
        "            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).\n",
        "            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).\n",
        "            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\n",
        "        \"\"\"\n",
        "        n_batch = query.size(0)\n",
        "        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n",
        "        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n",
        "        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n",
        "        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n",
        "        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n",
        "        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n",
        "\n",
        "        return q, k, v\n",
        "\n",
        "    def forward_attention(self, value, scores, mask, rtn_attn=False):\n",
        "        \"\"\"Compute attention context vector.\n",
        "        Args:\n",
        "            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).\n",
        "            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).\n",
        "            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).\n",
        "            rtn_attn (boolean): Flag of return attention score\n",
        "        Returns:\n",
        "            torch.Tensor: Transformed value (#batch, time1, d_model)\n",
        "                weighted by the attention score (#batch, time1, time2).\n",
        "        \"\"\"\n",
        "        n_batch = value.size(0)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)\n",
        "            min_value = float(\n",
        "                np.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n",
        "                #numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n",
        "            )\n",
        "            scores = scores.masked_fill(mask, min_value)\n",
        "            self.attn = torch.softmax(scores, dim=-1).masked_fill(\n",
        "                mask, 0.0\n",
        "            )  # (batch, head, time1, time2)\n",
        "        else:\n",
        "            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n",
        "\n",
        "        p_attn = self.dropout(self.attn)\n",
        "        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n",
        "        x = (\n",
        "            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)\n",
        "        )  # (batch, time1, d_model)\n",
        "        if rtn_attn:\n",
        "            return self.linear_out(x), self.attn\n",
        "        return self.linear_out(x)  # (batch, time1, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask, rtn_attn=False):\n",
        "        \"\"\"Compute scaled dot product attention.\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor (#batch, time1, size).\n",
        "            key (torch.Tensor): Key tensor (#batch, time2, size).\n",
        "            value (torch.Tensor): Value tensor (#batch, time2, size).\n",
        "            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n",
        "                (#batch, time1, time2).\n",
        "            rtn_attn (boolean): Flag of return attention score\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor (#batch, time1, d_model).\n",
        "        \"\"\"\n",
        "        q, k, v = self.forward_qkv(query, key, value)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        return self.forward_attention(v, scores, mask, rtn_attn)\n",
        "\n",
        "\n",
        "class LegacyRelPositionMultiHeadedAttention(MultiHeadedAttention):\n",
        "    \"\"\"Multi-Head Attention layer with relative position encoding (old version).\n",
        "    Details can be found in https://github.com/espnet/espnet/pull/2816.\n",
        "    Paper: https://arxiv.org/abs/1901.02860\n",
        "    Args:\n",
        "        n_head (int): The number of heads.\n",
        "        n_feat (int): The number of features.\n",
        "        dropout_rate (float): Dropout rate.\n",
        "        zero_triu (bool): Whether to zero the upper triangular part of attention matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_head, n_feat, dropout_rate, zero_triu=False):\n",
        "        \"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"\n",
        "        super().__init__(n_head, n_feat, dropout_rate)\n",
        "        self.zero_triu = zero_triu\n",
        "        # linear transformation for positional encoding\n",
        "        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)\n",
        "        # these two learnable bias are used in matrix c and matrix d\n",
        "        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n",
        "        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))\n",
        "        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))\n",
        "        torch.nn.init.xavier_uniform_(self.pos_bias_u)\n",
        "        torch.nn.init.xavier_uniform_(self.pos_bias_v)\n",
        "\n",
        "    def rel_shift(self, x):\n",
        "        \"\"\"Compute relative positional encoding.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (batch, head, time1, time2).\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)\n",
        "        x_padded = torch.cat([zero_pad, x], dim=-1)\n",
        "\n",
        "        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))\n",
        "        x = x_padded[:, :, 1:].view_as(x)\n",
        "\n",
        "        if self.zero_triu:\n",
        "            ones = torch.ones((x.size(2), x.size(3)))\n",
        "            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, query, key, value, pos_emb, mask):\n",
        "        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor (#batch, time1, size).\n",
        "            key (torch.Tensor): Key tensor (#batch, time2, size).\n",
        "            value (torch.Tensor): Value tensor (#batch, time2, size).\n",
        "            pos_emb (torch.Tensor): Positional embedding tensor (#batch, time1, size).\n",
        "            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n",
        "                (#batch, time1, time2).\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor (#batch, time1, d_model).\n",
        "        \"\"\"\n",
        "        q, k, v = self.forward_qkv(query, key, value)\n",
        "        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n",
        "\n",
        "        n_batch_pos = pos_emb.size(0)\n",
        "        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n",
        "        p = p.transpose(1, 2)  # (batch, head, time1, d_k)\n",
        "\n",
        "        # (batch, head, time1, d_k)\n",
        "        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n",
        "        # (batch, head, time1, d_k)\n",
        "        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n",
        "\n",
        "        # compute attention score\n",
        "        # first compute matrix a and matrix c\n",
        "        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n",
        "        # (batch, head, time1, time2)\n",
        "        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n",
        "\n",
        "        # compute matrix b and matrix d\n",
        "        # (batch, head, time1, time1)\n",
        "        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n",
        "        matrix_bd = self.rel_shift(matrix_bd)\n",
        "\n",
        "        scores = (matrix_ac + matrix_bd) / math.sqrt(\n",
        "            self.d_k\n",
        "        )  # (batch, head, time1, time2)\n",
        "\n",
        "        return self.forward_attention(v, scores, mask)\n",
        "\n",
        "\n",
        "class RelPositionMultiHeadedAttention(MultiHeadedAttention):\n",
        "    \"\"\"Multi-Head Attention layer with relative position encoding (new implementation).\n",
        "    Details can be found in https://github.com/espnet/espnet/pull/2816.\n",
        "    Paper: https://arxiv.org/abs/1901.02860\n",
        "    Args:\n",
        "        n_head (int): The number of heads.\n",
        "        n_feat (int): The number of features.\n",
        "        dropout_rate (float): Dropout rate.\n",
        "        zero_triu (bool): Whether to zero the upper triangular part of attention matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_head, n_feat, dropout_rate, zero_triu=False):\n",
        "        \"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"\n",
        "        super().__init__(n_head, n_feat, dropout_rate)\n",
        "        self.zero_triu = zero_triu\n",
        "        # linear transformation for positional encoding\n",
        "        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)\n",
        "        # these two learnable bias are used in matrix c and matrix d\n",
        "        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n",
        "        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))\n",
        "        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))\n",
        "        torch.nn.init.xavier_uniform_(self.pos_bias_u)\n",
        "        torch.nn.init.xavier_uniform_(self.pos_bias_v)\n",
        "\n",
        "    def rel_shift(self, x):\n",
        "        \"\"\"Compute relative positional encoding.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).\n",
        "            time1 means the length of query vector.\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)\n",
        "        x_padded = torch.cat([zero_pad, x], dim=-1)\n",
        "\n",
        "        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))\n",
        "        x = x_padded[:, :, 1:].view_as(x)[\n",
        "            :, :, :, : x.size(-1) // 2 + 1\n",
        "        ]  # only keep the positions from 0 to time2\n",
        "\n",
        "        if self.zero_triu:\n",
        "            ones = torch.ones((x.size(2), x.size(3)), device=x.device)\n",
        "            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, query, key, value, pos_emb, mask):\n",
        "        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor (#batch, time1, size).\n",
        "            key (torch.Tensor): Key tensor (#batch, time2, size).\n",
        "            value (torch.Tensor): Value tensor (#batch, time2, size).\n",
        "            pos_emb (torch.Tensor): Positional embedding tensor\n",
        "                (#batch, 2*time1-1, size).\n",
        "            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n",
        "                (#batch, time1, time2).\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor (#batch, time1, d_model).\n",
        "        \"\"\"\n",
        "        q, k, v = self.forward_qkv(query, key, value)\n",
        "        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n",
        "\n",
        "        n_batch_pos = pos_emb.size(0)\n",
        "        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n",
        "        p = p.transpose(1, 2)  # (batch, head, 2*time1-1, d_k)\n",
        "\n",
        "        # (batch, head, time1, d_k)\n",
        "        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n",
        "        # (batch, head, time1, d_k)\n",
        "        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n",
        "\n",
        "        # compute attention score\n",
        "        # first compute matrix a and matrix c\n",
        "        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n",
        "        # (batch, head, time1, time2)\n",
        "        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n",
        "\n",
        "        # compute matrix b and matrix d\n",
        "        # (batch, head, time1, 2*time1-1)\n",
        "        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n",
        "        matrix_bd = self.rel_shift(matrix_bd)\n",
        "\n",
        "        scores = (matrix_ac + matrix_bd) / math.sqrt(\n",
        "            self.d_k\n",
        "        )  # (batch, head, time1, time2)\n",
        "\n",
        "        return self.forward_attention(v, scores, mask)"
      ],
      "metadata": {
        "id": "m8jlXupij4_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bloques conformer"
      ],
      "metadata": {
        "id": "gPL6F3hminyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PositionwiseFeedForward"
      ],
      "metadata": {
        "id": "2pSgOj6Liukd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(torch.nn.Module):\n",
        "    \"\"\"Positionwise feed forward layer.\n",
        "\n",
        "    :param int idim: input dimenstion\n",
        "    :param int hidden_units: number of hidden units\n",
        "    :param float dropout_rate: dropout rate\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, idim, hidden_units, dropout_rate):\n",
        "        \"\"\"Construct an PositionwiseFeedForward object.\"\"\"\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = torch.nn.Linear(idim, hidden_units)\n",
        "        self.w_2 = torch.nn.Linear(hidden_units, idim)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward funciton.\"\"\"\n",
        "        return self.w_2(self.dropout(torch.relu(self.w_1(x))))"
      ],
      "metadata": {
        "id": "vrZVGR-sivgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ConvolutionModule"
      ],
      "metadata": {
        "id": "7qQLfrWejOjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionModule(nn.Module):\n",
        "    \"\"\"ConvolutionModule in Conformer model.\n",
        "\n",
        "    :param int channels: channels of cnn\n",
        "    :param int kernel_size: kernerl size of cnn\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, kernel_size, bias=True):\n",
        "        \"\"\"Construct an ConvolutionModule object.\"\"\"\n",
        "        super(ConvolutionModule, self).__init__()\n",
        "        # kernerl_size should be a odd number for 'SAME' padding\n",
        "        assert (kernel_size - 1) % 2 == 0\n",
        "\n",
        "        self.pointwise_cov1 = nn.Conv1d(\n",
        "            channels,\n",
        "            2 * channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=bias,\n",
        "        )\n",
        "        self.depthwise_conv = nn.Conv1d(\n",
        "            channels,\n",
        "            channels,\n",
        "            kernel_size,\n",
        "            stride=1,\n",
        "            padding=(kernel_size - 1) // 2,\n",
        "            groups=channels,\n",
        "            bias=bias,\n",
        "        )\n",
        "        self.norm = nn.BatchNorm1d(channels)\n",
        "        self.pointwise_cov2 = nn.Conv1d(\n",
        "            channels,\n",
        "            channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=bias,\n",
        "        )\n",
        "        self.activation = Swish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Compute covolution module.\n",
        "\n",
        "        :param torch.Tensor x: (batch, time, size)\n",
        "        :return torch.Tensor: convoluted `value` (batch, time, d_model)\n",
        "        \"\"\"\n",
        "        # exchange the temporal dimension and the feature dimension\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # GLU mechanism\n",
        "        x = self.pointwise_cov1(x)  # (batch, 2*channel, dim)\n",
        "        x = nn.functional.glu(x, dim=1)  # (batch, channel, dim)\n",
        "\n",
        "        # 1D Depthwise Conv\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.activation(self.norm(x))\n",
        "\n",
        "        x = self.pointwise_cov2(x)\n",
        "\n",
        "        return x.transpose(1, 2)"
      ],
      "metadata": {
        "id": "Ou93sSi0jS1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat"
      ],
      "metadata": {
        "id": "FiwOPaYzkiE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiSequential(torch.nn.Sequential):\n",
        "    \"\"\"Multi-input multi-output torch.nn.Sequential.\"\"\"\n",
        "\n",
        "    def forward(self, *args):\n",
        "        \"\"\"Repeat.\"\"\"\n",
        "        for m in self:\n",
        "            args = m(*args)\n",
        "        return args\n",
        "\n",
        "\n",
        "def repeat(N, fn):\n",
        "    \"\"\"Repeat module N times.\n",
        "\n",
        "    :param int N: repeat time\n",
        "    :param function fn: function to generate module\n",
        "    :return: repeated modules\n",
        "    :rtype: MultiSequential\n",
        "    \"\"\"\n",
        "    return MultiSequential(*[fn() for _ in range(N)])"
      ],
      "metadata": {
        "id": "07fecIHEkjsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LayerNorm"
      ],
      "metadata": {
        "id": "MwoN53F0kkFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(torch.nn.LayerNorm):\n",
        "    \"\"\"Layer normalization module.\n",
        "\n",
        "    :param int nout: output dim size\n",
        "    :param int dim: dimension to be normalized\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nout, dim=-1):\n",
        "        \"\"\"Construct an LayerNorm object.\"\"\"\n",
        "        super(LayerNorm, self).__init__(nout, eps=1e-12)\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Apply layer normalization.\n",
        "\n",
        "        :param torch.Tensor x: input tensor\n",
        "        :return: layer normalized tensor\n",
        "        :rtype torch.Tensor\n",
        "        \"\"\"\n",
        "        if self.dim == -1:\n",
        "            return super(LayerNorm, self).forward(x)\n",
        "        return super(LayerNorm, self).forward(x.transpose(1, -1)).transpose(1, -1)"
      ],
      "metadata": {
        "id": "Z-omRV-1koDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EncoderLayer (Conformer)"
      ],
      "metadata": {
        "id": "s9rqrInBks8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Encoder layer module.\n",
        "\n",
        "    :param int size: input dim\n",
        "    :param espnet.nets.pytorch_backend.transformer.attention.\n",
        "        MultiHeadedAttention self_attn: self attention module\n",
        "        RelPositionMultiHeadedAttention self_attn: self attention module\n",
        "    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n",
        "        PositionwiseFeedForward feed_forward:\n",
        "        feed forward module\n",
        "    :param espnet.nets.pytorch_backend.transformer.convolution.\n",
        "        ConvolutionModule feed_foreard:\n",
        "        feed forward module\n",
        "    :param float dropout_rate: dropout rate\n",
        "    :param bool normalize_before: whether to use layer_norm before the first block\n",
        "    :param bool concat_after: whether to concat attention layer's input and output\n",
        "        if True, additional linear will be applied.\n",
        "        i.e. x -> x + linear(concat(x, att(x)))\n",
        "        if False, no additional linear will be applied. i.e. x -> x + att(x)\n",
        "    :param bool macaron_style: whether to use macaron style for PositionwiseFeedForward\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        size,\n",
        "        self_attn,\n",
        "        feed_forward,\n",
        "        conv_module,\n",
        "        dropout_rate,\n",
        "        normalize_before=True,\n",
        "        concat_after=False,\n",
        "        macaron_style=False,\n",
        "    ):\n",
        "        \"\"\"Construct an EncoderLayer object.\"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.ff_scale = 1.0\n",
        "        self.conv_module = conv_module\n",
        "        self.macaron_style = macaron_style\n",
        "        self.norm_ff = LayerNorm(size)  # for the FNN module\n",
        "        self.norm_mha = LayerNorm(size)  # for the MHA module\n",
        "        if self.macaron_style:\n",
        "            self.feed_forward_macaron = copy.deepcopy(feed_forward)\n",
        "            self.ff_scale = 0.5\n",
        "            # for another FNN module in macaron style\n",
        "            self.norm_ff_macaron = LayerNorm(size)\n",
        "        if self.conv_module is not None:\n",
        "            self.norm_conv = LayerNorm(size)  # for the CNN module\n",
        "            self.norm_final = LayerNorm(size)  # for the final output of the block\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.size = size\n",
        "        self.normalize_before = normalize_before\n",
        "        self.concat_after = concat_after\n",
        "        if self.concat_after:\n",
        "            self.concat_linear = nn.Linear(size + size, size)\n",
        "\n",
        "    def forward(self, x_input, mask, cache=None):\n",
        "        \"\"\"Compute encoded features.\n",
        "\n",
        "        :param torch.Tensor x_input: encoded source features (batch, max_time_in, size)\n",
        "        :param torch.Tensor mask: mask for x (batch, max_time_in)\n",
        "        :param torch.Tensor cache: cache for x (batch, max_time_in - 1, size)\n",
        "        :rtype: Tuple[torch.Tensor, torch.Tensor]\n",
        "        \"\"\"\n",
        "        if isinstance(x_input, tuple):\n",
        "            x, pos_emb = x_input[0], x_input[1]\n",
        "        else:\n",
        "            x, pos_emb = x_input, None\n",
        "\n",
        "        # whether to use macaron style\n",
        "        if self.macaron_style:\n",
        "            residual = x\n",
        "            if self.normalize_before:\n",
        "                x = self.norm_ff_macaron(x)\n",
        "            x = residual + self.ff_scale * self.dropout(self.feed_forward_macaron(x))\n",
        "            if not self.normalize_before:\n",
        "                x = self.norm_ff_macaron(x)\n",
        "\n",
        "        # multi-headed self-attention module\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.norm_mha(x)\n",
        "\n",
        "        if cache is None:\n",
        "            x_q = x\n",
        "        else:\n",
        "            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)\n",
        "            x_q = x[:, -1:, :]\n",
        "            residual = residual[:, -1:, :]\n",
        "            mask = None if mask is None else mask[:, -1:, :]\n",
        "\n",
        "        if pos_emb is not None:\n",
        "            x_att = self.self_attn(x_q, x, x, pos_emb, mask)\n",
        "        else:\n",
        "            x_att = self.self_attn(x_q, x, x, mask)\n",
        "\n",
        "        if self.concat_after:\n",
        "            x_concat = torch.cat((x, x_att), dim=-1)\n",
        "            x = residual + self.concat_linear(x_concat)\n",
        "        else:\n",
        "            x = residual + self.dropout(x_att)\n",
        "        if not self.normalize_before:\n",
        "            x = self.norm_mha(x)\n",
        "\n",
        "        # convolution module\n",
        "        if self.conv_module is not None:\n",
        "            residual = x\n",
        "            if self.normalize_before:\n",
        "                x = self.norm_conv(x)\n",
        "            x = residual + self.dropout(self.conv_module(x))\n",
        "            if not self.normalize_before:\n",
        "                x = self.norm_conv(x)\n",
        "\n",
        "        # feed forward module\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.norm_ff(x)\n",
        "        x = residual + self.ff_scale * self.dropout(self.feed_forward(x))\n",
        "        if not self.normalize_before:\n",
        "            x = self.norm_ff(x)\n",
        "\n",
        "        if self.conv_module is not None:\n",
        "            x = self.norm_final(x)\n",
        "\n",
        "        if cache is not None:\n",
        "            x = torch.cat([cache, x], dim=1)\n",
        "\n",
        "        if pos_emb is not None:\n",
        "            return (x, pos_emb), mask\n",
        "        else:\n",
        "            return x, mask"
      ],
      "metadata": {
        "id": "iuCP5OywkuKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "JDY4Nxr7dbzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Encoder definition.\"\"\"\n",
        "\n",
        "# import torch\n",
        "# from espnet.nets.pytorch_backend.backbones.conv1d_extractor import Conv1dResNet\n",
        "# from espnet.nets.pytorch_backend.backbones.conv3d_extractor import Conv3dResNet\n",
        "\n",
        "# from espnet.nets.pytorch_backend.nets_utils import rename_state_dict\n",
        "\n",
        "# from espnet.nets.pytorch_backend.transformer.attention import (\n",
        "#     MultiHeadedAttention,  # noqa: H301\n",
        "#     RelPositionMultiHeadedAttention,  # noqa: H301\n",
        "# )\n",
        "# from espnet.nets.pytorch_backend.transformer.convolution import ConvolutionModule\n",
        "# from espnet.nets.pytorch_backend.transformer.embedding import (\n",
        "#     PositionalEncoding,  # noqa: H301\n",
        "#     RelPositionalEncoding,  # noqa: H301\n",
        "# )\n",
        "# from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n",
        "# from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
        "# from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n",
        "\n",
        "# from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
        "\n",
        "\n",
        "# def _pre_hook(\n",
        "#     state_dict,\n",
        "#     prefix,\n",
        "#     local_metadata,\n",
        "#     strict,\n",
        "#     missing_keys,\n",
        "#     unexpected_keys,\n",
        "#     error_msgs,\n",
        "# ):\n",
        "#     # https://github.com/espnet/espnet/commit/21d70286c354c66c0350e65dc098d2ee236faccc#diff-bffb1396f038b317b2b64dd96e6d3563\n",
        "#     rename_state_dict(prefix + \"input_layer.\", prefix + \"embed.\", state_dict)\n",
        "#     # https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563\n",
        "#     rename_state_dict(prefix + \"norm.\", prefix + \"after_norm.\", state_dict)\n",
        "\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    \"\"\"Transformer encoder module.\n",
        "\n",
        "    :param int attention_dim: dimention of attention\n",
        "    :param int attention_heads: the number of heads of multi head attention\n",
        "    :param int linear_units: the number of units of position-wise feed forward\n",
        "    :param int num_blocks: the number of decoder blocks\n",
        "    :param float dropout_rate: dropout rate\n",
        "    :param float attention_dropout_rate: dropout rate in attention\n",
        "    :param float positional_dropout_rate: dropout rate after adding positional encoding\n",
        "    :param str or torch.nn.Module input_layer: input layer type\n",
        "    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n",
        "    :param bool normalize_before: whether to use layer_norm before the first block\n",
        "    :param bool concat_after: whether to concat attention layer's input and output\n",
        "        if True, additional linear will be applied.\n",
        "        i.e. x -> x + linear(concat(x, att(x)))\n",
        "        if False, no additional linear will be applied. i.e. x -> x + att(x)\n",
        "    :param int positionwise_conv_kernel_size: kernel size of positionwise conv1d layer\n",
        "    :param str encoder_attn_layer_type: encoder attention layer type\n",
        "    :param bool macaron_style: whether to use macaron style for positionwise layer\n",
        "    :param bool use_cnn_module: whether to use convolution module\n",
        "    :param bool zero_triu: whether to zero the upper triangular part of attention matrix\n",
        "    :param int cnn_module_kernel: kernerl size of convolution module\n",
        "    :param int padding_idx: padding_idx for input_layer=embed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_dim=768,\n",
        "        attention_heads=12,\n",
        "        linear_units=3072,\n",
        "        num_blocks=12,\n",
        "        dropout_rate=0.1,\n",
        "        positional_dropout_rate=0.1,\n",
        "        attention_dropout_rate=0.0,\n",
        "        input_layer=\"conv2d\",\n",
        "        pos_enc_class=RelPositionalEncoding,\n",
        "        normalize_before=True,\n",
        "        concat_after=False,\n",
        "        positionwise_conv_kernel_size=1,\n",
        "        macaron_style=False,\n",
        "        encoder_attn_layer_type=\"mha\",\n",
        "        use_cnn_module=False,\n",
        "        zero_triu=False,\n",
        "        cnn_module_kernel=31,\n",
        "        padding_idx=-1,\n",
        "        relu_type=\"prelu\",\n",
        "        a_upsample_ratio=1,\n",
        "    ):\n",
        "        \"\"\"Construct an Encoder object.\"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        #self._register_load_state_dict_pre_hook(_pre_hook)\n",
        "\n",
        "        # -- frontend module.\n",
        "        if input_layer == \"conv1d\":\n",
        "            self.frontend = Conv1dResNet(relu_type=relu_type, a_upsample_ratio=a_upsample_ratio)\n",
        "        elif input_layer == \"conv3d\":\n",
        "            self.frontend = Conv3dResNet(relu_type=relu_type)\n",
        "        else:\n",
        "            self.frontend = None\n",
        "\n",
        "        # -- Embedding\n",
        "        if encoder_attn_layer_type == \"rel_mha\":\n",
        "            pos_enc_class = RelPositionalEncoding\n",
        "        if input_layer in [\"conv1d\", \"conv3d\"]:\n",
        "            self.embed = torch.nn.Sequential(\n",
        "                torch.nn.Linear(512, attention_dim),\n",
        "                pos_enc_class(attention_dim, positional_dropout_rate)\n",
        "                )\n",
        "        else:\n",
        "            raise NotImplementedError(\"Support only conv1d and conv3d\")\n",
        "\n",
        "        # -- backend module.\n",
        "        self.normalize_before = normalize_before\n",
        "        positionwise_layer = PositionwiseFeedForward\n",
        "        positionwise_layer_args = (attention_dim, linear_units, dropout_rate)\n",
        "\n",
        "        if encoder_attn_layer_type == \"mha\":\n",
        "            encoder_attn_layer = MultiHeadedAttention\n",
        "            encoder_attn_layer_args = (\n",
        "                attention_heads,\n",
        "                attention_dim,\n",
        "                attention_dropout_rate,\n",
        "            )\n",
        "        elif encoder_attn_layer_type == \"rel_mha\":\n",
        "            encoder_attn_layer = RelPositionMultiHeadedAttention\n",
        "            encoder_attn_layer_args = (\n",
        "                attention_heads,\n",
        "                attention_dim,\n",
        "                attention_dropout_rate,\n",
        "                zero_triu,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"unknown encoder_attn_layer: \" + encoder_attn_layer)\n",
        "\n",
        "        convolution_layer = ConvolutionModule\n",
        "        convolution_layer_args = (attention_dim, cnn_module_kernel)\n",
        "\n",
        "        self.encoders = repeat(\n",
        "            num_blocks,\n",
        "            lambda: EncoderLayer(\n",
        "                attention_dim,\n",
        "                encoder_attn_layer(*encoder_attn_layer_args),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                convolution_layer(*convolution_layer_args) if use_cnn_module else None,\n",
        "                dropout_rate,\n",
        "                normalize_before,\n",
        "                concat_after,\n",
        "                macaron_style,\n",
        "            ),\n",
        "        )\n",
        "        if self.normalize_before:\n",
        "            self.after_norm = LayerNorm(attention_dim)\n",
        "\n",
        "    def forward(self, xs, masks):\n",
        "        \"\"\"Encode input sequence.\n",
        "\n",
        "        :param torch.Tensor xs: input tensor\n",
        "        :param torch.Tensor masks: input mask\n",
        "        :return: position embedded tensor and mask\n",
        "        :rtype Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        if isinstance(self.frontend, (Conv1dResNet, Conv3dResNet)):\n",
        "            xs = self.frontend(xs)\n",
        "\n",
        "        xs = self.embed(xs)\n",
        "        xs, masks = self.encoders(xs, masks)\n",
        "\n",
        "        if isinstance(xs, tuple):\n",
        "            xs = xs[0]\n",
        "\n",
        "        if self.normalize_before:\n",
        "            xs = self.after_norm(xs)\n",
        "\n",
        "        return xs, masks\n",
        "\n",
        "    def forward_one_step(self, xs, masks, cache=None):\n",
        "        \"\"\"Encode input frame.\n",
        "\n",
        "        :param torch.Tensor xs: input tensor\n",
        "        :param torch.Tensor masks: input mask\n",
        "        :param List[torch.Tensor] cache: cache tensors\n",
        "        :return: position embedded tensor, mask and new cache\n",
        "        :rtype Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        if isinstance(self.frontend, (Conv1dResNet, Conv3dResNet)):\n",
        "            xs = self.frontend(xs)\n",
        "\n",
        "        xs = self.embed(xs)\n",
        "\n",
        "        if cache is None:\n",
        "            cache = [None for _ in range(len(self.encoders))]\n",
        "        new_cache = []\n",
        "        for c, e in zip(cache, self.encoders):\n",
        "            xs, masks = e(xs, masks, cache=c)\n",
        "            new_cache.append(xs)\n",
        "        if self.normalize_before:\n",
        "            xs = self.after_norm(xs)\n",
        "        return xs, masks, new_cache"
      ],
      "metadata": {
        "id": "cLRIFkwjdfBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLPHead"
      ],
      "metadata": {
        "id": "YvlnuzwKl8ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPHead(torch.nn.Module):\n",
        "    def __init__(self, idim, hdim, odim, norm=\"batchnorm\"):\n",
        "        super(MLPHead, self).__init__()\n",
        "        self.norm = norm\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(idim, hdim)\n",
        "        if norm == \"batchnorm\":\n",
        "            self.bn1 = torch.nn.BatchNorm1d(hdim)\n",
        "        elif norm == \"layernorm\":\n",
        "            self.norm1 = torch.nn.LayerNorm(hdim)\n",
        "        self.nonlin1 = torch.nn.ReLU(inplace=True)\n",
        "        self.fc2 = torch.nn.Linear(hdim, odim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        if self.norm == \"batchnorm\":\n",
        "            x = self.bn1(x.transpose(1, 2)).transpose(1, 2)\n",
        "        elif self.norm == \"layernorm\":\n",
        "            x = self.norm1(x)\n",
        "        x = self.nonlin1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5WS_YEjnl-73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ScorerInterface"
      ],
      "metadata": {
        "id": "JAA0ah8jnioL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScorerInterface:\n",
        "    \"\"\"Scorer interface for beam search.\n",
        "\n",
        "    The scorer performs scoring of the all tokens in vocabulary.\n",
        "\n",
        "    Examples:\n",
        "        * Search heuristics\n",
        "            * :class:`espnet.nets.scorers.length_bonus.LengthBonus`\n",
        "        * Decoder networks of the sequence-to-sequence models\n",
        "            * :class:`espnet.nets.pytorch_backend.nets.transformer.decoder.Decoder`\n",
        "            * :class:`espnet.nets.pytorch_backend.nets.rnn.decoders.Decoder`\n",
        "        * Neural language models\n",
        "            * :class:`espnet.nets.pytorch_backend.lm.transformer.TransformerLM`\n",
        "            * :class:`espnet.nets.pytorch_backend.lm.default.DefaultRNNLM`\n",
        "            * :class:`espnet.nets.pytorch_backend.lm.seq_rnn.SequentialRNNLM`\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def init_state(self, x: torch.Tensor) -> Any:\n",
        "        \"\"\"Get an initial state for decoding (optional).\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The encoded feature tensor\n",
        "\n",
        "        Returns: initial state\n",
        "\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def select_state(self, state: Any, i: int, new_id: int = None) -> Any:\n",
        "        \"\"\"Select state with relative ids in the main beam search.\n",
        "\n",
        "        Args:\n",
        "            state: Decoder state for prefix tokens\n",
        "            i (int): Index to select a state in the main beam search\n",
        "            new_id (int): New label index to select a state if necessary\n",
        "\n",
        "        Returns:\n",
        "            state: pruned state\n",
        "\n",
        "        \"\"\"\n",
        "        return None if state is None else state[i]\n",
        "\n",
        "    def score(\n",
        "        self, y: torch.Tensor, state: Any, x: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, Any]:\n",
        "        \"\"\"Score new token (required).\n",
        "\n",
        "        Args:\n",
        "            y (torch.Tensor): 1D torch.int64 prefix tokens.\n",
        "            state: Scorer state for prefix tokens\n",
        "            x (torch.Tensor): The encoder feature that generates ys.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, Any]: Tuple of\n",
        "                scores for next token that has a shape of `(n_vocab)`\n",
        "                and next state for ys\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def final_score(self, state: Any) -> float:\n",
        "        \"\"\"Score eos (optional).\n",
        "\n",
        "        Args:\n",
        "            state: Scorer state for prefix tokens\n",
        "\n",
        "        Returns:\n",
        "            float: final score\n",
        "\n",
        "        \"\"\"\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "class BatchScorerInterface(ScorerInterface):\n",
        "    \"\"\"Batch scorer interface.\"\"\"\n",
        "\n",
        "    def batch_init_state(self, x: torch.Tensor) -> Any:\n",
        "        \"\"\"Get an initial state for decoding (optional).\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The encoded feature tensor\n",
        "\n",
        "        Returns: initial state\n",
        "\n",
        "        \"\"\"\n",
        "        return self.init_state(x)\n",
        "\n",
        "    def batch_score(\n",
        "        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, List[Any]]:\n",
        "        \"\"\"Score new token batch (required).\n",
        "\n",
        "        Args:\n",
        "            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n",
        "            states (List[Any]): Scorer states for prefix tokens.\n",
        "            xs (torch.Tensor):\n",
        "                The encoder feature that generates ys (n_batch, xlen, n_feat).\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, List[Any]]: Tuple of\n",
        "                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n",
        "                and next state list for ys.\n",
        "\n",
        "        \"\"\"\n",
        "        warnings.warn(\n",
        "            \"{} batch score is implemented through for loop not parallelized\".format(\n",
        "                self.__class__.__name__\n",
        "            )\n",
        "        )\n",
        "        scores = list()\n",
        "        outstates = list()\n",
        "        for i, (y, state, x) in enumerate(zip(ys, states, xs)):\n",
        "            score, outstate = self.score(y, state, x)\n",
        "            outstates.append(outstate)\n",
        "            scores.append(score)\n",
        "        scores = torch.cat(scores, 0).view(ys.shape[0], -1)\n",
        "        return scores, outstates\n",
        "\n",
        "\n",
        "class PartialScorerInterface(ScorerInterface):\n",
        "    \"\"\"Partial scorer interface for beam search.\n",
        "\n",
        "    The partial scorer performs scoring when non-partial scorer finished scoring,\n",
        "    and receives pre-pruned next tokens to score because it is too heavy to score\n",
        "    all the tokens.\n",
        "\n",
        "    Examples:\n",
        "         * Prefix search for connectionist-temporal-classification models\n",
        "             * :class:`espnet.nets.scorers.ctc.CTCPrefixScorer`\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def score_partial(\n",
        "        self, y: torch.Tensor, next_tokens: torch.Tensor, state: Any, x: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, Any]:\n",
        "        \"\"\"Score new token (required).\n",
        "\n",
        "        Args:\n",
        "            y (torch.Tensor): 1D prefix token\n",
        "            next_tokens (torch.Tensor): torch.int64 next token to score\n",
        "            state: decoder state for prefix tokens\n",
        "            x (torch.Tensor): The encoder feature that generates ys\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, Any]:\n",
        "                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n",
        "                and next state for ys\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BatchPartialScorerInterface(BatchScorerInterface, PartialScorerInterface):\n",
        "    \"\"\"Batch partial scorer interface for beam search.\"\"\"\n",
        "\n",
        "    def batch_score_partial(\n",
        "        self,\n",
        "        ys: torch.Tensor,\n",
        "        next_tokens: torch.Tensor,\n",
        "        states: List[Any],\n",
        "        xs: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, Any]:\n",
        "        \"\"\"Score new token (required).\n",
        "\n",
        "        Args:\n",
        "            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n",
        "            next_tokens (torch.Tensor): torch.int64 tokens to score (n_batch, n_token).\n",
        "            states (List[Any]): Scorer states for prefix tokens.\n",
        "            xs (torch.Tensor):\n",
        "                The encoder feature that generates ys (n_batch, xlen, n_feat).\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, Any]:\n",
        "                Tuple of a score tensor for ys that has a shape `(n_batch, n_vocab)`\n",
        "                and next states for ys\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "qCdIDkbinj3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PositionalEncoding"
      ],
      "metadata": {
        "id": "apmMcHctoPrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    \"\"\"Positional encoding.\n",
        "    Args:\n",
        "        d_model (int): Embedding dimension.\n",
        "        dropout_rate (float): Dropout rate.\n",
        "        max_len (int): Maximum input length.\n",
        "        reverse (bool): Whether to reverse the input position. Only for\n",
        "        the class LegacyRelPositionalEncoding. We remove it in the current\n",
        "        class RelPositionalEncoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout_rate, max_len=5000, reverse=False):\n",
        "        \"\"\"Construct an PositionalEncoding object.\"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.reverse = reverse\n",
        "        self.xscale = math.sqrt(self.d_model)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "        self.pe = None\n",
        "        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n",
        "        #self._register_load_state_dict_pre_hook(_pre_hook)\n",
        "\n",
        "    def extend_pe(self, x):\n",
        "        \"\"\"Reset the positional encodings.\"\"\"\n",
        "        if self.pe is not None:\n",
        "            if self.pe.size(1) >= x.size(1):\n",
        "                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n",
        "                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n",
        "                return\n",
        "        pe = torch.zeros(x.size(1), self.d_model)\n",
        "        if self.reverse:\n",
        "            position = torch.arange(\n",
        "                x.size(1) - 1, -1, -1.0, dtype=torch.float32\n",
        "            ).unsqueeze(1)\n",
        "        else:\n",
        "            position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n",
        "            * -(math.log(10000.0) / self.d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.pe = pe.to(device=x.device, dtype=x.dtype)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"Add positional encoding.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (batch, time, `*`).\n",
        "        Returns:\n",
        "            torch.Tensor: Encoded tensor (batch, time, `*`).\n",
        "        \"\"\"\n",
        "        self.extend_pe(x)\n",
        "        x = x * self.xscale + self.pe[:, : x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "AvHCuslcoUZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DecoderLayer (Decoder del transformer)"
      ],
      "metadata": {
        "id": "iE1DYbe_pAeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Single decoder layer module.\n",
        "    :param int size: input dim\n",
        "    :param espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\n",
        "        self_attn: self attention module\n",
        "    :param espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\n",
        "        src_attn: source attention module\n",
        "    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n",
        "        PositionwiseFeedForward feed_forward: feed forward layer module\n",
        "    :param float dropout_rate: dropout rate\n",
        "    :param bool normalize_before: whether to use layer_norm before the first block\n",
        "    :param bool concat_after: whether to concat attention layer's input and output\n",
        "        if True, additional linear will be applied.\n",
        "        i.e. x -> x + linear(concat(x, att(x)))\n",
        "        if False, no additional linear will be applied. i.e. x -> x + att(x)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        size,\n",
        "        self_attn,\n",
        "        src_attn,\n",
        "        feed_forward,\n",
        "        dropout_rate,\n",
        "        normalize_before=True,\n",
        "        concat_after=False,\n",
        "    ):\n",
        "        \"\"\"Construct an DecoderLayer object.\"\"\"\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.norm1 = LayerNorm(size)\n",
        "        self.norm2 = LayerNorm(size)\n",
        "        self.norm3 = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.normalize_before = normalize_before\n",
        "        self.concat_after = concat_after\n",
        "        if self.concat_after:\n",
        "            self.concat_linear1 = nn.Linear(size + size, size)\n",
        "            self.concat_linear2 = nn.Linear(size + size, size)\n",
        "\n",
        "    def forward(self, tgt, tgt_mask, memory, memory_mask, cache=None):\n",
        "        \"\"\"Compute decoded features.\n",
        "        Args:\n",
        "            tgt (torch.Tensor):\n",
        "                decoded previous target features (batch, max_time_out, size)\n",
        "            tgt_mask (torch.Tensor): mask for x (batch, max_time_out)\n",
        "            memory (torch.Tensor): encoded source features (batch, max_time_in, size)\n",
        "            memory_mask (torch.Tensor): mask for memory (batch, max_time_in)\n",
        "            cache (torch.Tensor): cached output (batch, max_time_out-1, size)\n",
        "        \"\"\"\n",
        "        residual = tgt\n",
        "        if self.normalize_before:\n",
        "            tgt = self.norm1(tgt)\n",
        "\n",
        "        if cache is None:\n",
        "            tgt_q = tgt\n",
        "            tgt_q_mask = tgt_mask\n",
        "        else:\n",
        "            # compute only the last frame query keeping dim: max_time_out -> 1\n",
        "            assert cache.shape == (\n",
        "                tgt.shape[0],\n",
        "                tgt.shape[1] - 1,\n",
        "                self.size,\n",
        "            ), f\"{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}\"\n",
        "            tgt_q = tgt[:, -1:, :]\n",
        "            residual = residual[:, -1:, :]\n",
        "            tgt_q_mask = None\n",
        "            if tgt_mask is not None:\n",
        "                tgt_q_mask = tgt_mask[:, -1:, :]\n",
        "\n",
        "        if self.concat_after:\n",
        "            tgt_concat = torch.cat(\n",
        "                (tgt_q, self.self_attn(tgt_q, tgt, tgt, tgt_q_mask)), dim=-1\n",
        "            )\n",
        "            x = residual + self.concat_linear1(tgt_concat)\n",
        "        else:\n",
        "            x = residual + self.dropout(self.self_attn(tgt_q, tgt, tgt, tgt_q_mask))\n",
        "        if not self.normalize_before:\n",
        "            x = self.norm1(x)\n",
        "\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.norm2(x)\n",
        "        if self.concat_after:\n",
        "            x_concat = torch.cat(\n",
        "                (x, self.src_attn(x, memory, memory, memory_mask)), dim=-1\n",
        "            )\n",
        "            x = residual + self.concat_linear2(x_concat)\n",
        "        else:\n",
        "            x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))\n",
        "        if not self.normalize_before:\n",
        "            x = self.norm2(x)\n",
        "\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.norm3(x)\n",
        "        x = residual + self.dropout(self.feed_forward(x))\n",
        "        if not self.normalize_before:\n",
        "            x = self.norm3(x)\n",
        "\n",
        "        if cache is not None:\n",
        "            x = torch.cat([cache, x], dim=1)\n",
        "\n",
        "        return x, tgt_mask, memory, memory_mask"
      ],
      "metadata": {
        "id": "Wuztt-WOpDdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask"
      ],
      "metadata": {
        "id": "dQAWTi24pfBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distutils.version import LooseVersion\n",
        "\n",
        "is_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(\"1.2.0\")\n",
        "# LooseVersion('1.2.0') == LooseVersion(torch.__version__) can't include e.g. 1.2.0+aaa\n",
        "is_torch_1_2 = (\n",
        "    LooseVersion(\"1.3\") > LooseVersion(torch.__version__) >= LooseVersion(\"1.2\")\n",
        ")\n",
        "datatype = torch.bool if is_torch_1_2_plus else torch.uint8\n",
        "\n",
        "def subsequent_mask(size, device=\"cpu\", dtype=datatype):\n",
        "    \"\"\"Create mask for subsequent steps (1, size, size).\n",
        "\n",
        "    :param int size: size of mask\n",
        "    :param str device: \"cpu\" or \"cuda\" or torch.Tensor.device\n",
        "    :param torch.dtype dtype: result dtype\n",
        "    :rtype: torch.Tensor\n",
        "    >>> subsequent_mask(3)\n",
        "    [[1, 0, 0],\n",
        "     [1, 1, 0],\n",
        "     [1, 1, 1]]\n",
        "    \"\"\"\n",
        "    if is_torch_1_2 and dtype == torch.bool:\n",
        "        # torch=1.2 doesn't support tril for bool tensor\n",
        "        ret = torch.ones(size, size, device=device, dtype=torch.uint8)\n",
        "        return torch.tril(ret, out=ret).type(dtype)\n",
        "    else:\n",
        "        ret = torch.ones(size, size, device=device, dtype=dtype)\n",
        "        return torch.tril(ret, out=ret)\n",
        "\n",
        "\n",
        "def target_mask(ys_in_pad, ignore_id):\n",
        "    \"\"\"Create mask for decoder self-attention.\n",
        "\n",
        "    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n",
        "    :param int ignore_id: index of padding\n",
        "    :param torch.dtype dtype: result dtype\n",
        "    :rtype: torch.Tensor\n",
        "    \"\"\"\n",
        "    ys_mask = ys_in_pad != ignore_id\n",
        "    m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)\n",
        "    return ys_mask.unsqueeze(-2) & m"
      ],
      "metadata": {
        "id": "9nLsXl0cpgix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665aaad0-086e-4ad3-9fc5-68b7b5c60081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-ab144cb2b338>:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  is_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(\"1.2.0\")\n",
            "<ipython-input-30-ab144cb2b338>:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  LooseVersion(\"1.3\") > LooseVersion(torch.__version__) >= LooseVersion(\"1.2\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "Cf9JstLvmyhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Decoder definition.\"\"\"\n",
        "\n",
        "# from typing import Any, List, Tuple\n",
        "\n",
        "# import torch\n",
        "\n",
        "# from espnet.nets.pytorch_backend.nets_utils import rename_state_dict\n",
        "# from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n",
        "# from espnet.nets.pytorch_backend.transformer.decoder_layer import DecoderLayer\n",
        "# from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n",
        "# from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
        "# from espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\n",
        "# from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n",
        "#     PositionwiseFeedForward,  # noqa: H301\n",
        "# )\n",
        "# from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
        "# from espnet.nets.scorer_interface import BatchScorerInterface\n",
        "\n",
        "\n",
        "# def _pre_hook(\n",
        "#     state_dict,\n",
        "#     prefix,\n",
        "#     local_metadata,\n",
        "#     strict,\n",
        "#     missing_keys,\n",
        "#     unexpected_keys,\n",
        "#     error_msgs,\n",
        "# ):\n",
        "#     # https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563\n",
        "#     rename_state_dict(prefix + \"output_norm.\", prefix + \"after_norm.\", state_dict)\n",
        "\n",
        "\n",
        "class Decoder(BatchScorerInterface, torch.nn.Module):\n",
        "    \"\"\"Transfomer decoder module.\n",
        "\n",
        "    :param int odim: output dim\n",
        "    :param int attention_dim: dimention of attention\n",
        "    :param int attention_heads: the number of heads of multi head attention\n",
        "    :param int linear_units: the number of units of position-wise feed forward\n",
        "    :param int num_blocks: the number of decoder blocks\n",
        "    :param float dropout_rate: dropout rate\n",
        "    :param float attention_dropout_rate: dropout rate for attention\n",
        "    :param str or torch.nn.Module input_layer: input layer type\n",
        "    :param bool use_output_layer: whether to use output layer\n",
        "    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n",
        "    :param bool normalize_before: whether to use layer_norm before the first block\n",
        "    :param bool concat_after: whether to concat attention layer's input and output\n",
        "        if True, additional linear will be applied.\n",
        "        i.e. x -> x + linear(concat(x, att(x)))\n",
        "        if False, no additional linear will be applied. i.e. x -> x + att(x)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        odim,\n",
        "        attention_dim=256,\n",
        "        attention_heads=4,\n",
        "        linear_units=2048,\n",
        "        num_blocks=6,\n",
        "        dropout_rate=0.1,\n",
        "        positional_dropout_rate=0.1,\n",
        "        self_attention_dropout_rate=0.0,\n",
        "        src_attention_dropout_rate=0.0,\n",
        "        input_layer=\"embed\",\n",
        "        use_output_layer=True,\n",
        "        pos_enc_class=PositionalEncoding,\n",
        "        normalize_before=True,\n",
        "        concat_after=False,\n",
        "    ):\n",
        "        \"\"\"Construct an Decoder object.\"\"\"\n",
        "        torch.nn.Module.__init__(self)\n",
        "        #self._register_load_state_dict_pre_hook(_pre_hook)\n",
        "        if input_layer == \"embed\":\n",
        "            self.embed = torch.nn.Sequential(\n",
        "                torch.nn.Embedding(odim, attention_dim),\n",
        "                pos_enc_class(attention_dim, positional_dropout_rate),\n",
        "            )\n",
        "        # elif input_layer == \"linear\":\n",
        "        #     self.embed = torch.nn.Sequential(\n",
        "        #         torch.nn.Linear(odim, attention_dim),\n",
        "        #         torch.nn.LayerNorm(attention_dim),\n",
        "        #         torch.nn.Dropout(dropout_rate),\n",
        "        #         torch.nn.ReLU(),\n",
        "        #         pos_enc_class(attention_dim, positional_dropout_rate),\n",
        "        #     )\n",
        "        # elif isinstance(input_layer, torch.nn.Module):\n",
        "        #     self.embed = torch.nn.Sequential(\n",
        "        #         input_layer, pos_enc_class(attention_dim, positional_dropout_rate)\n",
        "        #     )\n",
        "        else:\n",
        "            raise NotImplementedError(\"only `embed` or torch.nn.Module is supported.\")\n",
        "\n",
        "        self.normalize_before = normalize_before\n",
        "        self.decoders = repeat(\n",
        "            num_blocks,\n",
        "            lambda: DecoderLayer(\n",
        "                attention_dim,\n",
        "                MultiHeadedAttention(\n",
        "                    attention_heads, attention_dim, self_attention_dropout_rate\n",
        "                ),\n",
        "                MultiHeadedAttention(\n",
        "                    attention_heads, attention_dim, src_attention_dropout_rate\n",
        "                ),\n",
        "                PositionwiseFeedForward(attention_dim, linear_units, dropout_rate),\n",
        "                dropout_rate,\n",
        "                normalize_before,\n",
        "                concat_after,\n",
        "            ),\n",
        "        )\n",
        "        if self.normalize_before:\n",
        "            self.after_norm = LayerNorm(attention_dim)\n",
        "        if use_output_layer:\n",
        "            self.output_layer = torch.nn.Linear(attention_dim, odim)\n",
        "        else:\n",
        "            self.output_layer = None\n",
        "\n",
        "    def forward(self, tgt, tgt_mask, memory, memory_mask):\n",
        "        \"\"\"Forward decoder.\n",
        "        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n",
        "                                 if input_layer == \"embed\"\n",
        "                                 input tensor (batch, maxlen_out, #mels)\n",
        "                                 in the other cases\n",
        "        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n",
        "                                      dtype=torch.uint8 in PyTorch 1.2-\n",
        "                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n",
        "        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n",
        "        :param torch.Tensor memory_mask: encoded memory mask,  (batch, maxlen_in)\n",
        "                                         dtype=torch.uint8 in PyTorch 1.2-\n",
        "                                         dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n",
        "        :return x: decoded token score before softmax (batch, maxlen_out, token)\n",
        "                   if use_output_layer is True,\n",
        "                   final block outputs (batch, maxlen_out, attention_dim)\n",
        "                   in the other cases\n",
        "        :rtype: torch.Tensor\n",
        "        :return tgt_mask: score mask before softmax (batch, maxlen_out)\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        x = self.embed(tgt)\n",
        "        x, tgt_mask, memory, memory_mask = self.decoders(\n",
        "            x, tgt_mask, memory, memory_mask\n",
        "        )\n",
        "        if self.normalize_before:\n",
        "            x = self.after_norm(x)\n",
        "        if self.output_layer is not None:\n",
        "            x = self.output_layer(x)\n",
        "        return x, tgt_mask\n",
        "\n",
        "    def forward_one_step(self, tgt, tgt_mask, memory, memory_mask=None, cache=None):\n",
        "        \"\"\"Forward one step.\n",
        "        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n",
        "        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n",
        "                                      dtype=torch.uint8 in PyTorch 1.2-\n",
        "                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n",
        "        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n",
        "        :param List[torch.Tensor] cache:\n",
        "            cached output list of (batch, max_time_out-1, size)\n",
        "        :return y, cache: NN output value and cache per `self.decoders`.\n",
        "            `y.shape` is (batch, maxlen_out, token)\n",
        "        :rtype: Tuple[torch.Tensor, List[torch.Tensor]]\n",
        "        \"\"\"\n",
        "        x = self.embed(tgt)\n",
        "        if cache is None:\n",
        "            cache = [None] * len(self.decoders)\n",
        "        new_cache = []\n",
        "        for c, decoder in zip(cache, self.decoders):\n",
        "            x, tgt_mask, memory, memory_mask = decoder(\n",
        "                x, tgt_mask, memory, memory_mask, cache=c\n",
        "            )\n",
        "            new_cache.append(x)\n",
        "\n",
        "        if self.normalize_before:\n",
        "            y = self.after_norm(x[:, -1])\n",
        "        else:\n",
        "            y = x[:, -1]\n",
        "        if self.output_layer is not None:\n",
        "            y = torch.log_softmax(self.output_layer(y), dim=-1)\n",
        "\n",
        "        return y, new_cache\n",
        "\n",
        "    # beam search API (see ScorerInterface)\n",
        "    def score(self, ys, state, x):\n",
        "        \"\"\"Score.\"\"\"\n",
        "        ys_mask = subsequent_mask(len(ys), device=x.device).unsqueeze(0)\n",
        "        logp, state = self.forward_one_step(\n",
        "            ys.unsqueeze(0), ys_mask, x.unsqueeze(0), cache=state\n",
        "        )\n",
        "        return logp.squeeze(0), state\n",
        "\n",
        "    # batch beam search API (see BatchScorerInterface)\n",
        "    def batch_score(\n",
        "        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, List[Any]]:\n",
        "        \"\"\"Score new token batch (required).\n",
        "        Args:\n",
        "            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n",
        "            states (List[Any]): Scorer states for prefix tokens.\n",
        "            xs (torch.Tensor):\n",
        "                The encoder feature that generates ys (n_batch, xlen, n_feat).\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, List[Any]]: Tuple of\n",
        "                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n",
        "                and next state list for ys.\n",
        "        \"\"\"\n",
        "        # merge states\n",
        "        n_batch = len(ys)\n",
        "        n_layers = len(self.decoders)\n",
        "        if states[0] is None:\n",
        "            batch_state = None\n",
        "        else:\n",
        "            # transpose state of [batch, layer] into [layer, batch]\n",
        "            batch_state = [\n",
        "                torch.stack([states[b][l] for b in range(n_batch)])\n",
        "                for l in range(n_layers)\n",
        "            ]\n",
        "\n",
        "        # batch decoding\n",
        "        ys_mask = subsequent_mask(ys.size(-1), device=xs.device).unsqueeze(0)\n",
        "        logp, states = self.forward_one_step(ys, ys_mask, xs, cache=batch_state)\n",
        "\n",
        "        # transpose state of [layer, batch] into [batch, layer]\n",
        "        state_list = [[states[l][b] for l in range(n_layers)] for b in range(n_batch)]\n",
        "        return logp, state_list"
      ],
      "metadata": {
        "id": "lCkqlATum0cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "D6w-KSvPtOO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_device(m, x):\n",
        "    \"\"\"Send tensor into the device of the module.\n",
        "\n",
        "    Args:\n",
        "        m (torch.nn.Module): Torch module.\n",
        "        x (Tensor): Torch tensor.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Torch tensor located in the same place as torch module.\n",
        "\n",
        "    \"\"\"\n",
        "    if isinstance(m, torch.nn.Module):\n",
        "        device = next(m.parameters()).device\n",
        "    elif isinstance(m, torch.Tensor):\n",
        "        device = m.device\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            \"Expected torch.nn.Module or torch.tensor, \" f\"bot got: {type(m)}\"\n",
        "        )\n",
        "    return x.to(device)\n",
        "\n",
        "def pad_list(xs, pad_value):\n",
        "    \"\"\"Perform padding for the list of tensors.\n",
        "\n",
        "    Args:\n",
        "        xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)].\n",
        "        pad_value (float): Value for padding.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Padded tensor (B, Tmax, `*`).\n",
        "\n",
        "    Examples:\n",
        "        >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)]\n",
        "        >>> x\n",
        "        [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]\n",
        "        >>> pad_list(x, 0)\n",
        "        tensor([[1., 1., 1., 1.],\n",
        "                [1., 1., 0., 0.],\n",
        "                [1., 0., 0., 0.]])\n",
        "\n",
        "    \"\"\"\n",
        "    n_batch = len(xs)\n",
        "    max_len = max(x.size(0) for x in xs)\n",
        "    pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)\n",
        "\n",
        "    for i in range(n_batch):\n",
        "        pad[i, : xs[i].size(0)] = xs[i]\n",
        "\n",
        "    return pad\n",
        "\n",
        "\n",
        "def make_pad_mask(lengths, xs=None, length_dim=-1, maxlen=None):\n",
        "    \"\"\"Make mask tensor containing indices of padded part.\n",
        "\n",
        "    Args:\n",
        "        lengths (LongTensor or List): Batch of lengths (B,).\n",
        "        xs (Tensor, optional): The reference tensor.\n",
        "            If set, masks will be the same shape as this tensor.\n",
        "        length_dim (int, optional): Dimension indicator of the above tensor.\n",
        "            See the example.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Mask tensor containing indices of padded part.\n",
        "                dtype=torch.uint8 in PyTorch 1.2-\n",
        "                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n",
        "\n",
        "    Examples:\n",
        "        With only lengths.\n",
        "\n",
        "        >>> lengths = [5, 3, 2]\n",
        "        >>> make_pad_mask(lengths)\n",
        "        masks = [[0, 0, 0, 0 ,0],\n",
        "                 [0, 0, 0, 1, 1],\n",
        "                 [0, 0, 1, 1, 1]]\n",
        "\n",
        "        With the reference tensor.\n",
        "\n",
        "        >>> xs = torch.zeros((3, 2, 4))\n",
        "        >>> make_pad_mask(lengths, xs)\n",
        "        tensor([[[0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0]],\n",
        "                [[0, 0, 0, 1],\n",
        "                 [0, 0, 0, 1]],\n",
        "                [[0, 0, 1, 1],\n",
        "                 [0, 0, 1, 1]]], dtype=torch.uint8)\n",
        "        >>> xs = torch.zeros((3, 2, 6))\n",
        "        >>> make_pad_mask(lengths, xs)\n",
        "        tensor([[[0, 0, 0, 0, 0, 1],\n",
        "                 [0, 0, 0, 0, 0, 1]],\n",
        "                [[0, 0, 0, 1, 1, 1],\n",
        "                 [0, 0, 0, 1, 1, 1]],\n",
        "                [[0, 0, 1, 1, 1, 1],\n",
        "                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
        "\n",
        "        With the reference tensor and dimension indicator.\n",
        "\n",
        "        >>> xs = torch.zeros((3, 6, 6))\n",
        "        >>> make_pad_mask(lengths, xs, 1)\n",
        "        tensor([[[0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [1, 1, 1, 1, 1, 1]],\n",
        "                [[0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1]],\n",
        "                [[0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
        "        >>> make_pad_mask(lengths, xs, 2)\n",
        "        tensor([[[0, 0, 0, 0, 0, 1],\n",
        "                 [0, 0, 0, 0, 0, 1],\n",
        "                 [0, 0, 0, 0, 0, 1],\n",
        "                 [0, 0, 0, 0, 0, 1],\n",
        "                 [0, 0, 0, 0, 0, 1],\n",
        "                 [0, 0, 0, 0, 0, 1]],\n",
        "                [[0, 0, 0, 1, 1, 1],\n",
        "                 [0, 0, 0, 1, 1, 1],\n",
        "                 [0, 0, 0, 1, 1, 1],\n",
        "                 [0, 0, 0, 1, 1, 1],\n",
        "                 [0, 0, 0, 1, 1, 1],\n",
        "                 [0, 0, 0, 1, 1, 1]],\n",
        "                [[0, 0, 1, 1, 1, 1],\n",
        "                 [0, 0, 1, 1, 1, 1],\n",
        "                 [0, 0, 1, 1, 1, 1],\n",
        "                 [0, 0, 1, 1, 1, 1],\n",
        "                 [0, 0, 1, 1, 1, 1],\n",
        "                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
        "\n",
        "    \"\"\"\n",
        "    if length_dim == 0:\n",
        "        raise ValueError(\"length_dim cannot be 0: {}\".format(length_dim))\n",
        "\n",
        "    if not isinstance(lengths, list):\n",
        "        lengths = lengths.tolist()\n",
        "    bs = int(len(lengths))\n",
        "    if maxlen is None:\n",
        "        if xs is None:\n",
        "            maxlen = int(max(lengths))\n",
        "        else:\n",
        "            maxlen = xs.size(length_dim)\n",
        "    else:\n",
        "        assert xs is None\n",
        "        assert maxlen >= int(max(lengths))\n",
        "\n",
        "    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n",
        "    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n",
        "    mask = seq_range_expand >= seq_length_expand\n",
        "\n",
        "    if xs is not None:\n",
        "        assert xs.size(0) == bs, (xs.size(0), bs)\n",
        "\n",
        "        if length_dim < 0:\n",
        "            length_dim = xs.dim() + length_dim\n",
        "        # ind = (:, None, ..., None, :, , None, ..., None)\n",
        "        ind = tuple(\n",
        "            slice(None) if i in (0, length_dim) else None for i in range(xs.dim())\n",
        "        )\n",
        "        mask = mask[ind].expand_as(xs).to(xs.device)\n",
        "    return mask\n",
        "\n",
        "def make_non_pad_mask(lengths, xs=None, length_dim=-1):\n",
        "    \"\"\"Make mask tensor containing indices of non-padded part.\n",
        "\n",
        "    Args:\n",
        "        lengths (LongTensor or List): Batch of lengths (B,).\n",
        "        xs (Tensor, optional): The reference tensor.\n",
        "            If set, masks will be the same shape as this tensor.\n",
        "        length_dim (int, optional): Dimension indicator of the above tensor.\n",
        "            See the example.\n",
        "\n",
        "    Returns:\n",
        "        ByteTensor: mask tensor containing indices of padded part.\n",
        "                    dtype=torch.uint8 in PyTorch 1.2-\n",
        "                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n",
        "\n",
        "    Examples:\n",
        "        With only lengths.\n",
        "\n",
        "        >>> lengths = [5, 3, 2]\n",
        "        >>> make_non_pad_mask(lengths)\n",
        "        masks = [[1, 1, 1, 1 ,1],\n",
        "                 [1, 1, 1, 0, 0],\n",
        "                 [1, 1, 0, 0, 0]]\n",
        "\n",
        "        With the reference tensor.\n",
        "\n",
        "        >>> xs = torch.zeros((3, 2, 4))\n",
        "        >>> make_non_pad_mask(lengths, xs)\n",
        "        tensor([[[1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1]],\n",
        "                [[1, 1, 1, 0],\n",
        "                 [1, 1, 1, 0]],\n",
        "                [[1, 1, 0, 0],\n",
        "                 [1, 1, 0, 0]]], dtype=torch.uint8)\n",
        "        >>> xs = torch.zeros((3, 2, 6))\n",
        "        >>> make_non_pad_mask(lengths, xs)\n",
        "        tensor([[[1, 1, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 1, 1, 0]],\n",
        "                [[1, 1, 1, 0, 0, 0],\n",
        "                 [1, 1, 1, 0, 0, 0]],\n",
        "                [[1, 1, 0, 0, 0, 0],\n",
        "                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n",
        "\n",
        "        With the reference tensor and dimension indicator.\n",
        "\n",
        "        >>> xs = torch.zeros((3, 6, 6))\n",
        "        >>> make_non_pad_mask(lengths, xs, 1)\n",
        "        tensor([[[1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [0, 0, 0, 0, 0, 0]],\n",
        "                [[1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0]],\n",
        "                [[1, 1, 1, 1, 1, 1],\n",
        "                 [1, 1, 1, 1, 1, 1],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0],\n",
        "                 [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)\n",
        "        >>> make_non_pad_mask(lengths, xs, 2)\n",
        "        tensor([[[1, 1, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 1, 1, 0]],\n",
        "                [[1, 1, 1, 0, 0, 0],\n",
        "                 [1, 1, 1, 0, 0, 0],\n",
        "                 [1, 1, 1, 0, 0, 0],\n",
        "                 [1, 1, 1, 0, 0, 0],\n",
        "                 [1, 1, 1, 0, 0, 0],\n",
        "                 [1, 1, 1, 0, 0, 0]],\n",
        "                [[1, 1, 0, 0, 0, 0],\n",
        "                 [1, 1, 0, 0, 0, 0],\n",
        "                 [1, 1, 0, 0, 0, 0],\n",
        "                 [1, 1, 0, 0, 0, 0],\n",
        "                 [1, 1, 0, 0, 0, 0],\n",
        "                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n",
        "\n",
        "    \"\"\"\n",
        "    return ~make_pad_mask(lengths, xs, length_dim)\n",
        "\n",
        "\n",
        "def th_accuracy(pad_outputs, pad_targets, ignore_label):\n",
        "    \"\"\"Calculate accuracy.\n",
        "\n",
        "    Args:\n",
        "        pad_outputs (Tensor): Prediction tensors (B * Lmax, D).\n",
        "        pad_targets (LongTensor): Target label tensors (B, Lmax, D).\n",
        "        ignore_label (int): Ignore label id.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy value (0.0 - 1.0).\n",
        "\n",
        "    \"\"\"\n",
        "    pad_pred = pad_outputs.view(\n",
        "        pad_targets.size(0), pad_targets.size(1), pad_outputs.size(1)\n",
        "    ).argmax(2)\n",
        "    mask = pad_targets != ignore_label\n",
        "    numerator = torch.sum(\n",
        "        pad_pred.masked_select(mask) == pad_targets.masked_select(mask)\n",
        "    )\n",
        "    denominator = torch.sum(mask)\n",
        "    return float(numerator) / float(denominator)\n"
      ],
      "metadata": {
        "id": "ESjiQFI8tQr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add_sos_eos"
      ],
      "metadata": {
        "id": "nnas4FCNuK-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_sos_eos(ys_pad, sos, eos, ignore_id):\n",
        "    \"\"\"Add <sos> and <eos> labels.\n",
        "\n",
        "    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n",
        "    :param int sos: index of <sos>\n",
        "    :param int eos: index of <eeos>\n",
        "    :param int ignore_id: index of padding\n",
        "    :return: padded tensor (B, Lmax)\n",
        "    :rtype: torch.Tensor\n",
        "    :return: padded tensor (B, Lmax)\n",
        "    :rtype: torch.Tensor\n",
        "    \"\"\"\n",
        "    #from espnet.nets.pytorch_backend.nets_utils import pad_list\n",
        "\n",
        "    _sos = ys_pad.new([sos])\n",
        "    _eos = ys_pad.new([eos])\n",
        "    ys = [y[y != ignore_id] for y in ys_pad]  # parse padded ys\n",
        "    ys_in = [torch.cat([_sos, y], dim=0) for y in ys]\n",
        "    ys_out = [torch.cat([y, _eos], dim=0) for y in ys]\n",
        "    return pad_list(ys_in, eos), pad_list(ys_out, ignore_id)"
      ],
      "metadata": {
        "id": "bhKNalMJuMWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LabelSmoothingLoss"
      ],
      "metadata": {
        "id": "w-Y9I92RqrVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    \"\"\"Label-smoothing loss.\n",
        "\n",
        "    :param int size: the number of class\n",
        "    :param int padding_idx: ignored class id\n",
        "    :param float smoothing: smoothing rate (0.0 means the conventional CE)\n",
        "    :param bool normalize_length: normalize loss by sequence length if True\n",
        "    :param torch.nn.Module criterion: loss function to be smoothed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        size,\n",
        "        padding_idx,\n",
        "        smoothing,\n",
        "        normalize_length=False,\n",
        "        criterion=nn.KLDivLoss(reduction=\"none\"),\n",
        "    ):\n",
        "        \"\"\"Construct an LabelSmoothingLoss object.\"\"\"\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.criterion = criterion\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        self.normalize_length = normalize_length\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        \"\"\"Compute loss between x and target.\n",
        "\n",
        "        :param torch.Tensor x: prediction (batch, seqlen, class)\n",
        "        :param torch.Tensor target:\n",
        "            target signal masked with self.padding_id (batch, seqlen)\n",
        "        :return: scalar float value\n",
        "        :rtype torch.Tensor\n",
        "        \"\"\"\n",
        "        assert x.size(2) == self.size\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(-1, self.size)\n",
        "        target = target.view(-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = x.clone()\n",
        "            true_dist.fill_(self.smoothing / (self.size - 1))\n",
        "            ignore = target == self.padding_idx  # (B,)\n",
        "            total = len(target) - ignore.sum().item()\n",
        "            target = target.masked_fill(ignore, 0)  # avoid -1 index\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        kl = self.criterion(torch.log_softmax(x, dim=1), true_dist)\n",
        "        denom = total if self.normalize_length else batch_size\n",
        "        return kl.masked_fill(ignore.unsqueeze(1), 0).sum() / denom"
      ],
      "metadata": {
        "id": "HJCMkwOZq11b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CTC"
      ],
      "metadata": {
        "id": "P4_0WJ1RrLa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CTC(torch.nn.Module):\n",
        "    \"\"\"CTC module\n",
        "\n",
        "    :param int odim: dimension of outputs\n",
        "    :param int eprojs: number of encoder projection units\n",
        "    :param float dropout_rate: dropout rate (0.0 ~ 1.0)\n",
        "    :param str ctc_type: builtin or warpctc\n",
        "    :param bool reduce: reduce the CTC loss into a scalar\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, odim, eprojs, dropout_rate, ctc_type=\"builtin\", reduce=True):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.loss = None\n",
        "        self.ctc_lo = torch.nn.Linear(eprojs, odim)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.probs = None  # for visualization\n",
        "\n",
        "        # In case of Pytorch >= 1.7.0, CTC will be always builtin\n",
        "        self.ctc_type = (\n",
        "            ctc_type\n",
        "            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")\n",
        "            else \"builtin\"\n",
        "        )\n",
        "\n",
        "        if ctc_type != self.ctc_type:\n",
        "            logging.debug(f\"CTC was set to {self.ctc_type} due to PyTorch version.\")\n",
        "\n",
        "        if self.ctc_type == \"builtin\":\n",
        "            reduction_type = \"sum\" if reduce else \"none\"\n",
        "            self.ctc_loss = torch.nn.CTCLoss(\n",
        "                reduction=reduction_type, zero_infinity=True\n",
        "            )\n",
        "        # elif self.ctc_type == \"cudnnctc\":\n",
        "        #     reduction_type = \"sum\" if reduce else \"none\"\n",
        "        #     self.ctc_loss = torch.nn.CTCLoss(reduction=reduction_type)\n",
        "        # elif self.ctc_type == \"warpctc\":\n",
        "        #     import warpctc_pytorch as warp_ctc\n",
        "\n",
        "        #     self.ctc_loss = warp_ctc.CTCLoss(size_average=True, reduce=reduce)\n",
        "        # elif self.ctc_type == \"gtnctc\":\n",
        "        #     from espnet.nets.pytorch_backend.gtn_ctc import GTNCTCLossFunction\n",
        "\n",
        "        #     self.ctc_loss = GTNCTCLossFunction.apply\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'ctc_type must be \"builtin\" or \"warpctc\": {}'.format(self.ctc_type)\n",
        "            )\n",
        "\n",
        "        self.ignore_id = -1\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def loss_fn(self, th_pred, th_target, th_ilen, th_olen):\n",
        "        if self.ctc_type in [\"builtin\", \"cudnnctc\"]:\n",
        "            th_pred = th_pred.log_softmax(2)\n",
        "            # Use the deterministic CuDNN implementation of CTC loss to avoid\n",
        "            #  [issue#17798](https://github.com/pytorch/pytorch/issues/17798)\n",
        "            with torch.backends.cudnn.flags(deterministic=True):\n",
        "                loss = self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n",
        "            # Batch-size average\n",
        "            loss = loss / th_pred.size(1)\n",
        "            return loss\n",
        "        elif self.ctc_type == \"warpctc\":\n",
        "            return self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n",
        "        elif self.ctc_type == \"gtnctc\":\n",
        "            targets = [t.tolist() for t in th_target]\n",
        "            log_probs = torch.nn.functional.log_softmax(th_pred, dim=2)\n",
        "            return self.ctc_loss(log_probs, targets, th_ilen, 0, \"none\")\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def forward(self, hs_pad, hlens, ys_pad):\n",
        "        \"\"\"CTC forward\n",
        "\n",
        "        :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D)\n",
        "        :param torch.Tensor hlens: batch of lengths of hidden state sequences (B)\n",
        "        :param torch.Tensor ys_pad:\n",
        "            batch of padded character id sequence tensor (B, Lmax)\n",
        "        :return: ctc loss value\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        # TODO(kan-bayashi): need to make more smart way\n",
        "        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n",
        "\n",
        "        # zero padding for hs\n",
        "        ys_hat = self.ctc_lo(self.dropout(hs_pad))\n",
        "        if self.ctc_type != \"gtnctc\":\n",
        "            ys_hat = ys_hat.transpose(0, 1)\n",
        "\n",
        "        if self.ctc_type == \"builtin\":\n",
        "            olens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))\n",
        "            hlens = hlens.long()\n",
        "            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix\n",
        "            self.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)\n",
        "        else:\n",
        "            self.loss = None\n",
        "            hlens = torch.from_numpy(np.fromiter(hlens, dtype=np.int32))\n",
        "            olens = torch.from_numpy(\n",
        "                np.fromiter((x.size(0) for x in ys), dtype=np.int32)\n",
        "            )\n",
        "            # zero padding for ys\n",
        "            ys_true = torch.cat(ys).cpu().int()  # batch x olen\n",
        "            # get ctc loss\n",
        "            # expected shape of seqLength x batchSize x alphabet_size\n",
        "            dtype = ys_hat.dtype\n",
        "            if self.ctc_type == \"warpctc\" or dtype == torch.float16:\n",
        "                # warpctc only supports float32\n",
        "                # torch.ctc does not support float16 (#1751)\n",
        "                ys_hat = ys_hat.to(dtype=torch.float32)\n",
        "            if self.ctc_type == \"cudnnctc\":\n",
        "                # use GPU when using the cuDNN implementation\n",
        "                ys_true = to_device(hs_pad, ys_true)\n",
        "            if self.ctc_type == \"gtnctc\":\n",
        "                # keep as list for gtn\n",
        "                ys_true = ys\n",
        "            self.loss = to_device(\n",
        "                hs_pad, self.loss_fn(ys_hat, ys_true, hlens, olens)\n",
        "            ).to(dtype=dtype)\n",
        "\n",
        "        # get length info\n",
        "        \"\"\"\n",
        "        logging.debug(\n",
        "            self.__class__.__name__\n",
        "            + \" input lengths:  \"\n",
        "            + \"\".join(str(hlens).split(\"\\n\"))\n",
        "        )\n",
        "        logging.debug(\n",
        "            self.__class__.__name__\n",
        "            + \" output lengths: \"\n",
        "            + \"\".join(str(olens).split(\"\\n\"))\n",
        "        )\n",
        "        \"\"\"\n",
        "        if self.reduce:\n",
        "            # NOTE: sum() is needed to keep consistency\n",
        "            # since warpctc return as tensor w/ shape (1,)\n",
        "            # but builtin return as tensor w/o shape (scalar).\n",
        "            self.loss = self.loss.sum()\n",
        "            # logging.debug(\"ctc loss:\" + str(float(self.loss)))\n",
        "\n",
        "        return self.loss, ys_hat\n",
        "\n",
        "    def softmax(self, hs_pad):\n",
        "        \"\"\"softmax of frame activations\n",
        "\n",
        "        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n",
        "        :return: log softmax applied 3d tensor (B, Tmax, odim)\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        self.probs = F.softmax(self.ctc_lo(hs_pad), dim=-1)\n",
        "        return self.probs\n",
        "\n",
        "    def log_softmax(self, hs_pad):\n",
        "        \"\"\"log_softmax of frame activations\n",
        "\n",
        "        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n",
        "        :return: log softmax applied 3d tensor (B, Tmax, odim)\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        return F.log_softmax(self.ctc_lo(hs_pad), dim=-1)\n",
        "\n",
        "    def argmax(self, hs_pad):\n",
        "        \"\"\"argmax of frame activations\n",
        "\n",
        "        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n",
        "        :return: argmax applied 2d tensor (B, Tmax)\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        return torch.argmax(self.ctc_lo(hs_pad), dim=-1)\n",
        "\n",
        "    def forced_align(self, h, y, blank_id=0):\n",
        "        \"\"\"forced alignment.\n",
        "\n",
        "        :param torch.Tensor h: hidden state sequence, 2d tensor (T, D)\n",
        "        :param torch.Tensor y: id sequence tensor 1d tensor (L)\n",
        "        :param int y: blank symbol index\n",
        "        :return: best alignment results\n",
        "        :rtype: list\n",
        "        \"\"\"\n",
        "\n",
        "        def interpolate_blank(label, blank_id=0):\n",
        "            \"\"\"Insert blank token between every two label token.\"\"\"\n",
        "            label = np.expand_dims(label, 1)\n",
        "            blanks = np.zeros((label.shape[0], 1), dtype=np.int64) + blank_id\n",
        "            label = np.concatenate([blanks, label], axis=1)\n",
        "            label = label.reshape(-1)\n",
        "            label = np.append(label, label[0])\n",
        "            return label\n",
        "\n",
        "        lpz = self.log_softmax(h)\n",
        "        lpz = lpz.squeeze(0)\n",
        "\n",
        "        y_int = interpolate_blank(y, blank_id)\n",
        "\n",
        "        logdelta = np.zeros((lpz.size(0), len(y_int))) - 100000000000.0  # log of zero\n",
        "        state_path = (\n",
        "            np.zeros((lpz.size(0), len(y_int)), dtype=np.int16) - 1\n",
        "        )  # state path\n",
        "\n",
        "        logdelta[0, 0] = lpz[0][y_int[0]]\n",
        "        logdelta[0, 1] = lpz[0][y_int[1]]\n",
        "\n",
        "        for t in six.moves.range(1, lpz.size(0)):\n",
        "            for s in six.moves.range(len(y_int)):\n",
        "                if y_int[s] == blank_id or s < 2 or y_int[s] == y_int[s - 2]:\n",
        "                    candidates = np.array([logdelta[t - 1, s], logdelta[t - 1, s - 1]])\n",
        "                    prev_state = [s, s - 1]\n",
        "                else:\n",
        "                    candidates = np.array(\n",
        "                        [\n",
        "                            logdelta[t - 1, s],\n",
        "                            logdelta[t - 1, s - 1],\n",
        "                            logdelta[t - 1, s - 2],\n",
        "                        ]\n",
        "                    )\n",
        "                    prev_state = [s, s - 1, s - 2]\n",
        "                logdelta[t, s] = np.max(candidates) + lpz[t][y_int[s]]\n",
        "                state_path[t, s] = prev_state[np.argmax(candidates)]\n",
        "\n",
        "        state_seq = -1 * np.ones((lpz.size(0), 1), dtype=np.int16)\n",
        "\n",
        "        candidates = np.array(\n",
        "            [logdelta[-1, len(y_int) - 1], logdelta[-1, len(y_int) - 2]]\n",
        "        )\n",
        "        prev_state = [len(y_int) - 1, len(y_int) - 2]\n",
        "        state_seq[-1] = prev_state[np.argmax(candidates)]\n",
        "        for t in six.moves.range(lpz.size(0) - 2, -1, -1):\n",
        "            state_seq[t] = state_path[t + 1, state_seq[t + 1, 0]]\n",
        "\n",
        "        output_state_seq = []\n",
        "        for t in six.moves.range(0, lpz.size(0)):\n",
        "            output_state_seq.append(y_int[state_seq[t, 0]])\n",
        "\n",
        "        return output_state_seq\n",
        "\n",
        "    def forced_align_batch(self, hs_pad, ys_pad, ilens, blank_id=0):\n",
        "        \"\"\"forced alignment with batch processing.\n",
        "\n",
        "        :param torch.Tensor hs_pad: hidden state sequence, 3d tensor (T, B, D)\n",
        "        :param torch.Tensor ys_pad: id sequence tensor 2d tensor (B, L)\n",
        "        :param torch.Tensor ilens: Input length of each utterance (B,)\n",
        "        :param int blank_id: blank symbol index\n",
        "        :return: best alignment results\n",
        "        :rtype: list of numpy.array\n",
        "        \"\"\"\n",
        "\n",
        "        def interpolate_blank(label, olens_int):\n",
        "            \"\"\"Insert blank token between every two label token.\"\"\"\n",
        "            lab_len = label.shape[1] * 2 + 1\n",
        "            label_out = np.full((label.shape[0], lab_len), blank_id, dtype=np.int64)\n",
        "            label_out[:, 1::2] = label\n",
        "            for b in range(label.shape[0]):\n",
        "                label_out[b, olens_int[b] * 2 + 1 :] = self.ignore_id\n",
        "            return label_out\n",
        "\n",
        "        neginf = float(\"-inf\")  # log of zero\n",
        "        # lpz = self.log_softmax(hs_pad).cpu().detach().numpy()\n",
        "        # hs_pad = hs_pad.transpose(1,0)\n",
        "        lpz = F.log_softmax(hs_pad, dim=-1).cpu().detach().numpy()\n",
        "        ilens = ilens.cpu().detach().numpy()\n",
        "\n",
        "        ys_pad = ys_pad.cpu().detach().numpy()\n",
        "        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n",
        "        olens = np.array([len(s) for s in ys])\n",
        "        olens_int = olens * 2 + 1\n",
        "        ys_int = interpolate_blank(ys_pad, olens_int)\n",
        "\n",
        "        Tmax, B, _ = lpz.shape\n",
        "        Lmax = ys_int.shape[-1]\n",
        "        logdelta = np.full((Tmax, B, Lmax), neginf, dtype=lpz.dtype)\n",
        "        state_path = -np.ones(logdelta.shape, dtype=np.int16)  # state path\n",
        "\n",
        "        b_indx = np.arange(B, dtype=np.int64)\n",
        "        t_0 = np.zeros(B, dtype=np.int64)\n",
        "        logdelta[0, :, 0] = lpz[t_0, b_indx, ys_int[:, 0]]\n",
        "        logdelta[0, :, 1] = lpz[t_0, b_indx, ys_int[:, 1]]\n",
        "\n",
        "        s_indx_mat = np.arange(Lmax)[None, :].repeat(B, 0)\n",
        "        notignore_mat = ys_int != self.ignore_id\n",
        "        same_lab_mat = np.zeros((B, Lmax), dtype=np.bool)\n",
        "        same_lab_mat[:, 3::2] = ys_int[:, 3::2] == ys_int[:, 1:-2:2]\n",
        "        Lmin = olens_int.min()\n",
        "        for t in range(1, Tmax):\n",
        "            s_start = max(0, Lmin - (Tmax - t) * 2)\n",
        "            s_end = min(Lmax, t * 2 + 2)\n",
        "            candidates = np.full((B, Lmax, 3), neginf, dtype=logdelta.dtype)\n",
        "            candidates[:, :, 0] = logdelta[t - 1, :, :]\n",
        "            candidates[:, 1:, 1] = logdelta[t - 1, :, :-1]\n",
        "            candidates[:, 3::2, 2] = logdelta[t - 1, :, 1:-2:2]\n",
        "            candidates[same_lab_mat, 2] = neginf\n",
        "            candidates_ = candidates[:, s_start:s_end, :]\n",
        "            idx = candidates_.argmax(-1)\n",
        "            b_i, s_i = np.ogrid[:B, : idx.shape[-1]]\n",
        "            nignore = notignore_mat[:, s_start:s_end]\n",
        "            logdelta[t, :, s_start:s_end][nignore] = (\n",
        "                candidates_[b_i, s_i, idx][nignore]\n",
        "                + lpz[t, b_i, ys_int[:, s_start:s_end]][nignore]\n",
        "            )\n",
        "            s = s_indx_mat[:, s_start:s_end]\n",
        "            state_path[t, :, s_start:s_end][nignore] = (s - idx)[nignore]\n",
        "\n",
        "        alignments = []\n",
        "        prev_states = logdelta[\n",
        "            ilens[:, None] - 1,\n",
        "            b_indx[:, None],\n",
        "            np.stack([olens_int - 2, olens_int - 1], -1),\n",
        "        ].argmax(-1)\n",
        "        for b in range(B):\n",
        "            T, L = ilens[b], olens_int[b]\n",
        "            prev_state = prev_states[b] + L - 2\n",
        "            ali = np.empty(T, dtype=ys_int.dtype)\n",
        "            ali[T - 1] = ys_int[b, prev_state]\n",
        "            for t in range(T - 2, -1, -1):\n",
        "                prev_state = state_path[t + 1, b, prev_state]\n",
        "                ali[t] = ys_int[b, prev_state]\n",
        "            alignments.append(ali)\n",
        "\n",
        "        return alignments"
      ],
      "metadata": {
        "id": "tVaK6pp-rNUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E2E AV"
      ],
      "metadata": {
        "id": "6-ciO8kw0Wda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Transformer speech recognition model (pytorch).\"\"\"\n",
        "\n",
        "# import logging\n",
        "# import numpy\n",
        "# import torch\n",
        "\n",
        "# from espnet.nets.pytorch_backend.ctc import CTC\n",
        "# from espnet.nets.pytorch_backend.nets_utils import (\n",
        "#     make_non_pad_mask,\n",
        "#     th_accuracy,\n",
        "# )\n",
        "# from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n",
        "# from espnet.nets.pytorch_backend.transformer.decoder import Decoder\n",
        "# from espnet.nets.pytorch_backend.transformer.encoder import Encoder\n",
        "# from espnet.nets.pytorch_backend.transformer.label_smoothing_loss import LabelSmoothingLoss\n",
        "# from espnet.nets.pytorch_backend.transformer.mask import target_mask\n",
        "# from espnet.nets.pytorch_backend.nets_utils import MLPHead\n",
        "\n",
        "\n",
        "class E2E(torch.nn.Module):\n",
        "    def __init__(self, odim, args, ignore_id=-1):\n",
        "        torch.nn.Module.__init__(self)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            attention_dim=args.adim,\n",
        "            attention_heads=args.aheads,\n",
        "            linear_units=args.eunits,\n",
        "            num_blocks=args.elayers,\n",
        "            input_layer=args.transformer_input_layer,\n",
        "            dropout_rate=args.dropout_rate,\n",
        "            positional_dropout_rate=args.dropout_rate,\n",
        "            attention_dropout_rate=args.transformer_attn_dropout_rate,\n",
        "            encoder_attn_layer_type=args.transformer_encoder_attn_layer_type,\n",
        "            macaron_style=args.macaron_style,\n",
        "            use_cnn_module=args.use_cnn_module,\n",
        "            cnn_module_kernel=args.cnn_module_kernel,\n",
        "            zero_triu=getattr(args, \"zero_triu\", False),\n",
        "            a_upsample_ratio=args.a_upsample_ratio,\n",
        "            relu_type=getattr(args, \"relu_type\", \"swish\"),\n",
        "        )\n",
        "\n",
        "        self.aux_encoder = Encoder(\n",
        "            attention_dim=args.aux_adim,\n",
        "            attention_heads=args.aux_aheads,\n",
        "            linear_units=args.aux_eunits,\n",
        "            num_blocks=args.aux_elayers,\n",
        "            input_layer=args.aux_transformer_input_layer,\n",
        "            dropout_rate=args.aux_dropout_rate,\n",
        "            positional_dropout_rate=args.aux_dropout_rate,\n",
        "            attention_dropout_rate=args.aux_transformer_attn_dropout_rate,\n",
        "            encoder_attn_layer_type=args.aux_transformer_encoder_attn_layer_type,\n",
        "            macaron_style=args.aux_macaron_style,\n",
        "            use_cnn_module=args.aux_use_cnn_module,\n",
        "            cnn_module_kernel=args.aux_cnn_module_kernel,\n",
        "            zero_triu=getattr(args, \"aux_zero_triu\", False),\n",
        "            a_upsample_ratio=args.aux_a_upsample_ratio,\n",
        "            relu_type=getattr(args, \"aux_relu_type\", \"swish\"),\n",
        "        )\n",
        "\n",
        "        # self.transformer_input_layer = args.transformer_input_layer\n",
        "        # self.a_upsample_ratio = args.a_upsample_ratio\n",
        "\n",
        "        self.fusion = MLPHead(\n",
        "            idim=args.adim + args.aux_adim,\n",
        "            hdim=args.fusion_hdim,\n",
        "            odim=args.adim,\n",
        "            norm=args.fusion_norm,\n",
        "        )\n",
        "\n",
        "        self.proj_decoder = None\n",
        "        if args.adim != args.ddim:\n",
        "            self.proj_decoder = torch.nn.Linear(args.adim, args.ddim)\n",
        "\n",
        "        if args.mtlalpha < 1:\n",
        "            self.decoder = Decoder(\n",
        "                odim=odim,\n",
        "                attention_dim=args.ddim,\n",
        "                attention_heads=args.dheads,\n",
        "                linear_units=args.dunits,\n",
        "                num_blocks=args.dlayers,\n",
        "                dropout_rate=args.dropout_rate,\n",
        "                positional_dropout_rate=args.dropout_rate,\n",
        "                self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n",
        "                src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n",
        "            )\n",
        "        else:\n",
        "            self.decoder = None\n",
        "\n",
        "        self.blank = 0\n",
        "        self.sos = odim - 1\n",
        "        self.eos = odim - 1\n",
        "        self.odim = odim\n",
        "        self.ignore_id = ignore_id\n",
        "\n",
        "        # self.lsm_weight = a\n",
        "        self.criterion = LabelSmoothingLoss(\n",
        "            self.odim,\n",
        "            self.ignore_id,\n",
        "            args.lsm_weight,\n",
        "            args.transformer_length_normalized_loss,\n",
        "        )\n",
        "\n",
        "        self.adim = args.adim\n",
        "        self.mtlalpha = args.mtlalpha\n",
        "        if args.mtlalpha > 0.0:\n",
        "            self.ctc = CTC(\n",
        "                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n",
        "            )\n",
        "        else:\n",
        "            self.ctc = None\n",
        "\n",
        "    def forward(self, video, audio, video_lengths, audio_lengths, label):\n",
        "        video_padding_mask = make_non_pad_mask(video_lengths).to(video.device).unsqueeze(-2)\n",
        "        video_feat, _ = self.encoder(video, video_padding_mask)\n",
        "\n",
        "        audio_lengths = torch.div(audio_lengths, 640, rounding_mode=\"trunc\")\n",
        "        audio_padding_mask = make_non_pad_mask(audio_lengths).to(video.device).unsqueeze(-2)\n",
        "        audio_feat, _ = self.aux_encoder(audio, audio_padding_mask)\n",
        "\n",
        "        x = self.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n",
        "\n",
        "        # ctc loss\n",
        "        loss_ctc, ys_hat = self.ctc(x, video_lengths, label)\n",
        "\n",
        "        if self.proj_decoder:\n",
        "            x = self.proj_decoder(x)\n",
        "\n",
        "        # decoder loss\n",
        "        ys_in_pad, ys_out_pad = add_sos_eos(label, self.sos, self.eos, self.ignore_id)\n",
        "        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n",
        "        pred_pad, _ = self.decoder(ys_in_pad, ys_mask, x, video_padding_mask)\n",
        "        loss_att = self.criterion(pred_pad, ys_out_pad)\n",
        "        loss = self.mtlalpha * loss_ctc + (1 - self.mtlalpha) * loss_att\n",
        "\n",
        "        acc = th_accuracy(\n",
        "            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n",
        "        )\n",
        "\n",
        "        return loss, loss_ctc, loss_att, acc"
      ],
      "metadata": {
        "id": "RSYDWjNC0V0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E2E"
      ],
      "metadata": {
        "id": "tYvrFGxC06jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Transformer speech recognition model (pytorch).\"\"\"\n",
        "\n",
        "# import logging\n",
        "# import numpy\n",
        "# import torch\n",
        "\n",
        "# from espnet.nets.pytorch_backend.ctc import CTC\n",
        "# from espnet.nets.pytorch_backend.nets_utils import (\n",
        "#     make_non_pad_mask,\n",
        "#     th_accuracy,\n",
        "# )\n",
        "# from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n",
        "# from espnet.nets.pytorch_backend.transformer.decoder import Decoder\n",
        "# from espnet.nets.pytorch_backend.transformer.encoder import Encoder\n",
        "# from espnet.nets.pytorch_backend.transformer.label_smoothing_loss import LabelSmoothingLoss\n",
        "# from espnet.nets.pytorch_backend.transformer.mask import target_mask\n",
        "\n",
        "\n",
        "class E2E(torch.nn.Module):\n",
        "    def __init__(self, odim, args, ignore_id=-1):\n",
        "        torch.nn.Module.__init__(self)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            attention_dim=args.adim,\n",
        "            attention_heads=args.aheads,\n",
        "            linear_units=args.eunits,\n",
        "            num_blocks=args.elayers,\n",
        "            input_layer=args.transformer_input_layer,\n",
        "            dropout_rate=args.dropout_rate,\n",
        "            positional_dropout_rate=args.dropout_rate,\n",
        "            attention_dropout_rate=args.transformer_attn_dropout_rate,\n",
        "            encoder_attn_layer_type=args.transformer_encoder_attn_layer_type,\n",
        "            macaron_style=args.macaron_style,\n",
        "            use_cnn_module=args.use_cnn_module,\n",
        "            cnn_module_kernel=args.cnn_module_kernel,\n",
        "            zero_triu=getattr(args, \"zero_triu\", False),\n",
        "            a_upsample_ratio=args.a_upsample_ratio,\n",
        "            relu_type=getattr(args, \"relu_type\", \"swish\"),\n",
        "        )\n",
        "\n",
        "        self.transformer_input_layer = args.transformer_input_layer\n",
        "        self.a_upsample_ratio = args.a_upsample_ratio\n",
        "\n",
        "        self.proj_decoder = None\n",
        "        if args.adim != args.ddim:\n",
        "            self.proj_decoder = torch.nn.Linear(args.adim, args.ddim)\n",
        "\n",
        "        if args.mtlalpha < 1:\n",
        "            self.decoder = Decoder(\n",
        "                odim=odim,\n",
        "                attention_dim=args.ddim,\n",
        "                attention_heads=args.dheads,\n",
        "                linear_units=args.dunits,\n",
        "                num_blocks=args.dlayers,\n",
        "                dropout_rate=args.dropout_rate,\n",
        "                positional_dropout_rate=args.dropout_rate,\n",
        "                self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n",
        "                src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n",
        "            )\n",
        "        else:\n",
        "            self.decoder = None\n",
        "        self.blank = 0\n",
        "        self.sos = odim - 1\n",
        "        self.eos = odim - 1\n",
        "        self.odim = odim\n",
        "        self.ignore_id = ignore_id\n",
        "\n",
        "        # self.lsm_weight = a\n",
        "        self.criterion = LabelSmoothingLoss(\n",
        "            self.odim,\n",
        "            self.ignore_id,\n",
        "            args.lsm_weight,\n",
        "            args.transformer_length_normalized_loss,\n",
        "        )\n",
        "\n",
        "        self.adim = args.adim\n",
        "        self.mtlalpha = args.mtlalpha\n",
        "        if args.mtlalpha > 0.0:\n",
        "            self.ctc = CTC(\n",
        "                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n",
        "            )\n",
        "        else:\n",
        "            self.ctc = None\n",
        "\n",
        "    def forward(self, x, lengths, label):\n",
        "        if self.transformer_input_layer == \"conv1d\":\n",
        "            lengths = torch.div(lengths, 640, rounding_mode=\"trunc\")\n",
        "        padding_mask = make_non_pad_mask(lengths).to(x.device).unsqueeze(-2)\n",
        "\n",
        "        x, _ = self.encoder(x, padding_mask)\n",
        "\n",
        "        # ctc loss\n",
        "        loss_ctc, ys_hat = self.ctc(x, lengths, label)\n",
        "\n",
        "        if self.proj_decoder:\n",
        "            x = self.proj_decoder(x)\n",
        "\n",
        "        # decoder loss\n",
        "        ys_in_pad, ys_out_pad = add_sos_eos(label, self.sos, self.eos, self.ignore_id)\n",
        "        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n",
        "        pred_pad, _ = self.decoder(ys_in_pad, ys_mask, x, padding_mask)\n",
        "        loss_att = self.criterion(pred_pad, ys_out_pad)\n",
        "        loss = self.mtlalpha * loss_ctc + (1 - self.mtlalpha) * loss_att\n",
        "\n",
        "        acc = th_accuracy(\n",
        "            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n",
        "        )\n",
        "\n",
        "        return loss, loss_ctc, loss_att, acc"
      ],
      "metadata": {
        "id": "aRhg5iO508ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine"
      ],
      "metadata": {
        "id": "3nQEmdFqNcjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupCosineScheduler(_LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_epochs, num_epochs, iter_per_epoch):\n",
        "        self.base_lrs = {\n",
        "            param_group[\"name\"]: param_group[\"lr\"]\n",
        "            for param_group in optimizer.param_groups\n",
        "        }\n",
        "        self.warmup_iter = warmup_epochs * iter_per_epoch\n",
        "        self.total_iter = num_epochs * iter_per_epoch\n",
        "        self.optimizer = optimizer\n",
        "        self.iter = 0\n",
        "        self.current_lr = 0\n",
        "\n",
        "        self.init_lr()  # so that at first step we have the correct step size\n",
        "\n",
        "    def get_lr(self, base_lr):\n",
        "        if self.iter < self.warmup_iter:\n",
        "            return base_lr * self.iter / self.warmup_iter\n",
        "        else:\n",
        "            decay_iter = self.total_iter - self.warmup_iter\n",
        "            return (\n",
        "                0.5\n",
        "                * base_lr\n",
        "                * (1 + np.cos(np.pi * (self.iter - self.warmup_iter) / decay_iter))\n",
        "            )\n",
        "\n",
        "    def update_param_groups(self):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = self.get_lr(self.base_lrs[param_group[\"name\"]])\n",
        "\n",
        "    def step(self):\n",
        "        self.update_param_groups()\n",
        "        self.iter += 1\n",
        "\n",
        "    def init_lr(self):\n",
        "        self.update_param_groups()"
      ],
      "metadata": {
        "id": "SdB5dFVWNfH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CTCPrefixScorer"
      ],
      "metadata": {
        "id": "5bvueiRMObJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CTCPrefixScoreTH(object):\n",
        "    \"\"\"Batch processing of CTCPrefixScore\n",
        "\n",
        "    which is based on Algorithm 2 in WATANABE et al.\n",
        "    \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\"\n",
        "    but extended to efficiently compute the label probablities for multiple\n",
        "    hypotheses simultaneously\n",
        "    See also Seki et al. \"Vectorized Beam Search for CTC-Attention-Based\n",
        "    Speech Recognition,\" In INTERSPEECH (pp. 3825-3829), 2019.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x, xlens, blank, eos, margin=0):\n",
        "        \"\"\"Construct CTC prefix scorer\n",
        "\n",
        "        :param torch.Tensor x: input label posterior sequences (B, T, O)\n",
        "        :param torch.Tensor xlens: input lengths (B,)\n",
        "        :param int blank: blank label id\n",
        "        :param int eos: end-of-sequence id\n",
        "        :param int margin: margin parameter for windowing (0 means no windowing)\n",
        "        \"\"\"\n",
        "        # In the comment lines,\n",
        "        # we assume T: input_length, B: batch size, W: beam width, O: output dim.\n",
        "        self.logzero = -10000000000.0\n",
        "        self.blank = blank\n",
        "        self.eos = eos\n",
        "        self.batch = x.size(0)\n",
        "        self.input_length = x.size(1)\n",
        "        self.odim = x.size(2)\n",
        "        self.dtype = x.dtype\n",
        "        self.device = (\n",
        "            torch.device(\"cuda:%d\" % x.get_device())\n",
        "            if x.is_cuda\n",
        "            else torch.device(\"cpu\")\n",
        "        )\n",
        "        # Pad the rest of posteriors in the batch\n",
        "        # TODO(takaaki-hori): need a better way without for-loops\n",
        "        for i, l in enumerate(xlens):\n",
        "            if l < self.input_length:\n",
        "                x[i, l:, :] = self.logzero\n",
        "                x[i, l:, blank] = 0\n",
        "        # Reshape input x\n",
        "        xn = x.transpose(0, 1)  # (B, T, O) -> (T, B, O)\n",
        "        xb = xn[:, :, self.blank].unsqueeze(2).expand(-1, -1, self.odim)\n",
        "        self.x = torch.stack([xn, xb])  # (2, T, B, O)\n",
        "        self.end_frames = torch.as_tensor(xlens) - 1\n",
        "\n",
        "        # Setup CTC windowing\n",
        "        self.margin = margin\n",
        "        if margin > 0:\n",
        "            self.frame_ids = torch.arange(\n",
        "                self.input_length, dtype=self.dtype, device=self.device\n",
        "            )\n",
        "        # Base indices for index conversion\n",
        "        self.idx_bh = None\n",
        "        self.idx_b = torch.arange(self.batch, device=self.device)\n",
        "        self.idx_bo = (self.idx_b * self.odim).unsqueeze(1)\n",
        "\n",
        "    def __call__(self, y, state, scoring_ids=None, att_w=None):\n",
        "        \"\"\"Compute CTC prefix scores for next labels\n",
        "\n",
        "        :param list y: prefix label sequences\n",
        "        :param tuple state: previous CTC state\n",
        "        :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O)\n",
        "        :param torch.Tensor att_w: attention weights to decide CTC window\n",
        "        :return new_state, ctc_local_scores (BW, O)\n",
        "        \"\"\"\n",
        "        output_length = len(y[0]) - 1  # ignore sos\n",
        "        last_ids = [yi[-1] for yi in y]  # last output label ids\n",
        "        n_bh = len(last_ids)  # batch * hyps\n",
        "        n_hyps = n_bh // self.batch  # assuming each utterance has the same # of hyps\n",
        "        self.scoring_num = scoring_ids.size(-1) if scoring_ids is not None else 0\n",
        "        # prepare state info\n",
        "        if state is None:\n",
        "            r_prev = torch.full(\n",
        "                (self.input_length, 2, self.batch, n_hyps),\n",
        "                self.logzero,\n",
        "                dtype=self.dtype,\n",
        "                device=self.device,\n",
        "            )\n",
        "            r_prev[:, 1] = torch.cumsum(self.x[0, :, :, self.blank], 0).unsqueeze(2)\n",
        "            r_prev = r_prev.view(-1, 2, n_bh)\n",
        "            s_prev = 0.0\n",
        "            f_min_prev = 0\n",
        "            f_max_prev = 1\n",
        "        else:\n",
        "            r_prev, s_prev, f_min_prev, f_max_prev = state\n",
        "\n",
        "        # select input dimensions for scoring\n",
        "        if self.scoring_num > 0:\n",
        "            scoring_idmap = torch.full(\n",
        "                (n_bh, self.odim), -1, dtype=torch.long, device=self.device\n",
        "            )\n",
        "            snum = self.scoring_num\n",
        "            if self.idx_bh is None or n_bh > len(self.idx_bh):\n",
        "                self.idx_bh = torch.arange(n_bh, device=self.device).view(-1, 1)\n",
        "            scoring_idmap[self.idx_bh[:n_bh], scoring_ids] = torch.arange(\n",
        "                snum, device=self.device\n",
        "            )\n",
        "            scoring_idx = (\n",
        "                scoring_ids + self.idx_bo.repeat(1, n_hyps).view(-1, 1)\n",
        "            ).view(-1)\n",
        "            x_ = torch.index_select(\n",
        "                self.x.view(2, -1, self.batch * self.odim), 2, scoring_idx\n",
        "            ).view(2, -1, n_bh, snum)\n",
        "        else:\n",
        "            scoring_ids = None\n",
        "            scoring_idmap = None\n",
        "            snum = self.odim\n",
        "            x_ = self.x.unsqueeze(3).repeat(1, 1, 1, n_hyps, 1).view(2, -1, n_bh, snum)\n",
        "\n",
        "        # new CTC forward probs are prepared as a (T x 2 x BW x S) tensor\n",
        "        # that corresponds to r_t^n(h) and r_t^b(h) in a batch.\n",
        "        r = torch.full(\n",
        "            (self.input_length, 2, n_bh, snum),\n",
        "            self.logzero,\n",
        "            dtype=self.dtype,\n",
        "            device=self.device,\n",
        "        )\n",
        "        if output_length == 0:\n",
        "            r[0, 0] = x_[0, 0]\n",
        "\n",
        "        r_sum = torch.logsumexp(r_prev, 1)\n",
        "        log_phi = r_sum.unsqueeze(2).repeat(1, 1, snum)\n",
        "        if scoring_ids is not None:\n",
        "            for idx in range(n_bh):\n",
        "                pos = scoring_idmap[idx, last_ids[idx]]\n",
        "                if pos >= 0:\n",
        "                    log_phi[:, idx, pos] = r_prev[:, 1, idx]\n",
        "        else:\n",
        "            for idx in range(n_bh):\n",
        "                log_phi[:, idx, last_ids[idx]] = r_prev[:, 1, idx]\n",
        "\n",
        "        # decide start and end frames based on attention weights\n",
        "        if att_w is not None and self.margin > 0:\n",
        "            f_arg = torch.matmul(att_w, self.frame_ids)\n",
        "            f_min = max(int(f_arg.min().cpu()), f_min_prev)\n",
        "            f_max = max(int(f_arg.max().cpu()), f_max_prev)\n",
        "            start = min(f_max_prev, max(f_min - self.margin, output_length, 1))\n",
        "            end = min(f_max + self.margin, self.input_length)\n",
        "        else:\n",
        "            f_min = f_max = 0\n",
        "            start = max(output_length, 1)\n",
        "            end = self.input_length\n",
        "\n",
        "        # compute forward probabilities log(r_t^n(h)) and log(r_t^b(h))\n",
        "        for t in range(start, end):\n",
        "            rp = r[t - 1]\n",
        "            rr = torch.stack([rp[0], log_phi[t - 1], rp[0], rp[1]]).view(\n",
        "                2, 2, n_bh, snum\n",
        "            )\n",
        "            r[t] = torch.logsumexp(rr, 1) + x_[:, t]\n",
        "\n",
        "        # compute log prefix probabilities log(psi)\n",
        "        log_phi_x = torch.cat((log_phi[0].unsqueeze(0), log_phi[:-1]), dim=0) + x_[0]\n",
        "        if scoring_ids is not None:\n",
        "            log_psi = torch.full(\n",
        "                (n_bh, self.odim), self.logzero, dtype=self.dtype, device=self.device\n",
        "            )\n",
        "            log_psi_ = torch.logsumexp(\n",
        "                torch.cat((log_phi_x[start:end], r[start - 1, 0].unsqueeze(0)), dim=0),\n",
        "                dim=0,\n",
        "            )\n",
        "            for si in range(n_bh):\n",
        "                log_psi[si, scoring_ids[si]] = log_psi_[si]\n",
        "        else:\n",
        "            log_psi = torch.logsumexp(\n",
        "                torch.cat((log_phi_x[start:end], r[start - 1, 0].unsqueeze(0)), dim=0),\n",
        "                dim=0,\n",
        "            )\n",
        "\n",
        "        for si in range(n_bh):\n",
        "            log_psi[si, self.eos] = r_sum[self.end_frames[si // n_hyps], si]\n",
        "\n",
        "        # exclude blank probs\n",
        "        log_psi[:, self.blank] = self.logzero\n",
        "\n",
        "        return (log_psi - s_prev), (r, log_psi, f_min, f_max, scoring_idmap)\n",
        "\n",
        "    def index_select_state(self, state, best_ids):\n",
        "        \"\"\"Select CTC states according to best ids\n",
        "\n",
        "        :param state    : CTC state\n",
        "        :param best_ids : index numbers selected by beam pruning (B, W)\n",
        "        :return selected_state\n",
        "        \"\"\"\n",
        "        r, s, f_min, f_max, scoring_idmap = state\n",
        "        # convert ids to BHO space\n",
        "        n_bh = len(s)\n",
        "        n_hyps = n_bh // self.batch\n",
        "        vidx = (best_ids + (self.idx_b * (n_hyps * self.odim)).view(-1, 1)).view(-1)\n",
        "        # select hypothesis scores\n",
        "        s_new = torch.index_select(s.view(-1), 0, vidx)\n",
        "        s_new = s_new.view(-1, 1).repeat(1, self.odim).view(n_bh, self.odim)\n",
        "        # convert ids to BHS space (S: scoring_num)\n",
        "        if scoring_idmap is not None:\n",
        "            snum = self.scoring_num\n",
        "            hyp_idx = (best_ids // self.odim + (self.idx_b * n_hyps).view(-1, 1)).view(\n",
        "                -1\n",
        "            )\n",
        "            label_ids = torch.fmod(best_ids, self.odim).view(-1)\n",
        "            score_idx = scoring_idmap[hyp_idx, label_ids]\n",
        "            score_idx[score_idx == -1] = 0\n",
        "            vidx = score_idx + hyp_idx * snum\n",
        "        else:\n",
        "            snum = self.odim\n",
        "        # select forward probabilities\n",
        "        r_new = torch.index_select(r.view(-1, 2, n_bh * snum), 2, vidx).view(\n",
        "            -1, 2, n_bh\n",
        "        )\n",
        "        return r_new, s_new, f_min, f_max\n",
        "\n",
        "    def extend_prob(self, x):\n",
        "        \"\"\"Extend CTC prob.\n",
        "\n",
        "        :param torch.Tensor x: input label posterior sequences (B, T, O)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.x.shape[1] < x.shape[1]:  # self.x (2,T,B,O); x (B,T,O)\n",
        "            # Pad the rest of posteriors in the batch\n",
        "            # TODO(takaaki-hori): need a better way without for-loops\n",
        "            xlens = [x.size(1)]\n",
        "            for i, l in enumerate(xlens):\n",
        "                if l < self.input_length:\n",
        "                    x[i, l:, :] = self.logzero\n",
        "                    x[i, l:, self.blank] = 0\n",
        "            tmp_x = self.x\n",
        "            xn = x.transpose(0, 1)  # (B, T, O) -> (T, B, O)\n",
        "            xb = xn[:, :, self.blank].unsqueeze(2).expand(-1, -1, self.odim)\n",
        "            self.x = torch.stack([xn, xb])  # (2, T, B, O)\n",
        "            self.x[:, : tmp_x.shape[1], :, :] = tmp_x\n",
        "            self.input_length = x.size(1)\n",
        "            self.end_frames = torch.as_tensor(xlens) - 1\n",
        "\n",
        "    def extend_state(self, state):\n",
        "        \"\"\"Compute CTC prefix state.\n",
        "\n",
        "\n",
        "        :param state    : CTC state\n",
        "        :return ctc_state\n",
        "        \"\"\"\n",
        "\n",
        "        if state is None:\n",
        "            # nothing to do\n",
        "            return state\n",
        "        else:\n",
        "            r_prev, s_prev, f_min_prev, f_max_prev = state\n",
        "\n",
        "            r_prev_new = torch.full(\n",
        "                (self.input_length, 2),\n",
        "                self.logzero,\n",
        "                dtype=self.dtype,\n",
        "                device=self.device,\n",
        "            )\n",
        "            start = max(r_prev.shape[0], 1)\n",
        "            r_prev_new[0:start] = r_prev\n",
        "            for t in six.moves.range(start, self.input_length):\n",
        "                r_prev_new[t, 1] = r_prev_new[t - 1, 1] + self.x[0, t, :, self.blank]\n",
        "\n",
        "            return (r_prev_new, s_prev, f_min_prev, f_max_prev)\n",
        "\n",
        "\n",
        "class CTCPrefixScore(object):\n",
        "    \"\"\"Compute CTC label sequence scores\n",
        "\n",
        "    which is based on Algorithm 2 in WATANABE et al.\n",
        "    \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\"\n",
        "    but extended to efficiently compute the probablities of multiple labels\n",
        "    simultaneously\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x, blank, eos, xp):\n",
        "        self.xp = xp\n",
        "        self.logzero = -10000000000.0\n",
        "        self.blank = blank\n",
        "        self.eos = eos\n",
        "        self.input_length = len(x)\n",
        "        self.x = x\n",
        "\n",
        "    def initial_state(self):\n",
        "        \"\"\"Obtain an initial CTC state\n",
        "\n",
        "        :return: CTC state\n",
        "        \"\"\"\n",
        "        # initial CTC state is made of a frame x 2 tensor that corresponds to\n",
        "        # r_t^n(<sos>) and r_t^b(<sos>), where 0 and 1 of axis=1 represent\n",
        "        # superscripts n and b (non-blank and blank), respectively.\n",
        "        r = self.xp.full((self.input_length, 2), self.logzero, dtype=np.float32)\n",
        "        r[0, 1] = self.x[0, self.blank]\n",
        "        for i in six.moves.range(1, self.input_length):\n",
        "            r[i, 1] = r[i - 1, 1] + self.x[i, self.blank]\n",
        "        return r\n",
        "\n",
        "    def __call__(self, y, cs, r_prev):\n",
        "        \"\"\"Compute CTC prefix scores for next labels\n",
        "\n",
        "        :param y     : prefix label sequence\n",
        "        :param cs    : array of next labels\n",
        "        :param r_prev: previous CTC state\n",
        "        :return ctc_scores, ctc_states\n",
        "        \"\"\"\n",
        "        # initialize CTC states\n",
        "        output_length = len(y) - 1  # ignore sos\n",
        "        # new CTC states are prepared as a frame x (n or b) x n_labels tensor\n",
        "        # that corresponds to r_t^n(h) and r_t^b(h).\n",
        "        r = self.xp.ndarray((self.input_length, 2, len(cs)), dtype=np.float32)\n",
        "        xs = self.x[:, cs]\n",
        "        if output_length == 0:\n",
        "            r[0, 0] = xs[0]\n",
        "            r[0, 1] = self.logzero\n",
        "        else:\n",
        "            r[output_length - 1] = self.logzero\n",
        "\n",
        "        # prepare forward probabilities for the last label\n",
        "        r_sum = self.xp.logaddexp(\n",
        "            r_prev[:, 0], r_prev[:, 1]\n",
        "        )  # log(r_t^n(g) + r_t^b(g))\n",
        "        last = y[-1]\n",
        "        if output_length > 0 and last in cs:\n",
        "            log_phi = self.xp.ndarray((self.input_length, len(cs)), dtype=np.float32)\n",
        "            for i in six.moves.range(len(cs)):\n",
        "                log_phi[:, i] = r_sum if cs[i] != last else r_prev[:, 1]\n",
        "        else:\n",
        "            log_phi = r_sum\n",
        "\n",
        "        # compute forward probabilities log(r_t^n(h)), log(r_t^b(h)),\n",
        "        # and log prefix probabilities log(psi)\n",
        "        start = max(output_length, 1)\n",
        "        log_psi = r[start - 1, 0]\n",
        "        for t in six.moves.range(start, self.input_length):\n",
        "            r[t, 0] = self.xp.logaddexp(r[t - 1, 0], log_phi[t - 1]) + xs[t]\n",
        "            r[t, 1] = (\n",
        "                self.xp.logaddexp(r[t - 1, 0], r[t - 1, 1]) + self.x[t, self.blank]\n",
        "            )\n",
        "            log_psi = self.xp.logaddexp(log_psi, log_phi[t - 1] + xs[t])\n",
        "\n",
        "        # get P(...eos|X) that ends with the prefix itself\n",
        "        eos_pos = self.xp.where(cs == self.eos)[0]\n",
        "        if len(eos_pos) > 0:\n",
        "            log_psi[eos_pos] = r_sum[-1]  # log(r_T^n(g) + r_T^b(g))\n",
        "\n",
        "        # exclude blank probs\n",
        "        blank_pos = self.xp.where(cs == self.blank)[0]\n",
        "        if len(blank_pos) > 0:\n",
        "            log_psi[blank_pos] = self.logzero\n",
        "\n",
        "        # return the log prefix probability and CTC states, where the label axis\n",
        "        # of the CTC states is moved to the first axis to slice it easily\n",
        "        return log_psi, self.xp.rollaxis(r, 2)"
      ],
      "metadata": {
        "id": "pyvw0k0AQCTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTCPrefixScorer(BatchPartialScorerInterface):\n",
        "    \"\"\"Decoder interface wrapper for CTCPrefixScore.\"\"\"\n",
        "\n",
        "    def __init__(self, ctc: torch.nn.Module, eos: int):\n",
        "        \"\"\"Initialize class.\n",
        "\n",
        "        Args:\n",
        "            ctc (torch.nn.Module): The CTC implementation.\n",
        "                For example, :class:`espnet.nets.pytorch_backend.ctc.CTC`\n",
        "            eos (int): The end-of-sequence id.\n",
        "\n",
        "        \"\"\"\n",
        "        self.ctc = ctc\n",
        "        self.eos = eos\n",
        "        self.impl = None\n",
        "\n",
        "    def init_state(self, x: torch.Tensor):\n",
        "        \"\"\"Get an initial state for decoding.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The encoded feature tensor\n",
        "\n",
        "        Returns: initial state\n",
        "\n",
        "        \"\"\"\n",
        "        logp = self.ctc.log_softmax(x.unsqueeze(0)).detach().squeeze(0).cpu().numpy()\n",
        "        # TODO(karita): use CTCPrefixScoreTH\n",
        "        self.impl = CTCPrefixScore(logp, 0, self.eos, np)\n",
        "        return 0, self.impl.initial_state()\n",
        "\n",
        "    def select_state(self, state, i, new_id=None):\n",
        "        \"\"\"Select state with relative ids in the main beam search.\n",
        "\n",
        "        Args:\n",
        "            state: Decoder state for prefix tokens\n",
        "            i (int): Index to select a state in the main beam search\n",
        "            new_id (int): New label id to select a state if necessary\n",
        "\n",
        "        Returns:\n",
        "            state: pruned state\n",
        "\n",
        "        \"\"\"\n",
        "        if type(state) == tuple:\n",
        "            if len(state) == 2:  # for CTCPrefixScore\n",
        "                sc, st = state\n",
        "                return sc[i], st[i]\n",
        "            else:  # for CTCPrefixScoreTH (need new_id > 0)\n",
        "                r, log_psi, f_min, f_max, scoring_idmap = state\n",
        "                s = log_psi[i, new_id].expand(log_psi.size(1))\n",
        "                if scoring_idmap is not None:\n",
        "                    return r[:, :, i, scoring_idmap[i, new_id]], s, f_min, f_max\n",
        "                else:\n",
        "                    return r[:, :, i, new_id], s, f_min, f_max\n",
        "        return None if state is None else state[i]\n",
        "\n",
        "    def score_partial(self, y, ids, state, x):\n",
        "        \"\"\"Score new token.\n",
        "\n",
        "        Args:\n",
        "            y (torch.Tensor): 1D prefix token\n",
        "            next_tokens (torch.Tensor): torch.int64 next token to score\n",
        "            state: decoder state for prefix tokens\n",
        "            x (torch.Tensor): 2D encoder feature that generates ys\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, Any]:\n",
        "                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n",
        "                and next state for ys\n",
        "\n",
        "        \"\"\"\n",
        "        prev_score, state = state\n",
        "        presub_score, new_st = self.impl(y.cpu(), ids.cpu(), state)\n",
        "        tscore = torch.as_tensor(\n",
        "            presub_score - prev_score, device=x.device, dtype=x.dtype\n",
        "        )\n",
        "        return tscore, (presub_score, new_st)\n",
        "\n",
        "    def batch_init_state(self, x: torch.Tensor):\n",
        "        \"\"\"Get an initial state for decoding.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The encoded feature tensor\n",
        "\n",
        "        Returns: initial state\n",
        "\n",
        "        \"\"\"\n",
        "        logp = self.ctc.log_softmax(x.unsqueeze(0))  # assuming batch_size = 1\n",
        "        xlen = torch.tensor([logp.size(1)])\n",
        "        self.impl = CTCPrefixScoreTH(logp, xlen, 0, self.eos)\n",
        "        return None\n",
        "\n",
        "    def batch_score_partial(self, y, ids, state, x):\n",
        "        \"\"\"Score new token.\n",
        "\n",
        "        Args:\n",
        "            y (torch.Tensor): 1D prefix token\n",
        "            ids (torch.Tensor): torch.int64 next token to score\n",
        "            state: decoder state for prefix tokens\n",
        "            x (torch.Tensor): 2D encoder feature that generates ys\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, Any]:\n",
        "                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n",
        "                and next state for ys\n",
        "\n",
        "        \"\"\"\n",
        "        batch_state = (\n",
        "            (\n",
        "                torch.stack([s[0] for s in state], dim=2),\n",
        "                torch.stack([s[1] for s in state]),\n",
        "                state[0][2],\n",
        "                state[0][3],\n",
        "            )\n",
        "            if state[0] is not None\n",
        "            else None\n",
        "        )\n",
        "        return self.impl(y, batch_state, ids)\n",
        "\n",
        "    def extend_prob(self, x: torch.Tensor):\n",
        "        \"\"\"Extend probs for decoding.\n",
        "\n",
        "        This extension is for streaming decoding\n",
        "        as in Eq (14) in https://arxiv.org/abs/2006.14941\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The encoded feature tensor\n",
        "\n",
        "        \"\"\"\n",
        "        logp = self.ctc.log_softmax(x.unsqueeze(0))\n",
        "        self.impl.extend_prob(logp)\n",
        "\n",
        "    def extend_state(self, state):\n",
        "        \"\"\"Extend state for decoding.\n",
        "\n",
        "        This extension is for streaming decoding\n",
        "        as in Eq (14) in https://arxiv.org/abs/2006.14941\n",
        "\n",
        "        Args:\n",
        "            state: The states of hyps\n",
        "\n",
        "        Returns: exteded state\n",
        "\n",
        "        \"\"\"\n",
        "        new_state = []\n",
        "        for s in state:\n",
        "            new_state.append(self.impl.extend_state(s))\n",
        "\n",
        "        return new_state"
      ],
      "metadata": {
        "id": "V0LFm_JLOcD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LengthBonus"
      ],
      "metadata": {
        "id": "gZ980wJWQULi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LengthBonus(BatchScorerInterface):\n",
        "    \"\"\"Length bonus in beam search.\"\"\"\n",
        "\n",
        "    def __init__(self, n_vocab: int):\n",
        "        \"\"\"Initialize class.\n",
        "\n",
        "        Args:\n",
        "            n_vocab (int): The number of tokens in vocabulary for beam search\n",
        "\n",
        "        \"\"\"\n",
        "        self.n = n_vocab\n",
        "\n",
        "    def score(self, y, state, x):\n",
        "        \"\"\"Score new token.\n",
        "\n",
        "        Args:\n",
        "            y (torch.Tensor): 1D torch.int64 prefix tokens.\n",
        "            state: Scorer state for prefix tokens\n",
        "            x (torch.Tensor): 2D encoder feature that generates ys.\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, Any]: Tuple of\n",
        "                torch.float32 scores for next token (n_vocab)\n",
        "                and None\n",
        "\n",
        "        \"\"\"\n",
        "        return torch.tensor([1.0], device=x.device, dtype=x.dtype).expand(self.n), None\n",
        "\n",
        "    def batch_score(\n",
        "        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, List[Any]]:\n",
        "        \"\"\"Score new token batch.\n",
        "\n",
        "        Args:\n",
        "            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n",
        "            states (List[Any]): Scorer states for prefix tokens.\n",
        "            xs (torch.Tensor):\n",
        "                The encoder feature that generates ys (n_batch, xlen, n_feat).\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, List[Any]]: Tuple of\n",
        "                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n",
        "                and next state list for ys.\n",
        "\n",
        "        \"\"\"\n",
        "        return (\n",
        "            torch.tensor([1.0], device=xs.device, dtype=xs.dtype).expand(\n",
        "                ys.shape[0], self.n\n",
        "            ),\n",
        "            None,\n",
        "        )"
      ],
      "metadata": {
        "id": "qE_RPRxTQVED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BeamSearch"
      ],
      "metadata": {
        "id": "0nHmNDnWQxcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def end_detect(ended_hyps, i, M=3, D_end=np.log(1 * np.exp(-10))):\n",
        "    \"\"\"End detection.\n",
        "\n",
        "    described in Eq. (50) of S. Watanabe et al\n",
        "    \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\"\n",
        "\n",
        "    :param ended_hyps:\n",
        "    :param i:\n",
        "    :param M:\n",
        "    :param D_end:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if len(ended_hyps) == 0:\n",
        "        return False\n",
        "    count = 0\n",
        "    best_hyp = sorted(ended_hyps, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "    for m in range(M):\n",
        "        # get ended_hyps with their length is i - m\n",
        "        hyp_length = i - m\n",
        "        hyps_same_length = [x for x in ended_hyps if len(x[\"yseq\"]) == hyp_length]\n",
        "        if len(hyps_same_length) > 0:\n",
        "            best_hyp_same_length = sorted(\n",
        "                hyps_same_length, key=lambda x: x[\"score\"], reverse=True\n",
        "            )[0]\n",
        "            if best_hyp_same_length[\"score\"] - best_hyp[\"score\"] < D_end:\n",
        "                count += 1\n",
        "\n",
        "    if count == M:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "class Hypothesis(NamedTuple):\n",
        "    \"\"\"Hypothesis data type.\"\"\"\n",
        "\n",
        "    yseq: torch.Tensor\n",
        "    score: Union[float, torch.Tensor] = 0\n",
        "    scores: Dict[str, Union[float, torch.Tensor]] = dict()\n",
        "    states: Dict[str, Any] = dict()\n",
        "\n",
        "    def asdict(self) -> dict:\n",
        "        \"\"\"Convert data to JSON-friendly dict.\"\"\"\n",
        "        return self._replace(\n",
        "            yseq=self.yseq.tolist(),\n",
        "            score=float(self.score),\n",
        "            scores={k: float(v) for k, v in self.scores.items()},\n",
        "        )._asdict()\n",
        "\n",
        "\n",
        "class BeamSearch(torch.nn.Module):\n",
        "    \"\"\"Beam search implementation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        scorers: Dict[str, ScorerInterface],\n",
        "        weights: Dict[str, float],\n",
        "        beam_size: int,\n",
        "        vocab_size: int,\n",
        "        sos: int,\n",
        "        eos: int,\n",
        "        token_list: List[str] = None,\n",
        "        pre_beam_ratio: float = 1.5,\n",
        "        pre_beam_score_key: str = None,\n",
        "    ):\n",
        "        \"\"\"Initialize beam search.\n",
        "\n",
        "        Args:\n",
        "            scorers (dict[str, ScorerInterface]): Dict of decoder modules\n",
        "                e.g., Decoder, CTCPrefixScorer, LM\n",
        "                The scorer will be ignored if it is `None`\n",
        "            weights (dict[str, float]): Dict of weights for each scorers\n",
        "                The scorer will be ignored if its weight is 0\n",
        "            beam_size (int): The number of hypotheses kept during search\n",
        "            vocab_size (int): The number of vocabulary\n",
        "            sos (int): Start of sequence id\n",
        "            eos (int): End of sequence id\n",
        "            token_list (list[str]): List of tokens for debug log\n",
        "            pre_beam_score_key (str): key of scores to perform pre-beam search\n",
        "            pre_beam_ratio (float): beam size in the pre-beam search\n",
        "                will be `int(pre_beam_ratio * beam_size)`\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # set scorers\n",
        "        self.weights = weights\n",
        "        self.scorers = dict()\n",
        "        self.full_scorers = dict()\n",
        "        self.part_scorers = dict()\n",
        "        # this module dict is required for recursive cast\n",
        "        # `self.to(device, dtype)` in `recog.py`\n",
        "        self.nn_dict = torch.nn.ModuleDict()\n",
        "        for k, v in scorers.items():\n",
        "            w = weights.get(k, 0)\n",
        "            if w == 0 or v is None:\n",
        "                continue\n",
        "            assert isinstance(\n",
        "                v, ScorerInterface\n",
        "            ), f\"{k} ({type(v)}) does not implement ScorerInterface\"\n",
        "            self.scorers[k] = v\n",
        "            if isinstance(v, PartialScorerInterface):\n",
        "                self.part_scorers[k] = v\n",
        "            else:\n",
        "                self.full_scorers[k] = v\n",
        "            if isinstance(v, torch.nn.Module):\n",
        "                self.nn_dict[k] = v\n",
        "\n",
        "        # set configurations\n",
        "        self.sos = sos\n",
        "        self.eos = eos\n",
        "        self.token_list = token_list\n",
        "        self.pre_beam_size = int(pre_beam_ratio * beam_size)\n",
        "        self.beam_size = beam_size\n",
        "        self.n_vocab = vocab_size\n",
        "        if (\n",
        "            pre_beam_score_key is not None\n",
        "            and pre_beam_score_key != \"full\"\n",
        "            and pre_beam_score_key not in self.full_scorers\n",
        "        ):\n",
        "            raise KeyError(f\"{pre_beam_score_key} is not found in {self.full_scorers}\")\n",
        "        self.pre_beam_score_key = pre_beam_score_key\n",
        "        self.do_pre_beam = (\n",
        "            self.pre_beam_score_key is not None\n",
        "            and self.pre_beam_size < self.n_vocab\n",
        "            and len(self.part_scorers) > 0\n",
        "        )\n",
        "\n",
        "    def init_hyp(self, x: torch.Tensor) -> List[Hypothesis]:\n",
        "        \"\"\"Get an initial hypothesis data.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The encoder output feature\n",
        "\n",
        "        Returns:\n",
        "            Hypothesis: The initial hypothesis.\n",
        "\n",
        "        \"\"\"\n",
        "        init_states = dict()\n",
        "        init_scores = dict()\n",
        "        for k, d in self.scorers.items():\n",
        "            init_states[k] = d.init_state(x)\n",
        "            init_scores[k] = 0.0\n",
        "        return [\n",
        "            Hypothesis(\n",
        "                score=0.0,\n",
        "                scores=init_scores,\n",
        "                states=init_states,\n",
        "                yseq=torch.tensor([self.sos], device=x.device),\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    @staticmethod\n",
        "    def append_token(xs: torch.Tensor, x: int) -> torch.Tensor:\n",
        "        \"\"\"Append new token to prefix tokens.\n",
        "\n",
        "        Args:\n",
        "            xs (torch.Tensor): The prefix token\n",
        "            x (int): The new token to append\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device\n",
        "\n",
        "        \"\"\"\n",
        "        x = torch.tensor([x], dtype=xs.dtype, device=xs.device)\n",
        "        return torch.cat((xs, x))\n",
        "\n",
        "    def score_full(\n",
        "        self, hyp: Hypothesis, x: torch.Tensor\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n",
        "        \"\"\"Score new hypothesis by `self.full_scorers`.\n",
        "\n",
        "        Args:\n",
        "            hyp (Hypothesis): Hypothesis with prefix tokens to score\n",
        "            x (torch.Tensor): Corresponding input feature\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n",
        "                score dict of `hyp` that has string keys of `self.full_scorers`\n",
        "                and tensor score values of shape: `(self.n_vocab,)`,\n",
        "                and state dict that has string keys\n",
        "                and state values of `self.full_scorers`\n",
        "\n",
        "        \"\"\"\n",
        "        scores = dict()\n",
        "        states = dict()\n",
        "        for k, d in self.full_scorers.items():\n",
        "            scores[k], states[k] = d.score(hyp.yseq, hyp.states[k], x)\n",
        "        return scores, states\n",
        "\n",
        "    def score_partial(\n",
        "        self, hyp: Hypothesis, ids: torch.Tensor, x: torch.Tensor\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n",
        "        \"\"\"Score new hypothesis by `self.part_scorers`.\n",
        "\n",
        "        Args:\n",
        "            hyp (Hypothesis): Hypothesis with prefix tokens to score\n",
        "            ids (torch.Tensor): 1D tensor of new partial tokens to score\n",
        "            x (torch.Tensor): Corresponding input feature\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n",
        "                score dict of `hyp` that has string keys of `self.part_scorers`\n",
        "                and tensor score values of shape: `(len(ids),)`,\n",
        "                and state dict that has string keys\n",
        "                and state values of `self.part_scorers`\n",
        "\n",
        "        \"\"\"\n",
        "        scores = dict()\n",
        "        states = dict()\n",
        "        for k, d in self.part_scorers.items():\n",
        "            scores[k], states[k] = d.score_partial(hyp.yseq, ids, hyp.states[k], x)\n",
        "        return scores, states\n",
        "\n",
        "    def beam(\n",
        "        self, weighted_scores: torch.Tensor, ids: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Compute topk full token ids and partial token ids.\n",
        "\n",
        "        Args:\n",
        "            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n",
        "            Its shape is `(self.n_vocab,)`.\n",
        "            ids (torch.Tensor): The partial token ids to compute topk\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]:\n",
        "                The topk full token ids and partial token ids.\n",
        "                Their shapes are `(self.beam_size,)`\n",
        "\n",
        "        \"\"\"\n",
        "        # no pre beam performed\n",
        "        if weighted_scores.size(0) == ids.size(0):\n",
        "            top_ids = weighted_scores.topk(self.beam_size)[1]\n",
        "            return top_ids, top_ids\n",
        "\n",
        "        # mask pruned in pre-beam not to select in topk\n",
        "        tmp = weighted_scores[ids]\n",
        "        weighted_scores[:] = -float(\"inf\")\n",
        "        weighted_scores[ids] = tmp\n",
        "        top_ids = weighted_scores.topk(self.beam_size)[1]\n",
        "        local_ids = weighted_scores[ids].topk(self.beam_size)[1]\n",
        "        return top_ids, local_ids\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_scores(\n",
        "        prev_scores: Dict[str, float],\n",
        "        next_full_scores: Dict[str, torch.Tensor],\n",
        "        full_idx: int,\n",
        "        next_part_scores: Dict[str, torch.Tensor],\n",
        "        part_idx: int,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Merge scores for new hypothesis.\n",
        "\n",
        "        Args:\n",
        "            prev_scores (Dict[str, float]):\n",
        "                The previous hypothesis scores by `self.scorers`\n",
        "            next_full_scores (Dict[str, torch.Tensor]): scores by `self.full_scorers`\n",
        "            full_idx (int): The next token id for `next_full_scores`\n",
        "            next_part_scores (Dict[str, torch.Tensor]):\n",
        "                scores of partial tokens by `self.part_scorers`\n",
        "            part_idx (int): The new token id for `next_part_scores`\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, torch.Tensor]: The new score dict.\n",
        "                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n",
        "                Its values are scalar tensors by the scorers.\n",
        "\n",
        "        \"\"\"\n",
        "        new_scores = dict()\n",
        "        for k, v in next_full_scores.items():\n",
        "            new_scores[k] = prev_scores[k] + v[full_idx]\n",
        "        for k, v in next_part_scores.items():\n",
        "            new_scores[k] = prev_scores[k] + v[part_idx]\n",
        "        return new_scores\n",
        "\n",
        "    def merge_states(self, states: Any, part_states: Any, part_idx: int) -> Any:\n",
        "        \"\"\"Merge states for new hypothesis.\n",
        "\n",
        "        Args:\n",
        "            states: states of `self.full_scorers`\n",
        "            part_states: states of `self.part_scorers`\n",
        "            part_idx (int): The new token id for `part_scores`\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, torch.Tensor]: The new score dict.\n",
        "                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n",
        "                Its values are states of the scorers.\n",
        "\n",
        "        \"\"\"\n",
        "        new_states = dict()\n",
        "        for k, v in states.items():\n",
        "            new_states[k] = v\n",
        "        for k, d in self.part_scorers.items():\n",
        "            new_states[k] = d.select_state(part_states[k], part_idx)\n",
        "        return new_states\n",
        "\n",
        "    def search(\n",
        "        self, running_hyps: List[Hypothesis], x: torch.Tensor\n",
        "    ) -> List[Hypothesis]:\n",
        "        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n",
        "\n",
        "        Args:\n",
        "            running_hyps (List[Hypothesis]): Running hypotheses on beam\n",
        "            x (torch.Tensor): Encoded speech feature (T, D)\n",
        "\n",
        "        Returns:\n",
        "            List[Hypotheses]: Best sorted hypotheses\n",
        "\n",
        "        \"\"\"\n",
        "        best_hyps = []\n",
        "        part_ids = torch.arange(self.n_vocab, device=x.device)  # no pre-beam\n",
        "        for hyp in running_hyps:\n",
        "            # scoring\n",
        "            weighted_scores = torch.zeros(self.n_vocab, dtype=x.dtype, device=x.device)\n",
        "            scores, states = self.score_full(hyp, x)\n",
        "            for k in self.full_scorers:\n",
        "                weighted_scores += self.weights[k] * scores[k]\n",
        "            # partial scoring\n",
        "            if self.do_pre_beam:\n",
        "                pre_beam_scores = (\n",
        "                    weighted_scores\n",
        "                    if self.pre_beam_score_key == \"full\"\n",
        "                    else scores[self.pre_beam_score_key]\n",
        "                )\n",
        "                part_ids = torch.topk(pre_beam_scores, self.pre_beam_size)[1]\n",
        "            part_scores, part_states = self.score_partial(hyp, part_ids, x)\n",
        "            for k in self.part_scorers:\n",
        "                weighted_scores[part_ids] += self.weights[k] * part_scores[k]\n",
        "            # add previous hyp score\n",
        "            weighted_scores += hyp.score\n",
        "\n",
        "            # update hyps\n",
        "            for j, part_j in zip(*self.beam(weighted_scores, part_ids)):\n",
        "                # will be (2 x beam at most)\n",
        "                best_hyps.append(\n",
        "                    Hypothesis(\n",
        "                        score=weighted_scores[j],\n",
        "                        yseq=self.append_token(hyp.yseq, j),\n",
        "                        scores=self.merge_scores(\n",
        "                            hyp.scores, scores, j, part_scores, part_j\n",
        "                        ),\n",
        "                        states=self.merge_states(states, part_states, part_j),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # sort and prune 2 x beam -> beam\n",
        "            best_hyps = sorted(best_hyps, key=lambda x: x.score, reverse=True)[\n",
        "                : min(len(best_hyps), self.beam_size)\n",
        "            ]\n",
        "        return best_hyps\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0\n",
        "    ) -> List[Hypothesis]:\n",
        "        \"\"\"Perform beam search.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Encoded speech feature (T, D)\n",
        "            maxlenratio (float): Input length ratio to obtain max output length.\n",
        "                If maxlenratio=0.0 (default), it uses a end-detect function\n",
        "                to automatically find maximum hypothesis lengths\n",
        "                If maxlenratio<0.0, its absolute value is interpreted\n",
        "                as a constant max output length.\n",
        "            minlenratio (float): Input length ratio to obtain min output length.\n",
        "\n",
        "        Returns:\n",
        "            list[Hypothesis]: N-best decoding results\n",
        "\n",
        "        \"\"\"\n",
        "        # set length bounds\n",
        "        if maxlenratio == 0:\n",
        "            maxlen = x.shape[0]\n",
        "        elif maxlenratio < 0:\n",
        "            maxlen = -1 * int(maxlenratio)\n",
        "        else:\n",
        "            maxlen = max(1, int(maxlenratio * x.size(0)))\n",
        "        minlen = int(minlenratio * x.size(0))\n",
        "        logging.debug(\"decoder input length: \" + str(x.shape[0]))\n",
        "        logging.debug(\"max output length: \" + str(maxlen))\n",
        "        logging.debug(\"min output length: \" + str(minlen))\n",
        "\n",
        "        # main loop of prefix search\n",
        "        running_hyps = self.init_hyp(x)\n",
        "        ended_hyps = []\n",
        "        for i in range(maxlen):\n",
        "            logging.debug(\"position \" + str(i))\n",
        "            best = self.search(running_hyps, x)\n",
        "            # post process of one iteration\n",
        "            running_hyps = self.post_process(i, maxlen, maxlenratio, best, ended_hyps)\n",
        "            # end detection\n",
        "            if maxlenratio == 0.0 and end_detect([h.asdict() for h in ended_hyps], i):\n",
        "                logging.debug(f\"end detected at {i}\")\n",
        "                break\n",
        "            if len(running_hyps) == 0:\n",
        "                logging.debug(\"no hypothesis. Finish decoding.\")\n",
        "                break\n",
        "            else:\n",
        "                logging.debug(f\"remained hypotheses: {len(running_hyps)}\")\n",
        "\n",
        "        nbest_hyps = sorted(ended_hyps, key=lambda x: x.score, reverse=True)\n",
        "        # check the number of hypotheses reaching to eos\n",
        "        if len(nbest_hyps) == 0:\n",
        "            logging.warning(\n",
        "                \"there is no N-best results, perform recognition \"\n",
        "                \"again with smaller minlenratio.\"\n",
        "            )\n",
        "            return (\n",
        "                []\n",
        "                if minlenratio < 0.1\n",
        "                else self.forward(x, maxlenratio, max(0.0, minlenratio - 0.1))\n",
        "            )\n",
        "\n",
        "        # report the best result\n",
        "        best = nbest_hyps[0]\n",
        "        for k, v in best.scores.items():\n",
        "            logging.debug(\n",
        "                f\"{v:6.2f} * {self.weights[k]:3} = {v * self.weights[k]:6.2f} for {k}\"\n",
        "            )\n",
        "        logging.debug(f\"total log probability: {best.score:.2f}\")\n",
        "        logging.debug(f\"normalized log probability: {best.score / len(best.yseq):.2f}\")\n",
        "        logging.debug(f\"total number of ended hypotheses: {len(nbest_hyps)}\")\n",
        "        if self.token_list is not None:\n",
        "            logging.debug(\n",
        "                \"best hypo: \"\n",
        "                + \"\".join([self.token_list[x] for x in best.yseq[1:-1]])\n",
        "                + \"\\n\"\n",
        "            )\n",
        "        return nbest_hyps\n",
        "\n",
        "    def post_process(\n",
        "        self,\n",
        "        i: int,\n",
        "        maxlen: int,\n",
        "        maxlenratio: float,\n",
        "        running_hyps: List[Hypothesis],\n",
        "        ended_hyps: List[Hypothesis],\n",
        "    ) -> List[Hypothesis]:\n",
        "        \"\"\"Perform post-processing of beam search iterations.\n",
        "\n",
        "        Args:\n",
        "            i (int): The length of hypothesis tokens.\n",
        "            maxlen (int): The maximum length of tokens in beam search.\n",
        "            maxlenratio (int): The maximum length ratio in beam search.\n",
        "            running_hyps (List[Hypothesis]): The running hypotheses in beam search.\n",
        "            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n",
        "\n",
        "        Returns:\n",
        "            List[Hypothesis]: The new running hypotheses.\n",
        "\n",
        "        \"\"\"\n",
        "        logging.debug(f\"the number of running hypotheses: {len(running_hyps)}\")\n",
        "        if self.token_list is not None:\n",
        "            logging.debug(\n",
        "                \"best hypo: \"\n",
        "                + \"\".join([self.token_list[x] for x in running_hyps[0].yseq[1:]])\n",
        "            )\n",
        "        # add eos in the final loop to avoid that there are no ended hyps\n",
        "        if i == maxlen - 1:\n",
        "            logging.debug(\"adding <eos> in the last position in the loop\")\n",
        "            running_hyps = [\n",
        "                h._replace(yseq=self.append_token(h.yseq, self.eos))\n",
        "                for h in running_hyps\n",
        "            ]\n",
        "\n",
        "        # add ended hypotheses to a final list, and removed them from current hypotheses\n",
        "        # (this will be a problem, number of hyps < beam)\n",
        "        remained_hyps = []\n",
        "        for hyp in running_hyps:\n",
        "            if hyp.yseq[-1] == self.eos:\n",
        "                # e.g., Word LM needs to add final <eos> score\n",
        "                for k, d in chain(self.full_scorers.items(), self.part_scorers.items()):\n",
        "                    s = d.final_score(hyp.states[k])\n",
        "                    hyp.scores[k] += s\n",
        "                    hyp = hyp._replace(score=hyp.score + self.weights[k] * s)\n",
        "                ended_hyps.append(hyp)\n",
        "            else:\n",
        "                remained_hyps.append(hyp)\n",
        "        return remained_hyps"
      ],
      "metadata": {
        "id": "OJ9U-Ge2QyfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BatchBeamSearch"
      ],
      "metadata": {
        "id": "9KU4cv09Qrkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchHypothesis(NamedTuple):\n",
        "    \"\"\"Batchfied/Vectorized hypothesis data type.\"\"\"\n",
        "\n",
        "    yseq: torch.Tensor = torch.tensor([])  # (batch, maxlen)\n",
        "    score: torch.Tensor = torch.tensor([])  # (batch,)\n",
        "    length: torch.Tensor = torch.tensor([])  # (batch,)\n",
        "    scores: Dict[str, torch.Tensor] = dict()  # values: (batch,)\n",
        "    states: Dict[str, Dict] = dict()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return a batch size.\"\"\"\n",
        "        return len(self.length)\n",
        "\n",
        "\n",
        "class BatchBeamSearch(BeamSearch):\n",
        "    \"\"\"Batch beam search implementation.\"\"\"\n",
        "\n",
        "    def batchfy(self, hyps: List[Hypothesis]) -> BatchHypothesis:\n",
        "        \"\"\"Convert list to batch.\"\"\"\n",
        "        if len(hyps) == 0:\n",
        "            return BatchHypothesis()\n",
        "        yseq = pad_sequence(\n",
        "            [h.yseq for h in hyps], batch_first=True, padding_value=self.eos\n",
        "        )\n",
        "        return BatchHypothesis(\n",
        "            yseq=yseq,\n",
        "            length=torch.tensor(\n",
        "                [len(h.yseq) for h in hyps], dtype=torch.int64, device=yseq.device\n",
        "            ),\n",
        "            score=torch.tensor([h.score for h in hyps]).to(yseq.device),\n",
        "            scores={\n",
        "                k: torch.tensor([h.scores[k] for h in hyps], device=yseq.device)\n",
        "                for k in self.scorers\n",
        "            },\n",
        "            states={k: [h.states[k] for h in hyps] for k in self.scorers},\n",
        "        )\n",
        "\n",
        "    def _batch_select(self, hyps: BatchHypothesis, ids: List[int]) -> BatchHypothesis:\n",
        "        return BatchHypothesis(\n",
        "            yseq=hyps.yseq[ids],\n",
        "            score=hyps.score[ids],\n",
        "            length=hyps.length[ids],\n",
        "            scores={k: v[ids] for k, v in hyps.scores.items()},\n",
        "            states={\n",
        "                k: [self.scorers[k].select_state(v, i) for i in ids]\n",
        "                for k, v in hyps.states.items()\n",
        "            },\n",
        "        )\n",
        "\n",
        "    def _select(self, hyps: BatchHypothesis, i: int) -> Hypothesis:\n",
        "        return Hypothesis(\n",
        "            yseq=hyps.yseq[i, : hyps.length[i]],\n",
        "            score=hyps.score[i],\n",
        "            scores={k: v[i] for k, v in hyps.scores.items()},\n",
        "            states={\n",
        "                k: self.scorers[k].select_state(v, i) for k, v in hyps.states.items()\n",
        "            },\n",
        "        )\n",
        "\n",
        "    def unbatchfy(self, batch_hyps: BatchHypothesis) -> List[Hypothesis]:\n",
        "        \"\"\"Revert batch to list.\"\"\"\n",
        "        return [\n",
        "            Hypothesis(\n",
        "                yseq=batch_hyps.yseq[i][: batch_hyps.length[i]],\n",
        "                score=batch_hyps.score[i],\n",
        "                scores={k: batch_hyps.scores[k][i] for k in self.scorers},\n",
        "                states={\n",
        "                    k: v.select_state(batch_hyps.states[k], i)\n",
        "                    for k, v in self.scorers.items()\n",
        "                },\n",
        "            )\n",
        "            for i in range(len(batch_hyps.length))\n",
        "        ]\n",
        "\n",
        "    def batch_beam(\n",
        "        self, weighted_scores: torch.Tensor, ids: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Batch-compute topk full token ids and partial token ids.\n",
        "\n",
        "        Args:\n",
        "            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n",
        "                Its shape is `(n_beam, self.vocab_size)`.\n",
        "            ids (torch.Tensor): The partial token ids to compute topk.\n",
        "                Its shape is `(n_beam, self.pre_beam_size)`.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "                The topk full (prev_hyp, new_token) ids\n",
        "                and partial (prev_hyp, new_token) ids.\n",
        "                Their shapes are all `(self.beam_size,)`\n",
        "\n",
        "        \"\"\"\n",
        "        top_ids = weighted_scores.view(-1).topk(self.beam_size)[1]\n",
        "        # Because of the flatten above, `top_ids` is organized as:\n",
        "        # [hyp1 * V + token1, hyp2 * V + token2, ..., hypK * V + tokenK],\n",
        "        # where V is `self.n_vocab` and K is `self.beam_size`\n",
        "        prev_hyp_ids = torch.div(top_ids, self.n_vocab, rounding_mode=\"trunc\")\n",
        "        new_token_ids = top_ids % self.n_vocab\n",
        "        return prev_hyp_ids, new_token_ids, prev_hyp_ids, new_token_ids\n",
        "\n",
        "    def init_hyp(self, x: torch.Tensor) -> BatchHypothesis:\n",
        "        \"\"\"Get an initial hypothesis data.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The encoder output feature\n",
        "\n",
        "        Returns:\n",
        "            Hypothesis: The initial hypothesis.\n",
        "\n",
        "        \"\"\"\n",
        "        init_states = dict()\n",
        "        init_scores = dict()\n",
        "        for k, d in self.scorers.items():\n",
        "            init_states[k] = d.batch_init_state(x)\n",
        "            init_scores[k] = 0.0\n",
        "        return self.batchfy(\n",
        "            [\n",
        "                Hypothesis(\n",
        "                    score=0.0,\n",
        "                    scores=init_scores,\n",
        "                    states=init_states,\n",
        "                    yseq=torch.tensor([self.sos], device=x.device),\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def score_full(\n",
        "        self, hyp: BatchHypothesis, x: torch.Tensor\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n",
        "        \"\"\"Score new hypothesis by `self.full_scorers`.\n",
        "\n",
        "        Args:\n",
        "            hyp (Hypothesis): Hypothesis with prefix tokens to score\n",
        "            x (torch.Tensor): Corresponding input feature\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n",
        "                score dict of `hyp` that has string keys of `self.full_scorers`\n",
        "                and tensor score values of shape: `(self.n_vocab,)`,\n",
        "                and state dict that has string keys\n",
        "                and state values of `self.full_scorers`\n",
        "\n",
        "        \"\"\"\n",
        "        scores = dict()\n",
        "        states = dict()\n",
        "        for k, d in self.full_scorers.items():\n",
        "            scores[k], states[k] = d.batch_score(hyp.yseq, hyp.states[k], x)\n",
        "        return scores, states\n",
        "\n",
        "    def score_partial(\n",
        "        self, hyp: BatchHypothesis, ids: torch.Tensor, x: torch.Tensor\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n",
        "        \"\"\"Score new hypothesis by `self.full_scorers`.\n",
        "\n",
        "        Args:\n",
        "            hyp (Hypothesis): Hypothesis with prefix tokens to score\n",
        "            ids (torch.Tensor): 2D tensor of new partial tokens to score\n",
        "            x (torch.Tensor): Corresponding input feature\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n",
        "                score dict of `hyp` that has string keys of `self.full_scorers`\n",
        "                and tensor score values of shape: `(self.n_vocab,)`,\n",
        "                and state dict that has string keys\n",
        "                and state values of `self.full_scorers`\n",
        "\n",
        "        \"\"\"\n",
        "        scores = dict()\n",
        "        states = dict()\n",
        "        for k, d in self.part_scorers.items():\n",
        "            scores[k], states[k] = d.batch_score_partial(\n",
        "                hyp.yseq, ids, hyp.states[k], x\n",
        "            )\n",
        "        return scores, states\n",
        "\n",
        "    def merge_states(self, states: Any, part_states: Any, part_idx: int) -> Any:\n",
        "        \"\"\"Merge states for new hypothesis.\n",
        "\n",
        "        Args:\n",
        "            states: states of `self.full_scorers`\n",
        "            part_states: states of `self.part_scorers`\n",
        "            part_idx (int): The new token id for `part_scores`\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, torch.Tensor]: The new score dict.\n",
        "                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n",
        "                Its values are states of the scorers.\n",
        "\n",
        "        \"\"\"\n",
        "        new_states = dict()\n",
        "        for k, v in states.items():\n",
        "            new_states[k] = v\n",
        "        for k, v in part_states.items():\n",
        "            new_states[k] = v\n",
        "        return new_states\n",
        "\n",
        "    def search(self, running_hyps: BatchHypothesis, x: torch.Tensor) -> BatchHypothesis:\n",
        "        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n",
        "\n",
        "        Args:\n",
        "            running_hyps (BatchHypothesis): Running hypotheses on beam\n",
        "            x (torch.Tensor): Encoded speech feature (T, D)\n",
        "\n",
        "        Returns:\n",
        "            BatchHypothesis: Best sorted hypotheses\n",
        "\n",
        "        \"\"\"\n",
        "        n_batch = len(running_hyps)\n",
        "        part_ids = None  # no pre-beam\n",
        "        # batch scoring\n",
        "        weighted_scores = torch.zeros(\n",
        "            n_batch, self.n_vocab, dtype=x.dtype, device=x.device\n",
        "        )\n",
        "        scores, states = self.score_full(running_hyps, x.expand(n_batch, *x.shape))\n",
        "        for k in self.full_scorers:\n",
        "            weighted_scores += self.weights[k] * scores[k]\n",
        "        # partial scoring\n",
        "        if self.do_pre_beam:\n",
        "            pre_beam_scores = (\n",
        "                weighted_scores\n",
        "                if self.pre_beam_score_key == \"full\"\n",
        "                else scores[self.pre_beam_score_key]\n",
        "            )\n",
        "            part_ids = torch.topk(pre_beam_scores, self.pre_beam_size, dim=-1)[1]\n",
        "        # NOTE(takaaki-hori): Unlike BeamSearch, we assume that score_partial returns\n",
        "        # full-size score matrices, which has non-zero scores for part_ids and zeros\n",
        "        # for others.\n",
        "        part_scores, part_states = self.score_partial(running_hyps, part_ids, x)\n",
        "        for k in self.part_scorers:\n",
        "            weighted_scores += self.weights[k] * part_scores[k]\n",
        "        # add previous hyp scores\n",
        "        weighted_scores += running_hyps.score.to(\n",
        "            dtype=x.dtype, device=x.device\n",
        "        ).unsqueeze(1)\n",
        "\n",
        "        # TODO(karita): do not use list. use batch instead\n",
        "        # see also https://github.com/espnet/espnet/pull/1402#discussion_r354561029\n",
        "        # update hyps\n",
        "        best_hyps = []\n",
        "        prev_hyps = self.unbatchfy(running_hyps)\n",
        "        for (\n",
        "            full_prev_hyp_id,\n",
        "            full_new_token_id,\n",
        "            part_prev_hyp_id,\n",
        "            part_new_token_id,\n",
        "        ) in zip(*self.batch_beam(weighted_scores, part_ids)):\n",
        "            prev_hyp = prev_hyps[full_prev_hyp_id]\n",
        "            best_hyps.append(\n",
        "                Hypothesis(\n",
        "                    score=weighted_scores[full_prev_hyp_id, full_new_token_id],\n",
        "                    yseq=self.append_token(prev_hyp.yseq, full_new_token_id),\n",
        "                    scores=self.merge_scores(\n",
        "                        prev_hyp.scores,\n",
        "                        {k: v[full_prev_hyp_id] for k, v in scores.items()},\n",
        "                        full_new_token_id,\n",
        "                        {k: v[part_prev_hyp_id] for k, v in part_scores.items()},\n",
        "                        part_new_token_id,\n",
        "                    ),\n",
        "                    states=self.merge_states(\n",
        "                        {\n",
        "                            k: self.full_scorers[k].select_state(v, full_prev_hyp_id)\n",
        "                            for k, v in states.items()\n",
        "                        },\n",
        "                        {\n",
        "                            k: self.part_scorers[k].select_state(\n",
        "                                v, part_prev_hyp_id, part_new_token_id\n",
        "                            )\n",
        "                            for k, v in part_states.items()\n",
        "                        },\n",
        "                        part_new_token_id,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "        return self.batchfy(best_hyps)\n",
        "\n",
        "    def post_process(\n",
        "        self,\n",
        "        i: int,\n",
        "        maxlen: int,\n",
        "        maxlenratio: float,\n",
        "        running_hyps: BatchHypothesis,\n",
        "        ended_hyps: List[Hypothesis],\n",
        "    ) -> BatchHypothesis:\n",
        "        \"\"\"Perform post-processing of beam search iterations.\n",
        "\n",
        "        Args:\n",
        "            i (int): The length of hypothesis tokens.\n",
        "            maxlen (int): The maximum length of tokens in beam search.\n",
        "            maxlenratio (int): The maximum length ratio in beam search.\n",
        "            running_hyps (BatchHypothesis): The running hypotheses in beam search.\n",
        "            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n",
        "\n",
        "        Returns:\n",
        "            BatchHypothesis: The new running hypotheses.\n",
        "\n",
        "        \"\"\"\n",
        "        n_batch = running_hyps.yseq.shape[0]\n",
        "        logging.debug(f\"the number of running hypothes: {n_batch}\")\n",
        "        if self.token_list is not None:\n",
        "            logging.debug(\n",
        "                \"best hypo: \"\n",
        "                + \"\".join(\n",
        "                    [\n",
        "                        self.token_list[x]\n",
        "                        for x in running_hyps.yseq[0, 1 : running_hyps.length[0]]\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "        # add eos in the final loop to avoid that there are no ended hyps\n",
        "        if i == maxlen - 1:\n",
        "            logging.debug(\"adding <eos> in the last position in the loop\")\n",
        "            yseq_eos = torch.cat(\n",
        "                (\n",
        "                    running_hyps.yseq,\n",
        "                    torch.full(\n",
        "                        (n_batch, 1),\n",
        "                        self.eos,\n",
        "                        device=running_hyps.yseq.device,\n",
        "                        dtype=torch.int64,\n",
        "                    ),\n",
        "                ),\n",
        "                1,\n",
        "            )\n",
        "            running_hyps.yseq.resize_as_(yseq_eos)\n",
        "            running_hyps.yseq[:] = yseq_eos\n",
        "            running_hyps.length[:] = yseq_eos.shape[1]\n",
        "\n",
        "        # add ended hypotheses to a final list, and removed them from current hypotheses\n",
        "        # (this will be a probmlem, number of hyps < beam)\n",
        "        is_eos = (\n",
        "            running_hyps.yseq[torch.arange(n_batch), running_hyps.length - 1]\n",
        "            == self.eos\n",
        "        )\n",
        "        for b in torch.nonzero(is_eos, as_tuple=False).view(-1):\n",
        "            hyp = self._select(running_hyps, b)\n",
        "            ended_hyps.append(hyp)\n",
        "        remained_ids = torch.nonzero(is_eos == 0, as_tuple=False).view(-1)\n",
        "        return self._batch_select(running_hyps, remained_ids)"
      ],
      "metadata": {
        "id": "e4xkn6M3QsgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ensemble"
      ],
      "metadata": {
        "id": "kJJTggAkS5Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_checkpoints(last):\n",
        "    avg = None\n",
        "    for path in last:\n",
        "        states = torch.load(path, map_location=lambda storage, loc: storage)[\n",
        "            \"state_dict\"\n",
        "        ]\n",
        "        states = {k[6:]: v for k, v in states.items() if k.startswith(\"model.\")}\n",
        "        if avg is None:\n",
        "            avg = states\n",
        "        else:\n",
        "            for k in avg.keys():\n",
        "                avg[k] += states[k]\n",
        "    # average\n",
        "    for k in avg.keys():\n",
        "        if avg[k] is not None:\n",
        "            if avg[k].is_floating_point():\n",
        "                avg[k] /= len(last)\n",
        "            else:\n",
        "                avg[k] //= len(last)\n",
        "    return avg\n",
        "\n",
        "\n",
        "def ensemble(args):\n",
        "    last = [\n",
        "        os.path.join(args.exp_dir, args.exp_name, f\"epoch={n}.ckpt\")\n",
        "        for n in range(\n",
        "            args.trainer.max_epochs - 10,\n",
        "            args.trainer.max_epochs,\n",
        "        )\n",
        "    ]\n",
        "    model_path = os.path.join(\n",
        "        args.exp_dir, args.exp_name, f\"model_avg_10.pth\"\n",
        "    )\n",
        "    torch.save(average_checkpoints(last), model_path)\n",
        "    return model_path"
      ],
      "metadata": {
        "id": "HLnnKKLOS6NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AVSR multimodal(Audio-video)"
      ],
      "metadata": {
        "id": "TXA40fu0zMBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torchaudio\n",
        "# from cosine import WarmupCosineScheduler\n",
        "# from datamodule.transforms import TextTransform\n",
        "\n",
        "# from pytorch_lightning import LightningModule\n",
        "# from espnet.nets.batch_beam_search import BatchBeamSearch\n",
        "# from espnet.nets.pytorch_backend.e2e_asr_conformer_av import E2E\n",
        "# from espnet.nets.scorers.length_bonus import LengthBonus\n",
        "# from espnet.nets.scorers.ctc import CTCPrefixScorer\n",
        "\n",
        "\n",
        "def compute_word_level_distance(seq1, seq2):\n",
        "    return torchaudio.functional.edit_distance(seq1.lower().split(), seq2.lower().split())\n",
        "\n",
        "\n",
        "class ModelModule(LightningModule):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(cfg)\n",
        "        self.cfg = cfg\n",
        "        self.backbone_args = self.cfg.model.audiovisual_backbone\n",
        "\n",
        "        self.text_transform = TextTransform()\n",
        "        self.token_list = self.text_transform.token_list\n",
        "        self.model = E2E(len(self.token_list), self.backbone_args)\n",
        "\n",
        "        # -- initialise\n",
        "        if self.cfg.pretrained_model_path:\n",
        "            ckpt = torch.load(self.cfg.pretrained_model_path, map_location=lambda storage, loc: storage)\n",
        "            if self.cfg.transfer_frontend:\n",
        "                tmp_ckpt = {k: v for k, v in ckpt[\"model_state_dict\"].items() if k.startswith(\"trunk.\") or k.startswith(\"frontend3D.\")}\n",
        "                self.model.encoder.frontend.load_state_dict(tmp_ckpt)\n",
        "            elif self.cfg.transfer_encoder:\n",
        "                tmp_ckpt = {k.replace(\"encoder.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"encoder.\")}\n",
        "                self.model.encoder.load_state_dict(tmp_ckpt, strict=True)\n",
        "            else:\n",
        "                self.model.load_state_dict(ckpt)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW([{\"name\": \"model\", \"params\": self.model.parameters(), \"lr\": self.cfg.optimizer.lr}], weight_decay=self.cfg.optimizer.weight_decay, betas=(0.9, 0.98))\n",
        "        scheduler = WarmupCosineScheduler(optimizer, self.cfg.optimizer.warmup_epochs, self.cfg.trainer.max_epochs, len(self.trainer.datamodule.train_dataloader()))\n",
        "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def forward(self, video, audio):\n",
        "        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n",
        "        video_feat, _ = self.model.encoder(video.unsqueeze(0).to(self.device), None)\n",
        "        audio_feat, _ = self.model.aux_encoder(audio.unsqueeze(0).to(self.device), None)\n",
        "        audiovisual_feat = self.model.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n",
        "\n",
        "        audiovisual_feat = audiovisual_feat.squeeze(0)\n",
        "\n",
        "        nbest_hyps = self.beam_search(audiovisual_feat)\n",
        "        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n",
        "        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n",
        "        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n",
        "        return predicted\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._step(batch, batch_idx, step_type=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._step(batch, batch_idx, step_type=\"val\")\n",
        "\n",
        "    def test_step(self, sample, sample_idx):\n",
        "        video_feat, _ = self.model.encoder(sample[\"video\"].unsqueeze(0).to(self.device), None)\n",
        "        audio_feat, _ = self.model.aux_encoder(sample[\"audio\"].unsqueeze(0).to(self.device), None)\n",
        "        audiovisual_feat = self.model.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n",
        "        audiovisual_feat = audiovisual_feat.squeeze(0)\n",
        "\n",
        "        nbest_hyps = self.beam_search(audiovisual_feat)\n",
        "        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n",
        "        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n",
        "        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n",
        "\n",
        "        token_id = sample[\"target\"]\n",
        "        actual = self.text_transform.post_process(token_id)\n",
        "\n",
        "        self.total_edit_distance += compute_word_level_distance(actual, predicted)\n",
        "        self.total_length += len(actual.split())\n",
        "        return\n",
        "\n",
        "    def _step(self, batch, batch_idx, step_type):\n",
        "        loss, loss_ctc, loss_att, acc = self.model(batch[\"videos\"], batch[\"audios\"], batch[\"video_lengths\"],\n",
        "                                                   batch[\"audio_lengths\"], batch[\"targets\"])\n",
        "        batch_size = len(batch[\"videos\"])\n",
        "\n",
        "        if step_type == \"train\":\n",
        "            self.log(\"loss\", loss, on_step=True, on_epoch=True, batch_size=batch_size)\n",
        "            self.log(\"loss_ctc\", loss_ctc, on_step=False, on_epoch=True, batch_size=batch_size)\n",
        "            self.log(\"loss_att\", loss_att, on_step=False, on_epoch=True, batch_size=batch_size)\n",
        "            self.log(\"decoder_acc\", acc, on_step=True, on_epoch=True, batch_size=batch_size)\n",
        "        else:\n",
        "            self.log(\"loss_val\", loss, batch_size=batch_size)\n",
        "            self.log(\"loss_ctc_val\", loss_ctc, batch_size=batch_size)\n",
        "            self.log(\"loss_att_val\", loss_att, batch_size=batch_size)\n",
        "            self.log(\"decoder_acc_val\", acc, batch_size=batch_size)\n",
        "\n",
        "        if step_type == \"train\":\n",
        "            self.log(\"monitoring_step\", torch.tensor(self.global_step, dtype=torch.float32))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        sampler = self.trainer.train_dataloader.loaders.batch_sampler\n",
        "        if hasattr(sampler, \"set_epoch\"):\n",
        "            sampler.set_epoch(self.current_epoch)\n",
        "        return super().on_train_epoch_start()\n",
        "\n",
        "    def on_test_epoch_start(self):\n",
        "        self.total_length = 0\n",
        "        self.total_edit_distance = 0\n",
        "        self.text_transform = TextTransform()\n",
        "        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        self.log(\"wer\", self.total_edit_distance / self.total_length)\n",
        "\n",
        "\n",
        "def get_beam_search_decoder(model, token_list, ctc_weight=0.1, beam_size=40):\n",
        "    scorers = {\n",
        "        \"decoder\": model.decoder,\n",
        "        \"ctc\": CTCPrefixScorer(model.ctc, model.eos),\n",
        "        \"length_bonus\": LengthBonus(len(token_list)),\n",
        "        \"lm\": None\n",
        "    }\n",
        "\n",
        "    weights = {\n",
        "        \"decoder\": 1.0 - ctc_weight,\n",
        "        \"ctc\": ctc_weight,\n",
        "        \"lm\": 0.0,\n",
        "        \"length_bonus\": 0.0,\n",
        "    }\n",
        "\n",
        "    return BatchBeamSearch(\n",
        "        beam_size=beam_size,\n",
        "        vocab_size=len(token_list),\n",
        "        weights=weights,\n",
        "        scorers=scorers,\n",
        "        sos=model.sos,\n",
        "        eos=model.eos,\n",
        "        token_list=token_list,\n",
        "        pre_beam_score_key=None if ctc_weight == 1.0 else \"decoder\",\n",
        "    )"
      ],
      "metadata": {
        "id": "hYkyP2H4zcZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AVSR Base"
      ],
      "metadata": {
        "id": "zEdCfDfmzabz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torchaudio\n",
        "# from cosine import WarmupCosineScheduler\n",
        "# from datamodule.transforms import TextTransform\n",
        "\n",
        "# from pytorch_lightning import LightningModule\n",
        "# from espnet.nets.batch_beam_search import BatchBeamSearch\n",
        "# from espnet.nets.pytorch_backend.e2e_asr_conformer import E2E\n",
        "# from espnet.nets.scorers.length_bonus import LengthBonus\n",
        "# from espnet.nets.scorers.ctc import CTCPrefixScorer\n",
        "\n",
        "\n",
        "def compute_word_level_distance(seq1, seq2):\n",
        "    return torchaudio.functional.edit_distance(seq1.lower().split(), seq2.lower().split())\n",
        "\n",
        "\n",
        "class ModelModule(LightningModule):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(cfg)\n",
        "        self.cfg = cfg\n",
        "        if self.cfg.data.modality == \"audio\":\n",
        "            self.backbone_args = self.cfg.model.audio_backbone\n",
        "        elif self.cfg.data.modality == \"video\":\n",
        "            self.backbone_args = self.cfg.model.visual_backbone\n",
        "\n",
        "        self.text_transform = TextTransform()\n",
        "        self.token_list = self.text_transform.token_list\n",
        "        self.model = E2E(len(self.token_list), self.backbone_args)\n",
        "\n",
        "        # -- initialise\n",
        "        if self.cfg.pretrained_model_path:\n",
        "            ckpt = torch.load(self.cfg.pretrained_model_path, map_location=lambda storage, loc: storage)\n",
        "            if self.cfg.transfer_frontend:\n",
        "                tmp_ckpt = {k: v for k, v in ckpt[\"model_state_dict\"].items() if k.startswith(\"trunk.\") or k.startswith(\"frontend3D.\")}\n",
        "                self.model.encoder.frontend.load_state_dict(tmp_ckpt)\n",
        "            elif self.cfg.transfer_encoder:\n",
        "                tmp_ckpt = {k.replace(\"encoder.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"encoder.\")}\n",
        "                self.model.encoder.load_state_dict(tmp_ckpt, strict=True)\n",
        "            else:\n",
        "                self.model.load_state_dict(ckpt)\n",
        "        else:\n",
        "            print('Entrenamiento de cero')\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW([{\"name\": \"model\", \"params\": self.model.parameters(), \"lr\": self.cfg.optimizer.lr}], weight_decay=self.cfg.optimizer.weight_decay, betas=(0.9, 0.98))\n",
        "        scheduler = WarmupCosineScheduler(optimizer, self.cfg.optimizer.warmup_epochs, self.cfg.trainer.max_epochs, len(self.trainer.datamodule.train_dataloader()))\n",
        "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def forward(self, sample):\n",
        "        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n",
        "        enc_feat, _ = self.model.encoder(sample.unsqueeze(0).to(self.device), None)\n",
        "        enc_feat = enc_feat.squeeze(0)\n",
        "\n",
        "        nbest_hyps = self.beam_search(enc_feat)\n",
        "        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n",
        "        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n",
        "        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n",
        "        return predicted\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._step(batch, batch_idx, step_type=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._step(batch, batch_idx, step_type=\"val\")\n",
        "\n",
        "    def test_step(self, sample, sample_idx):\n",
        "        enc_feat, _ = self.model.encoder(sample[\"input\"].unsqueeze(0).to(self.device), None)\n",
        "        enc_feat = enc_feat.squeeze(0)\n",
        "\n",
        "        nbest_hyps = self.beam_search(enc_feat)\n",
        "        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n",
        "        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n",
        "        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n",
        "\n",
        "        token_id = sample[\"target\"]\n",
        "        actual = self.text_transform.post_process(token_id)\n",
        "\n",
        "        self.total_edit_distance += compute_word_level_distance(actual, predicted)\n",
        "        self.total_length += len(actual.split())\n",
        "        return\n",
        "\n",
        "    def _step(self, batch, batch_idx, step_type):\n",
        "        loss, loss_ctc, loss_att, acc = self.model(batch[\"inputs\"], batch[\"input_lengths\"], batch[\"targets\"])\n",
        "        batch_size = len(batch[\"inputs\"])\n",
        "\n",
        "        if step_type == \"train\":\n",
        "            self.log(\"loss\", loss, on_step=True, on_epoch=True, batch_size=batch_size)\n",
        "            self.log(\"loss_ctc\", loss_ctc, on_step=False, on_epoch=True, batch_size=batch_size)\n",
        "            self.log(\"loss_att\", loss_att, on_step=False, on_epoch=True, batch_size=batch_size)\n",
        "            self.log(\"decoder_acc\", acc, on_step=True, on_epoch=True, batch_size=batch_size)\n",
        "        else:\n",
        "            self.log(\"loss_val\", loss, batch_size=batch_size)\n",
        "            self.log(\"loss_ctc_val\", loss_ctc, batch_size=batch_size)\n",
        "            self.log(\"loss_att_val\", loss_att, batch_size=batch_size)\n",
        "            self.log(\"decoder_acc_val\", acc, batch_size=batch_size)\n",
        "\n",
        "        if step_type == \"train\":\n",
        "            self.log(\"monitoring_step\", torch.tensor(self.global_step, dtype=torch.float32))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        #sampler = self.trainer.train_dataloader.loaders.batch_sampler\n",
        "        sampler = self.trainer.train_dataloader.batch_sampler\n",
        "        if hasattr(sampler, \"set_epoch\"):\n",
        "            sampler.set_epoch(self.current_epoch)\n",
        "        return super().on_train_epoch_start()\n",
        "\n",
        "    def on_test_epoch_start(self):\n",
        "        self.total_length = 0\n",
        "        self.total_edit_distance = 0\n",
        "        self.text_transform = TextTransform()\n",
        "        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        self.log(\"wer\", self.total_edit_distance / self.total_length)\n",
        "\n",
        "\n",
        "def get_beam_search_decoder(model, token_list, ctc_weight=0.1, beam_size=40):\n",
        "    scorers = {\n",
        "        \"decoder\": model.decoder,\n",
        "        \"ctc\": CTCPrefixScorer(model.ctc, model.eos),\n",
        "        \"length_bonus\": LengthBonus(len(token_list)),\n",
        "        \"lm\": None\n",
        "    }\n",
        "\n",
        "    weights = {\n",
        "        \"decoder\": 1.0 - ctc_weight,\n",
        "        \"ctc\": ctc_weight,\n",
        "        \"lm\": 0.0,\n",
        "        \"length_bonus\": 0.0,\n",
        "    }\n",
        "\n",
        "    return BatchBeamSearch(\n",
        "        beam_size=beam_size,\n",
        "        vocab_size=len(token_list),\n",
        "        weights=weights,\n",
        "        scorers=scorers,\n",
        "        sos=model.sos,\n",
        "        eos=model.eos,\n",
        "        token_list=token_list,\n",
        "        pre_beam_score_key=None if ctc_weight == 1.0 else \"decoder\",\n",
        "    )"
      ],
      "metadata": {
        "id": "Li8yCFQ4zmxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuraciones con Hydra"
      ],
      "metadata": {
        "id": "IzIaMo14x8me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "import pickle\n",
        "\n",
        "@hydra.main(config_path=\"/content/auto_avsr/configs\", config_name=\"config\")\n",
        "def my_app(cfg):\n",
        "    #print(OmegaConf.to_yaml(cfg))\n",
        "    cfg = dict(cfg)\n",
        "    with open(\"cfg.pickle\", \"wb\") as f:\n",
        "      pickle.dump(cfg, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    my_app()"
      ],
      "metadata": {
        "id": "sxQHAxxtbrMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSVbpUpHcKf_",
        "outputId": "a3dbb547-fd2a-4c6f-80b4-5d0a2d5af39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/main.py:5: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(config_path=\"/content/auto_avsr/configs\", config_name=\"config\")\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "CERfWWMgWOcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = DictConfig({'exp_dir': None, 'exp_name': None, 'gpus': None, 'slurm_job_id': None, 'file_path': None,\n",
        "                  'ckpt_path': None, 'pretrained_model_path': None, 'transfer_frontend': None,\n",
        "                  'transfer_encoder': None,\n",
        "                  'data': {'modality': 'video', 'use_audio_normalise': False, 'max_frames': 1800,\n",
        "                           'max_frames_val': 500,\n",
        "                           'dataset': {'root_dir': None, 'label_dir': 'labels',\n",
        "                                       'train_file': 'lrs3_train_transcript_lengths_seg24s.csv',\n",
        "                                       'val_file': 'lrs3_test_transcript_lengths_seg24s.csv',\n",
        "                                       'test_file': 'lrs3_test_transcript_lengths_seg24s.csv'}},\n",
        "                  'model': {'visual_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072,\n",
        "                                                'elayers': 12, 'transformer_input_layer': 'conv3d',\n",
        "                                                'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1,\n",
        "                                                'transformer_encoder_attn_layer_type': 'rel_mha',\n",
        "                                                'macaron_style': True, 'use_cnn_module': True,\n",
        "                                                'cnn_module_kernel': 31, 'zero_triu': False,\n",
        "                                                'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': 768,\n",
        "                                                'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n",
        "                                                'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n",
        "                                                'ctc_type': 'builtin', 'rel_pos_type': 'latest'},\n",
        "                            'audio_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072, 'elayers': 12,\n",
        "                                               'transformer_input_layer': 'conv1d', 'dropout_rate': 0.1,\n",
        "                                               'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n",
        "                                               'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n",
        "                                               'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n",
        "                                               'ddim': 768, 'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n",
        "                                               'transformer_length_normalized_loss': False, 'mtlalpha': 0.1, 'ctc_type': 'builtin',\n",
        "                                               'rel_pos_type': 'latest'},\n",
        "                            'audiovisual_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072, 'elayers': 12,\n",
        "                                                     'transformer_input_layer': 'conv3d', 'dropout_rate': 0.1,\n",
        "                                                     'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n",
        "                                                     'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n",
        "                                                     'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n",
        "                                                     'ddim': 768, 'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n",
        "                                                     'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n",
        "                                                     'ctc_type': 'builtin', 'rel_pos_type': 'latest', 'aux_adim': 768,\n",
        "                                                     'aux_aheads': 12, 'aux_eunits': 3072, 'aux_elayers': 12,\n",
        "                                                     'aux_transformer_input_layer': 'conv1d', 'aux_dropout_rate': 0.1,\n",
        "                                                     'aux_transformer_attn_dropout_rate': 0.1,\n",
        "                                                     'aux_transformer_encoder_attn_layer_type': 'rel_mha', 'aux_macaron_style': True,\n",
        "                                                     'aux_use_cnn_module': True, 'aux_cnn_module_kernel': 31, 'aux_zero_triu': False,\n",
        "                                                     'aux_a_upsample_ratio': 1, 'aux_relu_type': 'swish', 'aux_dunits': 3072,\n",
        "                                                     'aux_dlayers': 6, 'aux_lsm_weight': 0.1,\n",
        "                                                     'aux_transformer_length_normalized_loss': False, 'aux_mtlalpha': 0.1,\n",
        "                                                     'aux_ctc_type': 'builtin', 'aux_rel_pos_type': 'latest', 'fusion_hdim': 8192,\n",
        "                                                     'fusion_norm': 'batchnorm'}},\n",
        "                  'optimizer': {'name': 'adamw', 'lr': 0.001, 'warmup_epochs': 5, 'weight_decay': 0.03},\n",
        "                  'trainer': {'precision': 32, 'max_epochs': 75, 'num_nodes': 1, 'gpus': -1, 'sync_batchnorm': True,\n",
        "                              'default_root_dir': '${exp_dir}', 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1,\n",
        "                              'gradient_clip_val': 5.0, 'replace_sampler_ddp': False, 'resume_from_checkpoint': None},\n",
        "                  'decode': {'name': 'default', 'snr_target': 999999}})"
      ],
      "metadata": {
        "id": "SFHMxS-nA3Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = DictConfig({'exp_dir': None, 'exp_name': None, 'gpus': None, 'slurm_job_id': None, 'file_path': None,\n",
        "                  'ckpt_path': None, 'pretrained_model_path': None, 'transfer_frontend': None,\n",
        "                  'transfer_encoder': None,\n",
        "                  'data': {'modality': 'video', 'use_audio_normalise': False, 'max_frames': 1800,\n",
        "                           'max_frames_val': 1800,\n",
        "                           'dataset': {'root_dir': '/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing', 'label_dir': 'labels',\n",
        "                                       'train_file': 'VPHBUSFX_val.csv',\n",
        "                                       'val_file': 'VPHBUSFX_val.csv'}},#,\n",
        "                                       #'test_file': 'lrs3_test_transcript_lengths_seg24s.csv'}},\n",
        "                  'model': {'visual_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072,\n",
        "                                                'elayers': 12, 'transformer_input_layer': 'conv3d',\n",
        "                                                'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1,\n",
        "                                                'transformer_encoder_attn_layer_type': 'rel_mha',\n",
        "                                                'macaron_style': True, 'use_cnn_module': True,\n",
        "                                                'cnn_module_kernel': 31, 'zero_triu': False,\n",
        "                                                'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': 768,\n",
        "                                                'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n",
        "                                                'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n",
        "                                                'ctc_type': 'builtin', 'rel_pos_type': 'latest'},\n",
        "                            'audio_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072, 'elayers': 12,\n",
        "                                               'transformer_input_layer': 'conv1d', 'dropout_rate': 0.1,\n",
        "                                               'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n",
        "                                               'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n",
        "                                               'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n",
        "                                               'ddim': 768, 'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n",
        "                                               'transformer_length_normalized_loss': False, 'mtlalpha': 0.1, 'ctc_type': 'builtin',\n",
        "                                               'rel_pos_type': 'latest'},\n",
        "                            'audiovisual_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072, 'elayers': 12,\n",
        "                                                     'transformer_input_layer': 'conv3d', 'dropout_rate': 0.1,\n",
        "                                                     'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n",
        "                                                     'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n",
        "                                                     'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n",
        "                                                     'ddim': 768, 'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n",
        "                                                     'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n",
        "                                                     'ctc_type': 'builtin', 'rel_pos_type': 'latest', 'aux_adim': 768,\n",
        "                                                     'aux_aheads': 12, 'aux_eunits': 3072, 'aux_elayers': 12,\n",
        "                                                     'aux_transformer_input_layer': 'conv1d', 'aux_dropout_rate': 0.1,\n",
        "                                                     'aux_transformer_attn_dropout_rate': 0.1,\n",
        "                                                     'aux_transformer_encoder_attn_layer_type': 'rel_mha', 'aux_macaron_style': True,\n",
        "                                                     'aux_use_cnn_module': True, 'aux_cnn_module_kernel': 31, 'aux_zero_triu': False,\n",
        "                                                     'aux_a_upsample_ratio': 1, 'aux_relu_type': 'swish', 'aux_dunits': 3072,\n",
        "                                                     'aux_dlayers': 6, 'aux_lsm_weight': 0.1,\n",
        "                                                     'aux_transformer_length_normalized_loss': False, 'aux_mtlalpha': 0.1,\n",
        "                                                     'aux_ctc_type': 'builtin', 'aux_rel_pos_type': 'latest', 'fusion_hdim': 8192,\n",
        "                                                     'fusion_norm': 'batchnorm'}},\n",
        "                  'optimizer': {'name': 'adamw', 'lr': 0.001, 'warmup_epochs': 5, 'weight_decay': 0.03},\n",
        "                  'trainer': {'precision': 32, 'max_epochs': 75, 'num_nodes': 1, 'sync_batchnorm': True,\n",
        "                              'default_root_dir': '${exp_dir}', 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1,\n",
        "                              'gradient_clip_val': 5.0},\n",
        "                  'decode': {'name': 'default', 'snr_target': 999999}})"
      ],
      "metadata": {
        "id": "TcX8RNfWGe6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(42, workers=True)\n",
        "cfg.gpus = torch.cuda.device_count()\n",
        "#cfg.gpus = 1\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    monitor=\"monitoring_step\",\n",
        "    mode=\"max\",\n",
        "    dirpath=os.path.join(cfg.exp_dir, cfg.exp_name) if cfg.exp_dir else None,\n",
        "    save_last=True,\n",
        "    filename=\"{epoch}\",\n",
        "    save_top_k=10,\n",
        ")\n",
        "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
        "callbacks = [checkpoint, lr_monitor]\n",
        "\n",
        "# Set modules and trainer\n",
        "# if cfg.data.modality in [\"audio\", \"visual\"]:\n",
        "#     from lightning import ModelModule\n",
        "# elif cfg.data.modality == \"audiovisual\":\n",
        "#     from lightning_av import ModelModule\n",
        "datamodule = DataModule(cfg)\n",
        "modelmodule = ModelModule(cfg)\n",
        "trainer = Trainer(\n",
        "    **cfg.trainer,\n",
        "    #logger=WandbLogger(name=cfg.exp_name, project=\"auto_avsr\"),\n",
        "    callbacks=callbacks,\n",
        "    #strategy=DDPPlugin(find_unused_parameters=False)\n",
        ")\n",
        "\n",
        "trainer.fit(model=modelmodule, datamodule=datamodule)\n",
        "ensemble(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEacjNzBY2NW",
        "outputId": "bd70959f-ee05-4701-d01b-8af15f5a0384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de gpus 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-aadeebcb2c2c>:22: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento de cero\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type | Params\n",
            "-------------------------------\n",
            "0 | model | E2E  | 238 M \n",
            "-------------------------------\n",
            "238 M     Trainable params\n",
            "0         Non-trainable params\n",
            "238 M     Total params\n",
            "955.349   Total estimated model params size (MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_d6bRoEpJ0j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "suYIqs87J0gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWfyqnNsJ0eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uE626GGFJ0bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IS_cL9rOJ0Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQXdJIUwJ0VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@hydra.main(config_path=\"/content/auto_avsr/configs\", config_name=\"config\")\n",
        "def main(cfg):\n",
        "    seed_everything(42, workers=True)\n",
        "    cfg.gpus = torch.cuda.device_count()\n",
        "\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        monitor=\"monitoring_step\",\n",
        "        mode=\"max\",\n",
        "        dirpath=os.path.join(cfg.exp_dir, cfg.exp_name) if cfg.exp_dir else None,\n",
        "        save_last=True,\n",
        "        filename=\"{epoch}\",\n",
        "        save_top_k=10,\n",
        "    )\n",
        "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
        "    callbacks = [checkpoint, lr_monitor]\n",
        "\n",
        "    # Set modules and trainer\n",
        "    # if cfg.data.modality in [\"audio\", \"visual\"]:\n",
        "    #     from lightning import ModelModule\n",
        "    # elif cfg.data.modality == \"audiovisual\":\n",
        "    #     from lightning_av import ModelModule\n",
        "    datamodule = DataModule(cfg)\n",
        "    modelmodule = ModelModule(cfg)\n",
        "    trainer = Trainer(\n",
        "        **cfg.trainer,\n",
        "        #logger=WandbLogger(name=cfg.exp_name, project=\"auto_avsr\"),\n",
        "        callbacks=callbacks,\n",
        "        strategy=DDPPlugin(find_unused_parameters=False)\n",
        "    )\n",
        "\n",
        "    trainer.fit(model=modelmodule, datamodule=datamodule)\n",
        "    ensemble(cfg)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "uYjw17rJxzom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "BL-OitEkTMw5",
        "outputId": "e2e17f2f-a14d-403e-b60a-e94ea9f23ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [--help] [--hydra-help] [--version] [--cfg {job,hydra,all}]\n",
            "                                [--package PACKAGE] [--run] [--multirun] [--shell-completion]\n",
            "                                [--config-path CONFIG_PATH] [--config-name CONFIG_NAME]\n",
            "                                [--config-dir CONFIG_DIR] [--info]\n",
            "                                [overrides ...]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jsQNfgKdW06n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generar dataset"
      ],
      "metadata": {
        "id": "sLyTi9vH-V8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python"
      ],
      "metadata": {
        "id": "SIZYXVBUFoNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/auto_avsr/preparation/preprocess_lrs2lrs3.py \\\n",
        "    --data-dir [/content/drive/MyDrive/vsr/recortes_davinci] \\\n",
        "    --detector [retinaface] \\\n",
        "    --root-dir [/content/Dataset] \\\n",
        "    --dataset [lrs2] \\\n",
        "    --subset [train]"
      ],
      "metadata": {
        "id": "M6l3WSl4-bop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea97bed8-2057-426f-8636-b37bb4a76182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory [/content/Dataset]/labels created\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/auto_avsr/preparation/preprocess_lrs2lrs3.py\", line 161, in <module>\n",
            "    unit = math.ceil(len(filenames) * 1.0 / args.groups)\n",
            "NameError: name 'filenames' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ihp60LnEFr_4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}