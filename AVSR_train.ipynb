{"metadata":{"colab":{"provenance":[],"collapsed_sections":["lrOIKAfWsW9R","6LDIcdKAwaHC","N_Y-Polmt8de","PdZn0dVwu-dF","cgj1aC53vk09","EB1W0yKNwuDQ","pqTbNgn_wUkN","twaDpetGxIYG","IEGOCDxVe4qB","exinCbGXgAw4","Oj8NjiA_f8Ny","bM0t1pzbf9nX","Y-xkRUYSfc5x","CRX9rtXHfd2l","azqaGmtUj3Dw","gPL6F3hminyQ","2pSgOj6Liukd","7qQLfrWejOjd","FiwOPaYzkiE7","MwoN53F0kkFq","s9rqrInBks8b","JDY4Nxr7dbzg","YvlnuzwKl8ya","JAA0ah8jnioL","apmMcHctoPrh","iE1DYbe_pAeg","dQAWTi24pfBa","Cf9JstLvmyhc","D6w-KSvPtOO3","nnas4FCNuK-3","w-Y9I92RqrVT","P4_0WJ1RrLa0","6-ciO8kw0Wda","3nQEmdFqNcjT","5bvueiRMObJw","gZ980wJWQULi","0nHmNDnWQxcu","9KU4cv09Qrkz","kJJTggAkS5Pe","TXA40fu0zMBS","IzIaMo14x8me"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8dcde6792a8f4c4a8cd670d6ec7780d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f327c20b68d428183a1b0972227a30d","IPY_MODEL_39183003182d4e71860eb2c4ba49a434","IPY_MODEL_aeaebc80b25d435ba16ed85926821cdb"],"layout":"IPY_MODEL_06c169d08fe94e94a3042c91c50621ad"}},"5f327c20b68d428183a1b0972227a30d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b202dc6a3724c91a32ca06d57ff9233","placeholder":"​","style":"IPY_MODEL_48c99d692d0a4e4e91d939f2fdd45a26","value":"Epoch 5:   0%"}},"39183003182d4e71860eb2c4ba49a434":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e70054be0c2e43dc97553a87b5c501e3","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d789f2de1e7f46438cbe4842cee6f263","value":0}},"aeaebc80b25d435ba16ed85926821cdb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e89a251e9404c1ea940739de254af48","placeholder":"​","style":"IPY_MODEL_9dde1c06c1094dcd8dc9fd410c05078a","value":" 0/6 [00:00&lt;?, ?it/s, v_num=1]"}},"06c169d08fe94e94a3042c91c50621ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"6b202dc6a3724c91a32ca06d57ff9233":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48c99d692d0a4e4e91d939f2fdd45a26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e70054be0c2e43dc97553a87b5c501e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d789f2de1e7f46438cbe4842cee6f263":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e89a251e9404c1ea940739de254af48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dde1c06c1094dcd8dc9fd410c05078a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c20d9f403b594e499f0d61bf5c797204":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f10cf814b9ff4b4399a4d8e04e3f37a5","IPY_MODEL_955e583c27ee453b997a7a361d2e152b","IPY_MODEL_70442ddd990543509a116a14e3736e1c"],"layout":"IPY_MODEL_e9edfcca7eac4418892ae2b45e642f86"}},"f10cf814b9ff4b4399a4d8e04e3f37a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7d63b324b694d00853db7f8b9ee090a","placeholder":"​","style":"IPY_MODEL_cca092384bbc4b63ae36f0060daed41f","value":"Validation DataLoader 0: 100%"}},"955e583c27ee453b997a7a361d2e152b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_99199c0d46b043d0bcf750e9049af880","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9c9e52b487d47f0bf679e96a6c4b99b","value":6}},"70442ddd990543509a116a14e3736e1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_137b09249057418babd6a3f1a8ab4845","placeholder":"​","style":"IPY_MODEL_ec64c45351b541dfa806c25339544c4b","value":" 6/6 [00:09&lt;00:00,  0.64it/s]"}},"e9edfcca7eac4418892ae2b45e642f86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"d7d63b324b694d00853db7f8b9ee090a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cca092384bbc4b63ae36f0060daed41f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99199c0d46b043d0bcf750e9049af880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9c9e52b487d47f0bf679e96a6c4b99b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"137b09249057418babd6a3f1a8ab4845":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec64c45351b541dfa806c25339544c4b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37be8de018694624a41cbc5752393675":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1dc2f7029df64bdbb3f5a435f6566dbb","IPY_MODEL_c74ba06bfa5c4e2db6f6c394c7b101ef","IPY_MODEL_590d0226ec6748c993cf678a5ace7fb3"],"layout":"IPY_MODEL_6f59abc2c3674181ae6b228b03023bdd"}},"1dc2f7029df64bdbb3f5a435f6566dbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bbab8d198364b0cb1b8b06540cc26a2","placeholder":"​","style":"IPY_MODEL_4420d700dc5842a8bd1416ae84eb9dd9","value":"Validation DataLoader 0: 100%"}},"c74ba06bfa5c4e2db6f6c394c7b101ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc34a518cf6246a1855f69d975a8db2e","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11363e68ee204a209b8eec19aade6da1","value":6}},"590d0226ec6748c993cf678a5ace7fb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed32ae1fcb3e424f8d1e4255e1de4f2f","placeholder":"​","style":"IPY_MODEL_02e357894b614c17a1e86d8b08805b25","value":" 6/6 [00:09&lt;00:00,  0.61it/s]"}},"6f59abc2c3674181ae6b228b03023bdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"7bbab8d198364b0cb1b8b06540cc26a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4420d700dc5842a8bd1416ae84eb9dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc34a518cf6246a1855f69d975a8db2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11363e68ee204a209b8eec19aade6da1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed32ae1fcb3e424f8d1e4255e1de4f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02e357894b614c17a1e86d8b08805b25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b598d402f874dea94eb2bdd4142489b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_071ed9bc7b97418cbea3a26cd2f89613","IPY_MODEL_518610cd11344fd1be914624fb59169a","IPY_MODEL_d1e0a6c553fd432da3d1c5d60b7444d3"],"layout":"IPY_MODEL_3b23a41bddfe402cab70a5dfab848d1b"}},"071ed9bc7b97418cbea3a26cd2f89613":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3b14874d4f64c1b916f6913b8351d94","placeholder":"​","style":"IPY_MODEL_780d3a53c02440c291d614ed4c24af75","value":"Validation DataLoader 0: 100%"}},"518610cd11344fd1be914624fb59169a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ce819ef0dc6480a8c5aeee9b85c0780","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec5d9a598cee4c9389f10d1711cda6a8","value":6}},"d1e0a6c553fd432da3d1c5d60b7444d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ffc251c1545403b94814ca87ceffb7c","placeholder":"​","style":"IPY_MODEL_90cafacd21894e7f8fa3b0d38b4c0ba2","value":" 6/6 [00:09&lt;00:00,  0.63it/s]"}},"3b23a41bddfe402cab70a5dfab848d1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"e3b14874d4f64c1b916f6913b8351d94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"780d3a53c02440c291d614ed4c24af75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ce819ef0dc6480a8c5aeee9b85c0780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec5d9a598cee4c9389f10d1711cda6a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ffc251c1545403b94814ca87ceffb7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90cafacd21894e7f8fa3b0d38b4c0ba2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcaa5c8a699e421f9e5ab1d2aabebdac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35997b55fd4d4340a45bc760570790a9","IPY_MODEL_5c1b3d2439114a308dccd23084582663","IPY_MODEL_0a2338d3e1ca49b78679d18a825038ce"],"layout":"IPY_MODEL_27de67756c314679b39bf06444c60576"}},"35997b55fd4d4340a45bc760570790a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d1713d1a16641ee9fc03e5024c055be","placeholder":"​","style":"IPY_MODEL_559c645fccee45ee903d7418d81d6664","value":"Validation DataLoader 0: 100%"}},"5c1b3d2439114a308dccd23084582663":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_92f71c757ca1468c899232943deb02bf","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9f3e28d8b6941449014cb03a9ef0e02","value":6}},"0a2338d3e1ca49b78679d18a825038ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afd7b2402b7046b38b3d2c70478025ad","placeholder":"​","style":"IPY_MODEL_2ce15fc22d3240558aeb330d1e145ea4","value":" 6/6 [00:09&lt;00:00,  0.61it/s]"}},"27de67756c314679b39bf06444c60576":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"0d1713d1a16641ee9fc03e5024c055be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"559c645fccee45ee903d7418d81d6664":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92f71c757ca1468c899232943deb02bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f3e28d8b6941449014cb03a9ef0e02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"afd7b2402b7046b38b3d2c70478025ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ce15fc22d3240558aeb330d1e145ea4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b13f1db49b24474d9b4b24b2de734c2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80bbd68de74d45bcb3adadda464eb928","IPY_MODEL_c78b62b7ff064268a84bba448d94e12f","IPY_MODEL_45e4c228598b45c6a881f49491cff000"],"layout":"IPY_MODEL_35e92c8127844aa988cd1dff0b4a2653"}},"80bbd68de74d45bcb3adadda464eb928":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cffd4b5a463429e979fbbffaa91d881","placeholder":"​","style":"IPY_MODEL_ea5796b40e2048099f916917d3361579","value":"Validation DataLoader 0: 100%"}},"c78b62b7ff064268a84bba448d94e12f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e9b659ca9e74cc9b039e0e8886619dd","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99c79108dae8471996edf9730b952b61","value":6}},"45e4c228598b45c6a881f49491cff000":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6ca995486d84befb924a7bcfb877331","placeholder":"​","style":"IPY_MODEL_f9e950b700b2447a8d81050a9c8e490b","value":" 6/6 [00:09&lt;00:00,  0.61it/s]"}},"35e92c8127844aa988cd1dff0b4a2653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"6cffd4b5a463429e979fbbffaa91d881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea5796b40e2048099f916917d3361579":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e9b659ca9e74cc9b039e0e8886619dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99c79108dae8471996edf9730b952b61":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6ca995486d84befb924a7bcfb877331":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9e950b700b2447a8d81050a9c8e490b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8684435,"sourceType":"datasetVersion","datasetId":4785917},{"sourceId":31689,"sourceType":"modelInstanceVersion","modelInstanceId":26577},{"sourceId":58177,"sourceType":"modelInstanceVersion","modelInstanceId":27295},{"sourceId":65098,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":54312}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/mpc001/auto_avsr","metadata":{"id":"T-nHfbP_HGZf","outputId":"cde08d2a-4947-4095-93a4-b1c27880b3f1","execution":{"iopub.status.busy":"2024-06-14T22:56:04.128394Z","iopub.execute_input":"2024-06-14T22:56:04.128726Z","iopub.status.idle":"2024-06-14T22:56:07.164824Z","shell.execute_reply.started":"2024-06-14T22:56:04.128698Z","shell.execute_reply":"2024-06-14T22:56:07.163715Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'auto_avsr'...\nremote: Enumerating objects: 272, done.\u001b[K\nremote: Counting objects: 100% (91/91), done.\u001b[K\nremote: Compressing objects: 100% (56/56), done.\u001b[K\nremote: Total 272 (delta 53), reused 35 (delta 35), pack-reused 181\u001b[K\nReceiving objects: 100% (272/272), 31.46 MiB | 27.16 MiB/s, done.\nResolving deltas: 100% (95/95), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install fairseq","metadata":{"id":"ZCZMfAdNISz2","outputId":"e098cd26-7f60-4da1-aa5b-8e98d0247dbb","execution":{"iopub.status.busy":"2024-06-14T22:56:07.167118Z","iopub.execute_input":"2024-06-14T22:56:07.167510Z","iopub.status.idle":"2024-06-14T22:57:16.036557Z","shell.execute_reply.started":"2024-06-14T22:56:07.167473Z","shell.execute_reply":"2024-06-14T22:57:16.035562Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting fairseq\n  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cffi in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.16.0)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from fairseq) (3.0.8)\nCollecting hydra-core<1.1,>=1.0.7 (from fairseq)\n  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\nCollecting omegaconf<2.1 (from fairseq)\n  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from fairseq) (2023.12.25)\nCollecting sacrebleu>=1.4.12 (from fairseq)\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fairseq) (4.66.1)\nCollecting bitarray (from fairseq)\n  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nRequirement already satisfied: torchaudio>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from fairseq) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fairseq) (1.26.4)\nCollecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (6.0.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.1->fairseq) (4.9.0)\nCollecting portalocker (from sacrebleu>=1.4.12->fairseq)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (5.2.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi->fairseq) (2.21)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->fairseq) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->fairseq) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->fairseq) (1.3.0)\nDownloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: fairseq, antlr4-python3-runtime\n  Building wheel for fairseq (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=10416014 sha256=204cb0e7bd796f60d333e88f298e2c99a5d1e7a3a0d311a4e2b827ecaf78bcf3\n  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=12986725df833fdfca09fa23ef996a3fc17d11c5fc5fb539cea0acdb71937a27\n  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\nSuccessfully built fairseq antlr4-python3-runtime\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, sacrebleu, hydra-core, fairseq\nSuccessfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pytorch-lightning","metadata":{"id":"qiIp4XKezxsC","outputId":"57446db8-d9f7-4944-cc0b-b576410823bb","execution":{"iopub.status.busy":"2024-06-14T22:57:16.042527Z","iopub.execute_input":"2024-06-14T22:57:16.043134Z","iopub.status.idle":"2024-06-14T22:57:28.403584Z","shell.execute_reply.started":"2024-06-14T22:57:16.043093Z","shell.execute_reply":"2024-06-14T22:57:28.402550Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.26.4)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (2.1.2)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (6.0.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.3.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.9.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.11.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch-lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install av","metadata":{"id":"8ldscqWJfg82","outputId":"7e50826c-fddc-4fe2-e2ac-570a374b4b1d","execution":{"iopub.status.busy":"2024-06-14T22:57:28.405127Z","iopub.execute_input":"2024-06-14T22:57:28.405525Z","iopub.status.idle":"2024-06-14T22:57:42.136664Z","shell.execute_reply.started":"2024-06-14T22:57:28.405484Z","shell.execute_reply":"2024-06-14T22:57:42.135480Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nDownloading av-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: av\nSuccessfully installed av-12.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install comet_ml","metadata":{"execution":{"iopub.status.busy":"2024-06-14T22:57:42.138289Z","iopub.execute_input":"2024-06-14T22:57:42.138664Z","iopub.status.idle":"2024-06-14T22:57:56.867388Z","shell.execute_reply.started":"2024-06-14T22:57:42.138627Z","shell.execute_reply":"2024-06-14T22:57:56.866297Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting comet_ml\n  Downloading comet_ml-3.43.1-py3-none-any.whl.metadata (3.9 kB)\nCollecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (4.20.0)\nRequirement already satisfied: psutil>=5.6.3 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (5.9.3)\nCollecting python-box<7.0.0 (from comet_ml)\n  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\nRequirement already satisfied: requests-toolbelt>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (0.10.1)\nRequirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (2.31.0)\nCollecting semantic-version>=2.8.0 (from comet_ml)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: sentry-sdk>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.45.0)\nRequirement already satisfied: simplejson in /opt/conda/lib/python3.10/site-packages (from comet_ml) (3.19.2)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.26.18)\nRequirement already satisfied: wrapt>=1.11.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.14.1)\nCollecting wurlitzer>=1.0.2 (from comet_ml)\n  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n  Downloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: rich>=13.3.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (13.7.0)\nCollecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n  Downloading configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.16.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.3.2->comet_ml) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.3.2->comet_ml) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from configobj->everett[ini]<3.2.0,>=1.0.1->comet_ml) (1.16.0)\nDownloading comet_ml-3.43.1-py3-none-any.whl (675 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m675.8/675.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (979 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.1/979.1 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\nDownloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\nDownloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: everett, wurlitzer, semantic-version, python-box, dulwich, configobj, comet_ml\nSuccessfully installed comet_ml-3.43.1 configobj-5.0.8 dulwich-0.22.1 everett-3.1.0 python-box-6.1.0 semantic-version-2.10.0 wurlitzer-3.1.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Librerias","metadata":{"id":"GUufWC2jsdEW"}},{"cell_type":"code","source":"import os\nimport hydra\nfrom omegaconf import OmegaConf, DictConfig\nimport logging\nimport comet_ml\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.nn.utils.rnn import pad_sequence\nimport torchaudio\nimport torchvision\nfrom torch.utils.data import Dataset, DistributedSampler, RandomSampler\nfrom torch.utils.data.sampler import Sampler\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning import LightningDataModule\nfrom pytorch_lightning import seed_everything, Trainer\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import CometLogger\nfrom pytorch_lightning.utilities.model_summary import ModelSummary\n#from pytorch_lightning.plugins import DDPPlugin\n\nfrom fairseq.data import data_utils\n\nimport numpy as np\nimport math\nimport random\nimport sentencepiece\nfrom typing import Iterator, Optional\nfrom operator import itemgetter\nimport warnings\nfrom typing import Any, List, Tuple, Dict, NamedTuple, Union\nimport six\nfrom itertools import chain\nimport copy","metadata":{"id":"7-vB2bNLtIeK","execution":{"iopub.status.busy":"2024-06-14T22:57:56.868732Z","iopub.execute_input":"2024-06-14T22:57:56.869006Z","iopub.status.idle":"2024-06-14T22:58:40.411807Z","shell.execute_reply.started":"2024-06-14T22:57:56.868979Z","shell.execute_reply":"2024-06-14T22:58:40.410934Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-06-14 22:58:08.229443: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-14 22:58:08.229581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-14 22:58:08.363777: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# DataModule (Dataset)","metadata":{"id":"6LDIcdKAwaHC"}},{"cell_type":"markdown","source":"## AVDataset","metadata":{"id":"N_Y-Polmt8de"}},{"cell_type":"code","source":"def cut_or_pad(data, size, dim=0):\n    \"\"\"\n    Pads or trims the data along a dimension.\n    \"\"\"\n    if data.size(dim) < size:\n        padding = size - data.size(dim)\n        data = torch.nn.functional.pad(data, (0, 0, 0, padding), \"constant\")\n        size = data.size(dim)\n    elif data.size(dim) > size:\n        data = data[:size]\n    assert data.size(dim) == size\n    return data\n\n\ndef load_video(path):\n    \"\"\"\n    rtype: torch, T x C x H x W\n    \"\"\"\n    vid = torchvision.io.read_video(path, pts_unit=\"sec\", output_format=\"THWC\")[0]\n    vid = vid.permute((0, 3, 1, 2))\n    return vid\n\n\ndef load_audio(path):\n    \"\"\"\n    rtype: torch, T x 1\n    \"\"\"\n    waveform, sample_rate = torchaudio.load(path[:-4] + \".wav\", normalize=True)\n    return waveform.transpose(1, 0)\n\n\nclass AVDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        root_dir,\n        label_path,\n        subset,\n        modality,\n        audio_transform,\n        video_transform,\n        rate_ratio=640,\n    ):\n\n        self.root_dir = root_dir\n\n        self.modality = modality\n        self.rate_ratio = rate_ratio\n\n        self.list = self.load_list(label_path)\n\n        self.audio_transform = audio_transform\n        self.video_transform = video_transform\n\n    def load_list(self, label_path):\n        paths_counts_labels = []\n        for path_count_label in open(label_path).read().splitlines()[1:]:\n            id, dataset_name, rel_path, input_length, token_id = path_count_label.split(\",\")\n            #print(input_length)\n            paths_counts_labels.append(\n                (\n                    dataset_name,\n                    rel_path,\n                    int(input_length),\n                    torch.tensor([int(_) for _ in token_id.split()]),\n                )\n            )\n        return paths_counts_labels\n\n    def __getitem__(self, idx):\n        dataset_name, rel_path, input_length, token_id = self.list[idx]\n        path = os.path.join(self.root_dir, dataset_name, rel_path)\n        if self.modality == \"video\":\n            video = load_video(path)\n            video = self.video_transform(video)\n            return {\"input\": video, \"target\": token_id}\n        elif self.modality == \"audio\":\n            audio = load_audio(path)\n            audio = self.audio_transform(audio)\n            return {\"input\": audio, \"target\": token_id}\n        elif self.modality == \"audiovisual\":\n            video = load_video(path)\n            audio = load_audio(path)\n            audio = cut_or_pad(audio, len(video) * self.rate_ratio)\n            video = self.video_transform(video)\n            audio = self.audio_transform(audio)\n            return {\"video\": video, \"audio\": audio, \"target\": token_id}\n\n    def __len__(self):\n        return len(self.list)","metadata":{"id":"ZLIGNqVUt_i9","execution":{"iopub.status.busy":"2024-06-14T22:59:18.545105Z","iopub.execute_input":"2024-06-14T22:59:18.545909Z","iopub.status.idle":"2024-06-14T22:59:18.562634Z","shell.execute_reply.started":"2024-06-14T22:59:18.545875Z","shell.execute_reply":"2024-06-14T22:59:18.561493Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Tranformaciones","metadata":{"id":"PdZn0dVwu-dF"}},{"cell_type":"code","source":"NOISE_FILENAME = os.path.join(\n    os.path.dirname(os.path.abspath('')),\n    \"working\",\n    \"auto_avsr\",\n    \"datamodule\",\n    \"babble_noise.wav\"\n)\n\nSP_MODEL_PATH = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n    \"kaggle\",\n    \"input\",\n    \"sentencepiece_castellano\",\n    \"other\",\n    \"v2\",\n    \"3\",\n    \"unigram100.model\",\n)\n\nDICT_PATH = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n    \"kaggle\",\n    \"input\",\n    \"sentencepiece_castellano\",\n    \"other\",\n    \"v2\",\n    \"3\",\n    \"unigram100_units.txt\",\n)\n\n\nclass FunctionalModule(torch.nn.Module):\n    def __init__(self, functional):\n        super().__init__()\n        self.functional = functional\n\n    def forward(self, input):\n        return self.functional(input)\n\n\nclass AdaptiveTimeMask(torch.nn.Module):\n    def __init__(self, window, stride):\n        super().__init__()\n        self.window = window\n        self.stride = stride\n\n    def forward(self, x):\n        # x: [T, ...]\n        cloned = x.clone()\n        length = cloned.size(0)\n        n_mask = int((length + self.stride - 0.1) // self.stride)\n        ts = torch.randint(0, self.window, size=(n_mask, 2))\n        for t, t_end in ts:\n            if length - t <= 0:\n                continue\n            t_start = random.randrange(0, length - t)\n            if t_start == t_start + t:\n                continue\n            t_end += t_start\n            cloned[t_start:t_end] = 0\n        return cloned\n\n\nclass AddNoise(torch.nn.Module):\n    def __init__(\n        self,\n        noise_filename=NOISE_FILENAME,\n        snr_target=None,\n    ):\n        super().__init__()\n        self.snr_levels = [snr_target] if snr_target else [-5, 0, 5, 10, 15, 20, 999999]\n        self.noise, sample_rate = torchaudio.load(noise_filename)\n        assert sample_rate == 16000\n\n    def forward(self, speech):\n        # speech: T x 1\n        # return: T x 1\n        speech = speech.t()\n        start_idx = random.randint(0, self.noise.shape[1] - speech.shape[1])\n        noise_segment = self.noise[:, start_idx : start_idx + speech.shape[1]]\n        snr_level = torch.tensor([random.choice(self.snr_levels)])\n        noisy_speech = torchaudio.functional.add_noise(speech, noise_segment, snr_level)\n        return noisy_speech.t()\n\n\nclass VideoTransform:\n    def __init__(self, subset):\n        if subset == \"train\":\n            self.video_pipeline = torch.nn.Sequential(\n                FunctionalModule(lambda x: x / 255.0),\n                torchvision.transforms.RandomCrop(88),\n                torchvision.transforms.Grayscale(),\n                AdaptiveTimeMask(10, 25),\n                torchvision.transforms.Normalize(0.421, 0.165),\n            )\n        elif subset == \"val\" or subset == \"test\":\n            self.video_pipeline = torch.nn.Sequential(\n                FunctionalModule(lambda x: x / 255.0),\n                torchvision.transforms.CenterCrop(88),\n                torchvision.transforms.Grayscale(),\n                torchvision.transforms.Normalize(0.421, 0.165),\n            )\n\n    def __call__(self, sample):\n        # sample: T x C x H x W\n        # rtype: T x 1 x H x W\n        return self.video_pipeline(sample)\n\n\nclass AudioTransform:\n    def __init__(self, subset, snr_target=None):\n        if subset == \"train\":\n            self.audio_pipeline = torch.nn.Sequential(\n                AdaptiveTimeMask(6400, 16000),\n                AddNoise(),\n                FunctionalModule(\n                    lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=1e-8)\n                ),\n            )\n        elif subset == \"val\" or subset == \"test\":\n            self.audio_pipeline = torch.nn.Sequential(\n                AddNoise(snr_target=snr_target)\n                if snr_target is not None\n                else FunctionalModule(lambda x: x),\n                FunctionalModule(\n                    lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=1e-8)\n                ),\n            )\n\n    def __call__(self, sample):\n        # sample: T x 1\n        # rtype: T x 1\n        return self.audio_pipeline(sample)\n\n\nclass TextTransform:\n    \"\"\"Mapping Dictionary Class for SentencePiece tokenization.\"\"\"\n\n    def __init__(\n        self,\n        sp_model_path=SP_MODEL_PATH,\n        dict_path=DICT_PATH,\n    ):\n\n        # Load SentencePiece model\n        self.spm = sentencepiece.SentencePieceProcessor(model_file=sp_model_path)\n\n        # Load units and create dictionary\n        units = open(dict_path, encoding='utf8').read().splitlines()\n        self.hashmap = {unit.split()[0]: unit.split()[-1] for unit in units}\n        # 0 will be used for \"blank\" in CTC\n        self.token_list = [\"<blank>\"] + list(self.hashmap.keys()) + [\"<eos>\"]\n        self.ignore_id = -1\n\n    def tokenize(self, text):\n        tokens = self.spm.EncodeAsPieces(text)\n        token_ids = [self.hashmap.get(token, self.hashmap[\"<unk>\"]) for token in tokens]\n        return torch.tensor(list(map(int, token_ids)))\n\n    def post_process(self, token_ids):\n        token_ids = token_ids[token_ids != -1]\n        text = self._ids_to_str(token_ids, self.token_list)\n        text = text.replace(\"\\u2581\", \" \").strip()\n        return text\n\n    def _ids_to_str(self, token_ids, char_list):\n        token_as_list = [char_list[idx] for idx in token_ids]\n        return \"\".join(token_as_list).replace(\"<space>\", \" \")","metadata":{"id":"C_o27zBmvAek","execution":{"iopub.status.busy":"2024-06-14T22:59:19.911502Z","iopub.execute_input":"2024-06-14T22:59:19.912109Z","iopub.status.idle":"2024-06-14T22:59:19.940700Z","shell.execute_reply.started":"2024-06-14T22:59:19.912076Z","shell.execute_reply":"2024-06-14T22:59:19.939715Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## ByFrameCountSampler","metadata":{"id":"cgj1aC53vk09"}},{"cell_type":"code","source":"class ByFrameCountSampler(Sampler):\n    def __init__(self, dataset, max_frames_per_gpu, shuffle=True, seed=0):\n        self.dataset = dataset\n        self.max_frames_per_gpu = max_frames_per_gpu\n        self.sizes = [item[2] for item in self.dataset.list]\n\n        self.shuffle = shuffle\n        self.seed = seed\n        self.epoch = 0\n\n        batch_indices = data_utils.batch_by_size(\n            self._get_indices(), lambda i: self.sizes[i], max_tokens=max_frames_per_gpu\n        )\n        self.num_batches = len(batch_indices)\n\n    def _get_indices(self):\n        if self.shuffle:  # shuffles indices corresponding to equal lengths\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            order = [torch.randperm(len(self.dataset), generator=g).tolist()]\n        else:\n            order = [list(range(len(self.dataset)))]\n        order.append(self.sizes)\n        return np.lexsort(order)[::-1]\n\n    def __len__(self):\n        return self.num_batches\n\n    def __iter__(self):\n        batch_indices = data_utils.batch_by_size(\n            self._get_indices(),\n            lambda i: self.sizes[i],\n            max_tokens=self.max_frames_per_gpu,\n        )\n        return iter(batch_indices)\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch","metadata":{"id":"zq2myP33vlTg","execution":{"iopub.status.busy":"2024-06-14T22:59:20.881020Z","iopub.execute_input":"2024-06-14T22:59:20.881850Z","iopub.status.idle":"2024-06-14T22:59:20.892379Z","shell.execute_reply.started":"2024-06-14T22:59:20.881816Z","shell.execute_reply":"2024-06-14T22:59:20.891516Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## DatasetFromSampler","metadata":{"id":"EB1W0yKNwuDQ"}},{"cell_type":"code","source":"class DatasetFromSampler(Dataset):\n    \"\"\"Dataset to create indexes from `Sampler`.\n    Args:\n        sampler: PyTorch sampler\n    \"\"\"\n\n    def __init__(self, sampler: Sampler):\n        \"\"\"Initialisation for DatasetFromSampler.\"\"\"\n        self.sampler = sampler\n        self.sampler_list = None\n\n    def __getitem__(self, index: int):\n        \"\"\"Gets element of the dataset.\n        Args:\n            index: index of the element in the dataset\n        Returns:\n            Single element by index\n        \"\"\"\n        if self.sampler_list is None:\n            self.sampler_list = list(self.sampler)\n        return self.sampler_list[index]\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns:\n            int: length of the dataset\n        \"\"\"\n        return len(self.sampler)\n","metadata":{"id":"Mwmb5zDLwu_e","execution":{"iopub.status.busy":"2024-06-14T22:59:21.941957Z","iopub.execute_input":"2024-06-14T22:59:21.942307Z","iopub.status.idle":"2024-06-14T22:59:21.948817Z","shell.execute_reply.started":"2024-06-14T22:59:21.942277Z","shell.execute_reply":"2024-06-14T22:59:21.947916Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## DistributedSamplerWrapper","metadata":{"id":"pqTbNgn_wUkN"}},{"cell_type":"code","source":"class DistributedSamplerWrapper(DistributedSampler):\n    \"\"\"\n    Wrapper over `Sampler` for distributed training.\n    Allows you to use any sampler in distributed mode.\n    It is especially useful in conjunction with\n    `torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSamplerWrapper instance as a DataLoader\n    sampler, and load a subset of subsampled data of the original dataset\n    that is exclusive to it.\n    .. note::\n        Sampler is assumed to be of constant size.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler,\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = True,\n        drop_last: bool = False,\n    ):\n        \"\"\"\n        Args:\n            sampler: Sampler used for subsampling\n            num_replicas (int, optional): Number of processes participating in\n                distributed training\n            rank (int, optional): Rank of the current process\n                within ``num_replicas``\n            shuffle (bool, optional): If true (default),\n                sampler will shuffle the indices\n        \"\"\"\n        super(DistributedSamplerWrapper, self).__init__(\n            DatasetFromSampler(sampler),\n            num_replicas=num_replicas,\n            rank=rank,\n            shuffle=shuffle,\n            drop_last=drop_last,\n        )\n        self.sampler = sampler\n\n    def __iter__(self) -> Iterator[int]:\n        \"\"\"Iterate over sampler.\n        Returns:\n            python iterator\n        \"\"\"\n        self.dataset = DatasetFromSampler(self.sampler)\n        indexes_of_indexes = super().__iter__()\n\n        subsampler_indexes = self.dataset\n        return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n        self.sampler.set_epoch(epoch)","metadata":{"id":"IyT2ML3IwTx7","execution":{"iopub.status.busy":"2024-06-14T22:59:22.988248Z","iopub.execute_input":"2024-06-14T22:59:22.988605Z","iopub.status.idle":"2024-06-14T22:59:22.997088Z","shell.execute_reply.started":"2024-06-14T22:59:22.988578Z","shell.execute_reply":"2024-06-14T22:59:22.996248Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## RandomSamplerWrapper","metadata":{"id":"twaDpetGxIYG"}},{"cell_type":"code","source":"class RandomSamplerWrapper(RandomSampler):\n    def __init__(self, sampler):\n        super(RandomSamplerWrapper, self).__init__(DatasetFromSampler(sampler))\n        self.sampler = sampler\n\n    def __iter__(self) -> Iterator[int]:\n        \"\"\"Iterate over sampler.\n        Returns:\n            python iterator\n        \"\"\"\n        self.dataset = DatasetFromSampler(self.sampler)\n        indexes_of_indexes = super().__iter__()\n        subsampler_indexes = self.dataset\n        return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))","metadata":{"id":"roBfR_dCxJbd","execution":{"iopub.status.busy":"2024-06-14T22:59:23.962057Z","iopub.execute_input":"2024-06-14T22:59:23.962946Z","iopub.status.idle":"2024-06-14T22:59:23.969039Z","shell.execute_reply.started":"2024-06-14T22:59:23.962913Z","shell.execute_reply":"2024-06-14T22:59:23.967935Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## DataModule","metadata":{"id":"lTz0BtKMwMdz"}},{"cell_type":"code","source":"def pad(samples, pad_val=0.0):\n    lengths = [len(s) for s in samples]\n    max_size = max(lengths)\n    sample_shape = list(samples[0].shape[1:])\n    collated_batch = samples[0].new_zeros([len(samples), max_size] + sample_shape)\n    for i, sample in enumerate(samples):\n        diff = len(sample) - max_size\n        if diff == 0:\n            collated_batch[i] = sample\n        else:\n            collated_batch[i] = torch.cat(\n                [sample, sample.new_full([-diff] + sample_shape, pad_val)]\n            )\n    if len(samples[0].shape) == 1:\n        collated_batch = collated_batch.unsqueeze(1)  # targets\n    elif len(samples[0].shape) == 2:\n        pass  # collated_batch: [B, T, 1]\n    elif len(samples[0].shape) == 4:\n        pass  # collated_batch: [B, T, C, H, W]\n    return collated_batch, lengths\n\ndef collate_pad(batch):\n    batch_out = {}\n    for data_type in batch[0].keys():\n        pad_val = -1 if data_type == \"target\" else 0.0\n        c_batch, sample_lengths = pad(\n            [s[data_type] for s in batch if s[data_type] is not None], pad_val\n        )\n        batch_out[data_type + \"s\"] = c_batch\n        batch_out[data_type + \"_lengths\"] = torch.tensor(sample_lengths)\n    return batch_out\n\n\nclass DataModule(LightningDataModule):\n    def __init__(self, cfg=None):\n        super().__init__()\n        self.cfg = cfg\n        self.cfg.gpus = torch.cuda.device_count()\n        self.total_gpus = self.cfg.gpus * self.cfg.trainer.num_nodes\n        print('Numero de gpus',self.total_gpus)\n\n    def _dataloader(self, ds, sampler, collate_fn):\n        return torch.utils.data.DataLoader(\n            ds,\n            num_workers=0,\n            pin_memory=True,\n            batch_sampler=sampler,\n            collate_fn=collate_fn,\n        )\n\n    def train_dataloader(self):\n        ds_args = self.cfg.data.dataset\n        train_ds = AVDataset(\n            root_dir=ds_args.root_dir,\n            label_path=os.path.join(\n                ds_args.root_dir, ds_args.label_dir, ds_args.train_file\n            ),\n            subset=\"train\",\n            modality=self.cfg.data.modality,\n            audio_transform=AudioTransform(\"train\"),\n            video_transform=VideoTransform(\"train\"),\n        )\n        print(f'num_train:{len(train_ds)}')\n        sampler = ByFrameCountSampler(train_ds, self.cfg.data.max_frames)\n        if self.total_gpus > 1:\n            sampler = DistributedSamplerWrapper(sampler)\n        else:\n            sampler = RandomSamplerWrapper(sampler)\n        return self._dataloader(train_ds, sampler, collate_pad)\n\n    def val_dataloader(self):\n        ds_args = self.cfg.data.dataset\n        val_ds = AVDataset(\n            root_dir=ds_args.root_dir,\n            label_path=os.path.join(\n                ds_args.root_dir, ds_args.label_dir, ds_args.val_file\n            ),\n            subset=\"val\",\n            modality=self.cfg.data.modality,\n            audio_transform=AudioTransform(\"val\"),\n            video_transform=VideoTransform(\"val\"),\n        )\n        print(f'num_val:{len(val_ds)}')\n        sampler = ByFrameCountSampler(\n            val_ds, self.cfg.data.max_frames_val, shuffle=False\n        )\n        if self.total_gpus > 1:\n            sampler = DistributedSamplerWrapper(sampler, shuffle=False, drop_last=True)\n        return self._dataloader(val_ds, sampler, collate_pad)\n\n    def test_dataloader(self):\n        ds_args = self.cfg.data.dataset\n        dataset = AVDataset(\n            root_dir=ds_args.root_dir,\n            label_path=os.path.join(\n                ds_args.root_dir, ds_args.label_dir, ds_args.test_file\n            ),\n            subset=\"test\",\n            modality=self.cfg.data.modality,\n            audio_transform=AudioTransform(\n                \"test\", snr_target=self.cfg.decode.snr_target\n            ),\n            video_transform=VideoTransform(\"test\"),\n        )\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=None)\n        return dataloader","metadata":{"id":"F2mb_uPzs-g1","execution":{"iopub.status.busy":"2024-06-14T22:59:24.942883Z","iopub.execute_input":"2024-06-14T22:59:24.943721Z","iopub.status.idle":"2024-06-14T22:59:24.963913Z","shell.execute_reply.started":"2024-06-14T22:59:24.943685Z","shell.execute_reply":"2024-06-14T22:59:24.962782Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# ModelModule (Modelo)","metadata":{"id":"SDhdr-AvzGi6"}},{"cell_type":"markdown","source":"## RelPositionalEncoding","metadata":{"id":"IEGOCDxVe4qB"}},{"cell_type":"code","source":"class RelPositionalEncoding(torch.nn.Module):\n    \"\"\"Relative positional encoding module (new implementation).\n    Details can be found in https://github.com/espnet/espnet/pull/2816.\n    See : Appendix B in https://arxiv.org/abs/1901.02860\n    Args:\n        d_model (int): Embedding dimension.\n        dropout_rate (float): Dropout rate.\n        max_len (int): Maximum input length.\n    \"\"\"\n\n    def __init__(self, d_model, dropout_rate, max_len=5000):\n        \"\"\"Construct an PositionalEncoding object.\"\"\"\n        super(RelPositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.xscale = math.sqrt(self.d_model)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.pe = None\n        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n\n    def extend_pe(self, x):\n        \"\"\"Reset the positional encodings.\"\"\"\n        if self.pe is not None:\n            # self.pe contains both positive and negative parts\n            # the length of self.pe is 2 * input_len - 1\n            if self.pe.size(1) >= x.size(1) * 2 - 1:\n                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n                return\n        # Suppose `i` means to the position of query vecotr and `j` means the\n        # position of key vector. We use position relative positions when keys\n        # are to the left (i>j) and negative relative positions otherwise (i<j).\n        pe_positive = torch.zeros(x.size(1), self.d_model)\n        pe_negative = torch.zeros(x.size(1), self.d_model)\n        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n            * -(math.log(10000.0) / self.d_model)\n        )\n        pe_positive[:, 0::2] = torch.sin(position * div_term)\n        pe_positive[:, 1::2] = torch.cos(position * div_term)\n        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)\n        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)\n\n        # Reserve the order of positive indices and concat both positive and\n        # negative indices. This is used to support the shifting trick\n        # as in https://arxiv.org/abs/1901.02860\n        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n        pe_negative = pe_negative[1:].unsqueeze(0)\n        pe = torch.cat([pe_positive, pe_negative], dim=1)\n        self.pe = pe.to(device=x.device, dtype=x.dtype)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Add positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n        \"\"\"\n        self.extend_pe(x)\n        x = x * self.xscale\n        pos_emb = self.pe[\n            :,\n            self.pe.size(1) // 2 - x.size(1) + 1 : self.pe.size(1) // 2 + x.size(1),\n        ]\n        return self.dropout(x), self.dropout(pos_emb)","metadata":{"id":"whTGzHZNe51i","execution":{"iopub.status.busy":"2024-06-14T22:59:26.309256Z","iopub.execute_input":"2024-06-14T22:59:26.309745Z","iopub.status.idle":"2024-06-14T22:59:26.325061Z","shell.execute_reply.started":"2024-06-14T22:59:26.309712Z","shell.execute_reply":"2024-06-14T22:59:26.324042Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Swish","metadata":{"id":"exinCbGXgAw4"}},{"cell_type":"code","source":"class Swish(nn.Module):\n    \"\"\"Construct an Swish object.\"\"\"\n\n    def forward(self, x):\n        \"\"\"Return Swich activation function.\"\"\"\n        return x * torch.sigmoid(x)","metadata":{"id":"ZSQU5guKfWUo","execution":{"iopub.status.busy":"2024-06-14T22:59:27.466204Z","iopub.execute_input":"2024-06-14T22:59:27.466892Z","iopub.status.idle":"2024-06-14T22:59:27.471563Z","shell.execute_reply.started":"2024-06-14T22:59:27.466861Z","shell.execute_reply":"2024-06-14T22:59:27.470498Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## ResNet 2D","metadata":{"id":"Oj8NjiA_f8Ny"}},{"cell_type":"code","source":"def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"conv3x3.\n\n    :param in_planes: int, number of channels in the input sequence.\n    :param out_planes: int,  number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False,\n    )\n\n\ndef downsample_basic_block(inplanes, outplanes, stride):\n    \"\"\"downsample_basic_block.\n\n    :param inplanes: int, number of channels in the input sequence.\n    :param outplanes: int, number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(\n            inplanes,\n            outplanes,\n            kernel_size=1,\n            stride=stride,\n            bias=False,\n        ),\n        nn.BatchNorm2d(outplanes),\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        downsample=None,\n        relu_type=\"swish\",\n    ):\n        \"\"\"__init__.\n\n        :param inplanes: int, number of channels in the input sequence.\n        :param planes: int,  number of channels produced by the convolution.\n        :param stride: int, size of the convolving kernel.\n        :param downsample: boolean, if True, the temporal resolution is downsampled.\n        :param relu_type: str, type of activation function.\n        \"\"\"\n        super(BasicBlock, self).__init__()\n\n        assert relu_type in [\"relu\", \"prelu\", \"swish\"]\n\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        if relu_type == \"relu\":\n            self.relu1 = nn.ReLU(inplace=True)\n            self.relu2 = nn.ReLU(inplace=True)\n        elif relu_type == \"prelu\":\n            self.relu1 = nn.PReLU(num_parameters=planes)\n            self.relu2 = nn.PReLU(num_parameters=planes)\n        elif relu_type == \"swish\":\n            self.relu1 = Swish()\n            self.relu2 = Swish()\n        else:\n            raise NotImplementedError\n        # --------\n\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T, H, W).\n        \"\"\"\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu2(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        block,\n        layers,\n        relu_type=\"swish\",\n    ):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.relu_type = relu_type\n        self.downsample_block = downsample_basic_block\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        \"\"\"_make_layer.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param planes: int,  number of channels produced by the convolution.\n        :param blocks: int, number of layers in a block.\n        :param stride: int, size of the convolving kernel.\n        \"\"\"\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = self.downsample_block(\n                inplanes=self.inplanes,\n                outplanes=planes * block.expansion,\n                stride=stride,\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.inplanes,\n                planes,\n                stride,\n                downsample,\n                relu_type=self.relu_type,\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    relu_type=self.relu_type,\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T, H, W).\n        \"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        return x","metadata":{"id":"E3WrvoRKf9U3","execution":{"iopub.status.busy":"2024-06-14T22:59:28.468235Z","iopub.execute_input":"2024-06-14T22:59:28.468616Z","iopub.status.idle":"2024-06-14T22:59:28.491056Z","shell.execute_reply.started":"2024-06-14T22:59:28.468586Z","shell.execute_reply":"2024-06-14T22:59:28.490032Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## ResNet 1D","metadata":{"id":"bM0t1pzbf9nX"}},{"cell_type":"code","source":"def conv3x31D(in_planes, out_planes, stride=1):\n    \"\"\"conv3x3.\n\n    :param in_planes: int, number of channels in the input sequence.\n    :param out_planes: int,  number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"\n    return nn.Conv1d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False,\n    )\n\n\ndef downsample_basic_block1D(inplanes, outplanes, stride):\n    \"\"\"downsample_basic_block.\n\n    :param inplanes: int, number of channels in the input sequence.\n    :param outplanes: int, number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv1d(\n            inplanes,\n            outplanes,\n            kernel_size=1,\n            stride=stride,\n            bias=False,\n        ),\n        nn.BatchNorm1d(outplanes),\n    )\n\n\nclass BasicBlock1D(nn.Module):\n    expansion = 1\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        downsample=None,\n        relu_type=\"relu\",\n    ):\n        \"\"\"__init__.\n\n        :param inplanes: int, number of channels in the input sequence.\n        :param planes: int,  number of channels produced by the convolution.\n        :param stride: int, size of the convolving kernel.\n        :param downsample: boolean, if True, the temporal resolution is downsampled.\n        :param relu_type: str, type of activation function.\n        \"\"\"\n        super(BasicBlock1D, self).__init__()\n\n        assert relu_type in [\"relu\", \"prelu\", \"swish\"]\n\n        self.conv1 = conv3x31D(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm1d(planes)\n\n        # type of ReLU is an input option\n        if relu_type == \"relu\":\n            self.relu1 = nn.ReLU(inplace=True)\n            self.relu2 = nn.ReLU(inplace=True)\n        elif relu_type == \"prelu\":\n            self.relu1 = nn.PReLU(num_parameters=planes)\n            self.relu2 = nn.PReLU(num_parameters=planes)\n        elif relu_type == \"swish\":\n            self.relu1 = Swish()\n            self.relu2 = Swish()\n        else:\n            raise NotImplementedError\n        # --------\n\n        self.conv2 = conv3x31D(planes, planes)\n        self.bn2 = nn.BatchNorm1d(planes)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T)\n        \"\"\"\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu2(out)\n\n        return out\n\n\nclass ResNet1D(nn.Module):\n    def __init__(\n        self,\n        block,\n        layers,\n        relu_type=\"swish\",\n        a_upsample_ratio=1,\n    ):\n        \"\"\"__init__.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param layers: List, customised layers in each block.\n        :param relu_type: str, type of activation function.\n        :param a_upsample_ratio: int, The ratio related to the \\\n            temporal resolution of output features of the frontend. \\\n            a_upsample_ratio=1 produce features with a fps of 25.\n        \"\"\"\n        super(ResNet1D, self).__init__()\n        self.inplanes = 64\n        self.relu_type = relu_type\n        self.downsample_block = downsample_basic_block1D\n        self.a_upsample_ratio = a_upsample_ratio\n\n        self.conv1 = nn.Conv1d(\n            in_channels=1,\n            out_channels=self.inplanes,\n            kernel_size=80,\n            stride=4,\n            padding=38,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm1d(self.inplanes)\n\n        if relu_type == \"relu\":\n            self.relu = nn.ReLU(inplace=True)\n        elif relu_type == \"prelu\":\n            self.relu = nn.PReLU(num_parameters=self.inplanes)\n        elif relu_type == \"swish\":\n            self.relu = Swish()\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool1d(\n            kernel_size=20 // self.a_upsample_ratio,\n            stride=20 // self.a_upsample_ratio,\n        )\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        \"\"\"_make_layer.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param planes: int,  number of channels produced by the convolution.\n        :param blocks: int, number of layers in a block.\n        :param stride: int, size of the convolving kernel.\n        \"\"\"\n\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = self.downsample_block(\n                inplanes=self.inplanes,\n                outplanes=planes * block.expansion,\n                stride=stride,\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.inplanes,\n                planes,\n                stride,\n                downsample,\n                relu_type=self.relu_type,\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    relu_type=self.relu_type,\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        return x","metadata":{"id":"6-OF93Qpf_ZO","execution":{"iopub.status.busy":"2024-06-14T22:59:29.357079Z","iopub.execute_input":"2024-06-14T22:59:29.357473Z","iopub.status.idle":"2024-06-14T22:59:29.381899Z","shell.execute_reply.started":"2024-06-14T22:59:29.357427Z","shell.execute_reply":"2024-06-14T22:59:29.380965Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Conv3dResNet","metadata":{"id":"Y-xkRUYSfc5x"}},{"cell_type":"code","source":"def threeD_to_2D_tensor(x):\n    n_batch, n_channels, s_time, sx, sy = x.shape\n    x = x.transpose(1, 2)\n    return x.reshape(n_batch * s_time, n_channels, sx, sy)\n\n\nclass Conv3dResNet(torch.nn.Module):\n    \"\"\"Conv3dResNet module\"\"\"\n\n    def __init__(self, backbone_type=\"resnet\", relu_type=\"swish\"):\n        \"\"\"__init__.\n\n        :param backbone_type: str, the type of a visual front-end.\n        :param relu_type: str, activation function used in an audio front-end.\n        \"\"\"\n        super(Conv3dResNet, self).__init__()\n        self.frontend_nout = 64\n        self.trunk = ResNet(BasicBlock, [2, 2, 2, 2], relu_type=relu_type)\n        self.frontend3D = nn.Sequential(\n            nn.Conv3d(\n                1, self.frontend_nout, (5, 7, 7), (1, 2, 2), (2, 3, 3), bias=False\n            ),\n            nn.BatchNorm3d(self.frontend_nout),\n            Swish(),\n            nn.MaxPool3d((1, 3, 3), (1, 2, 2), (0, 1, 1)),\n        )\n\n    def forward(self, xs_pad):\n        xs_pad = xs_pad.transpose(1, 2)  # [B, T, C, H, W] -> [B, C, T, H, W]\n\n        B, C, T, H, W = xs_pad.size()\n        xs_pad = self.frontend3D(xs_pad)\n        Tnew = xs_pad.shape[2]\n        xs_pad = threeD_to_2D_tensor(xs_pad)\n        xs_pad = self.trunk(xs_pad)\n        return xs_pad.view(B, Tnew, xs_pad.size(1))","metadata":{"id":"jMIj_0NBfsEC","execution":{"iopub.status.busy":"2024-06-14T22:59:30.306367Z","iopub.execute_input":"2024-06-14T22:59:30.306719Z","iopub.status.idle":"2024-06-14T22:59:30.318034Z","shell.execute_reply.started":"2024-06-14T22:59:30.306694Z","shell.execute_reply":"2024-06-14T22:59:30.315970Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Conv1dResNet","metadata":{"id":"CRX9rtXHfd2l"}},{"cell_type":"code","source":"class Conv1dResNet(torch.nn.Module):\n    def __init__(self, relu_type=\"swish\", a_upsample_ratio=1):\n        super().__init__()\n        self.a_upsample_ratio = a_upsample_ratio\n        self.trunk = ResNet1D(\n            BasicBlock1D,\n            [2, 2, 2, 2],\n            relu_type=relu_type,\n            a_upsample_ratio=a_upsample_ratio,\n        )\n\n    def forward(self, xs_pad):\n        \"\"\"forward.\n\n        :param xs_pad: torch.Tensor, batch of padded input sequences (B, Tmax, idim)\n        \"\"\"\n        B, T, C = xs_pad.size()\n        xs_pad = xs_pad[:, : T // 640 * 640, :]\n        xs_pad = xs_pad.transpose(1, 2)\n        xs_pad = self.trunk(xs_pad)\n        return xs_pad.transpose(1, 2)","metadata":{"id":"a1d5lvfEhFg_","execution":{"iopub.status.busy":"2024-06-14T22:59:31.287397Z","iopub.execute_input":"2024-06-14T22:59:31.287787Z","iopub.status.idle":"2024-06-14T22:59:31.295094Z","shell.execute_reply.started":"2024-06-14T22:59:31.287757Z","shell.execute_reply":"2024-06-14T22:59:31.293971Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## MultiHeadedAttention y RelPositionMultiHeadedAttention","metadata":{"id":"azqaGmtUj3Dw"}},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    \"\"\"Multi-Head Attention layer.\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n    \"\"\"\n\n    def __init__(self, n_head, n_feat, dropout_rate):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super(MultiHeadedAttention, self).__init__()\n        assert n_feat % n_head == 0\n        # We assume d_v always equals d_k\n        self.d_k = n_feat // n_head\n        self.h = n_head\n        self.linear_q = nn.Linear(n_feat, n_feat)\n        self.linear_k = nn.Linear(n_feat, n_feat)\n        self.linear_v = nn.Linear(n_feat, n_feat)\n        self.linear_out = nn.Linear(n_feat, n_feat)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n    def forward_qkv(self, query, key, value):\n        \"\"\"Transform query, key and value.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n        Returns:\n            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).\n            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).\n            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\n        \"\"\"\n        n_batch = query.size(0)\n        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n\n        return q, k, v\n\n    def forward_attention(self, value, scores, mask, rtn_attn=False):\n        \"\"\"Compute attention context vector.\n        Args:\n            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).\n            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).\n            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).\n            rtn_attn (boolean): Flag of return attention score\n        Returns:\n            torch.Tensor: Transformed value (#batch, time1, d_model)\n                weighted by the attention score (#batch, time1, time2).\n        \"\"\"\n        n_batch = value.size(0)\n        if mask is not None:\n            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)\n            min_value = float(\n                np.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n                #numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n            )\n            scores = scores.masked_fill(mask, min_value)\n            self.attn = torch.softmax(scores, dim=-1).masked_fill(\n                mask, 0.0\n            )  # (batch, head, time1, time2)\n        else:\n            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\n        p_attn = self.dropout(self.attn)\n        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n        x = (\n            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)\n        )  # (batch, time1, d_model)\n        if rtn_attn:\n            return self.linear_out(x), self.attn\n        return self.linear_out(x)  # (batch, time1, d_model)\n\n    def forward(self, query, key, value, mask, rtn_attn=False):\n        \"\"\"Compute scaled dot product attention.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n            rtn_attn (boolean): Flag of return attention score\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        return self.forward_attention(v, scores, mask, rtn_attn)\n\n\nclass LegacyRelPositionMultiHeadedAttention(MultiHeadedAttention):\n    \"\"\"Multi-Head Attention layer with relative position encoding (old version).\n    Details can be found in https://github.com/espnet/espnet/pull/2816.\n    Paper: https://arxiv.org/abs/1901.02860\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n        zero_triu (bool): Whether to zero the upper triangular part of attention matrix.\n    \"\"\"\n\n    def __init__(self, n_head, n_feat, dropout_rate, zero_triu=False):\n        \"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"\n        super().__init__(n_head, n_feat, dropout_rate)\n        self.zero_triu = zero_triu\n        # linear transformation for positional encoding\n        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)\n        # these two learnable bias are used in matrix c and matrix d\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        torch.nn.init.xavier_uniform_(self.pos_bias_u)\n        torch.nn.init.xavier_uniform_(self.pos_bias_v)\n\n    def rel_shift(self, x):\n        \"\"\"Compute relative positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, head, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=-1)\n\n        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))\n        x = x_padded[:, :, 1:].view_as(x)\n\n        if self.zero_triu:\n            ones = torch.ones((x.size(2), x.size(3)))\n            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]\n\n        return x\n\n    def forward(self, query, key, value, pos_emb, mask):\n        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            pos_emb (torch.Tensor): Positional embedding tensor (#batch, time1, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n\n        n_batch_pos = pos_emb.size(0)\n        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n        p = p.transpose(1, 2)  # (batch, head, time1, d_k)\n\n        # (batch, head, time1, d_k)\n        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n        # (batch, head, time1, d_k)\n        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n\n        # compute attention score\n        # first compute matrix a and matrix c\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        # (batch, head, time1, time2)\n        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n\n        # compute matrix b and matrix d\n        # (batch, head, time1, time1)\n        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n        matrix_bd = self.rel_shift(matrix_bd)\n\n        scores = (matrix_ac + matrix_bd) / math.sqrt(\n            self.d_k\n        )  # (batch, head, time1, time2)\n\n        return self.forward_attention(v, scores, mask)\n\n\nclass RelPositionMultiHeadedAttention(MultiHeadedAttention):\n    \"\"\"Multi-Head Attention layer with relative position encoding (new implementation).\n    Details can be found in https://github.com/espnet/espnet/pull/2816.\n    Paper: https://arxiv.org/abs/1901.02860\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n        zero_triu (bool): Whether to zero the upper triangular part of attention matrix.\n    \"\"\"\n\n    def __init__(self, n_head, n_feat, dropout_rate, zero_triu=False):\n        \"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"\n        super().__init__(n_head, n_feat, dropout_rate)\n        self.zero_triu = zero_triu\n        # linear transformation for positional encoding\n        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)\n        # these two learnable bias are used in matrix c and matrix d\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        torch.nn.init.xavier_uniform_(self.pos_bias_u)\n        torch.nn.init.xavier_uniform_(self.pos_bias_v)\n\n    def rel_shift(self, x):\n        \"\"\"Compute relative positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).\n            time1 means the length of query vector.\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=-1)\n\n        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))\n        x = x_padded[:, :, 1:].view_as(x)[\n            :, :, :, : x.size(-1) // 2 + 1\n        ]  # only keep the positions from 0 to time2\n\n        if self.zero_triu:\n            ones = torch.ones((x.size(2), x.size(3)), device=x.device)\n            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]\n\n        return x\n\n    def forward(self, query, key, value, pos_emb, mask):\n        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            pos_emb (torch.Tensor): Positional embedding tensor\n                (#batch, 2*time1-1, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n\n        n_batch_pos = pos_emb.size(0)\n        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n        p = p.transpose(1, 2)  # (batch, head, 2*time1-1, d_k)\n\n        # (batch, head, time1, d_k)\n        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n        # (batch, head, time1, d_k)\n        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n\n        # compute attention score\n        # first compute matrix a and matrix c\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        # (batch, head, time1, time2)\n        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n\n        # compute matrix b and matrix d\n        # (batch, head, time1, 2*time1-1)\n        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n        matrix_bd = self.rel_shift(matrix_bd)\n\n        scores = (matrix_ac + matrix_bd) / math.sqrt(\n            self.d_k\n        )  # (batch, head, time1, time2)\n\n        return self.forward_attention(v, scores, mask)","metadata":{"id":"m8jlXupij4_b","execution":{"iopub.status.busy":"2024-06-14T22:59:32.268038Z","iopub.execute_input":"2024-06-14T22:59:32.268950Z","iopub.status.idle":"2024-06-14T22:59:32.309314Z","shell.execute_reply.started":"2024-06-14T22:59:32.268915Z","shell.execute_reply":"2024-06-14T22:59:32.308431Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Bloques conformer","metadata":{"id":"gPL6F3hminyQ"}},{"cell_type":"markdown","source":"### PositionwiseFeedForward","metadata":{"id":"2pSgOj6Liukd"}},{"cell_type":"code","source":"class PositionwiseFeedForward(torch.nn.Module):\n    \"\"\"Positionwise feed forward layer.\n\n    :param int idim: input dimenstion\n    :param int hidden_units: number of hidden units\n    :param float dropout_rate: dropout rate\n\n    \"\"\"\n\n    def __init__(self, idim, hidden_units, dropout_rate):\n        \"\"\"Construct an PositionwiseFeedForward object.\"\"\"\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = torch.nn.Linear(idim, hidden_units)\n        self.w_2 = torch.nn.Linear(hidden_units, idim)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        \"\"\"Forward funciton.\"\"\"\n        return self.w_2(self.dropout(torch.relu(self.w_1(x))))","metadata":{"id":"vrZVGR-sivgW","execution":{"iopub.status.busy":"2024-06-14T22:59:32.974657Z","iopub.execute_input":"2024-06-14T22:59:32.975046Z","iopub.status.idle":"2024-06-14T22:59:32.981915Z","shell.execute_reply.started":"2024-06-14T22:59:32.975017Z","shell.execute_reply":"2024-06-14T22:59:32.980907Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### ConvolutionModule","metadata":{"id":"7qQLfrWejOjd"}},{"cell_type":"code","source":"class ConvolutionModule(nn.Module):\n    \"\"\"ConvolutionModule in Conformer model.\n\n    :param int channels: channels of cnn\n    :param int kernel_size: kernerl size of cnn\n\n    \"\"\"\n\n    def __init__(self, channels, kernel_size, bias=True):\n        \"\"\"Construct an ConvolutionModule object.\"\"\"\n        super(ConvolutionModule, self).__init__()\n        # kernerl_size should be a odd number for 'SAME' padding\n        assert (kernel_size - 1) % 2 == 0\n\n        self.pointwise_cov1 = nn.Conv1d(\n            channels,\n            2 * channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.depthwise_conv = nn.Conv1d(\n            channels,\n            channels,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n            groups=channels,\n            bias=bias,\n        )\n        self.norm = nn.BatchNorm1d(channels)\n        self.pointwise_cov2 = nn.Conv1d(\n            channels,\n            channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.activation = Swish()\n\n    def forward(self, x):\n        \"\"\"Compute covolution module.\n\n        :param torch.Tensor x: (batch, time, size)\n        :return torch.Tensor: convoluted `value` (batch, time, d_model)\n        \"\"\"\n        # exchange the temporal dimension and the feature dimension\n        x = x.transpose(1, 2)\n\n        # GLU mechanism\n        x = self.pointwise_cov1(x)  # (batch, 2*channel, dim)\n        x = nn.functional.glu(x, dim=1)  # (batch, channel, dim)\n\n        # 1D Depthwise Conv\n        x = self.depthwise_conv(x)\n        x = self.activation(self.norm(x))\n\n        x = self.pointwise_cov2(x)\n\n        return x.transpose(1, 2)","metadata":{"id":"Ou93sSi0jS1G","execution":{"iopub.status.busy":"2024-06-14T22:59:33.418310Z","iopub.execute_input":"2024-06-14T22:59:33.418955Z","iopub.status.idle":"2024-06-14T22:59:33.429015Z","shell.execute_reply.started":"2024-06-14T22:59:33.418923Z","shell.execute_reply":"2024-06-14T22:59:33.427979Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Repeat","metadata":{"id":"FiwOPaYzkiE7"}},{"cell_type":"code","source":"class MultiSequential(torch.nn.Sequential):\n    \"\"\"Multi-input multi-output torch.nn.Sequential.\"\"\"\n\n    def forward(self, *args):\n        \"\"\"Repeat.\"\"\"\n        for m in self:\n            args = m(*args)\n        return args\n\n\ndef repeat(N, fn):\n    \"\"\"Repeat module N times.\n\n    :param int N: repeat time\n    :param function fn: function to generate module\n    :return: repeated modules\n    :rtype: MultiSequential\n    \"\"\"\n    return MultiSequential(*[fn() for _ in range(N)])","metadata":{"id":"07fecIHEkjsE","execution":{"iopub.status.busy":"2024-06-14T22:59:33.844469Z","iopub.execute_input":"2024-06-14T22:59:33.844853Z","iopub.status.idle":"2024-06-14T22:59:33.850759Z","shell.execute_reply.started":"2024-06-14T22:59:33.844815Z","shell.execute_reply":"2024-06-14T22:59:33.849751Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### LayerNorm","metadata":{"id":"MwoN53F0kkFq"}},{"cell_type":"code","source":"class LayerNorm(torch.nn.LayerNorm):\n    \"\"\"Layer normalization module.\n\n    :param int nout: output dim size\n    :param int dim: dimension to be normalized\n    \"\"\"\n\n    def __init__(self, nout, dim=-1):\n        \"\"\"Construct an LayerNorm object.\"\"\"\n        super(LayerNorm, self).__init__(nout, eps=1e-12)\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"Apply layer normalization.\n\n        :param torch.Tensor x: input tensor\n        :return: layer normalized tensor\n        :rtype torch.Tensor\n        \"\"\"\n        if self.dim == -1:\n            return super(LayerNorm, self).forward(x)\n        return super(LayerNorm, self).forward(x.transpose(1, -1)).transpose(1, -1)","metadata":{"id":"Z-omRV-1koDy","execution":{"iopub.status.busy":"2024-06-14T22:59:34.288928Z","iopub.execute_input":"2024-06-14T22:59:34.289735Z","iopub.status.idle":"2024-06-14T22:59:34.296482Z","shell.execute_reply.started":"2024-06-14T22:59:34.289700Z","shell.execute_reply":"2024-06-14T22:59:34.295422Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### EncoderLayer (Conformer)","metadata":{"id":"s9rqrInBks8b"}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    \"\"\"Encoder layer module.\n\n    :param int size: input dim\n    :param espnet.nets.pytorch_backend.transformer.attention.\n        MultiHeadedAttention self_attn: self attention module\n        RelPositionMultiHeadedAttention self_attn: self attention module\n    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n        PositionwiseFeedForward feed_forward:\n        feed forward module\n    :param espnet.nets.pytorch_backend.transformer.convolution.\n        ConvolutionModule feed_foreard:\n        feed forward module\n    :param float dropout_rate: dropout rate\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    :param bool macaron_style: whether to use macaron style for PositionwiseFeedForward\n\n    \"\"\"\n\n    def __init__(\n        self,\n        size,\n        self_attn,\n        feed_forward,\n        conv_module,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n        macaron_style=False,\n    ):\n        \"\"\"Construct an EncoderLayer object.\"\"\"\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.ff_scale = 1.0\n        self.conv_module = conv_module\n        self.macaron_style = macaron_style\n        self.norm_ff = LayerNorm(size)  # for the FNN module\n        self.norm_mha = LayerNorm(size)  # for the MHA module\n        if self.macaron_style:\n            self.feed_forward_macaron = copy.deepcopy(feed_forward)\n            self.ff_scale = 0.5\n            # for another FNN module in macaron style\n            self.norm_ff_macaron = LayerNorm(size)\n        if self.conv_module is not None:\n            self.norm_conv = LayerNorm(size)  # for the CNN module\n            self.norm_final = LayerNorm(size)  # for the final output of the block\n        self.dropout = nn.Dropout(dropout_rate)\n        self.size = size\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear = nn.Linear(size + size, size)\n\n    def forward(self, x_input, mask, cache=None):\n        \"\"\"Compute encoded features.\n\n        :param torch.Tensor x_input: encoded source features (batch, max_time_in, size)\n        :param torch.Tensor mask: mask for x (batch, max_time_in)\n        :param torch.Tensor cache: cache for x (batch, max_time_in - 1, size)\n        :rtype: Tuple[torch.Tensor, torch.Tensor]\n        \"\"\"\n        if isinstance(x_input, tuple):\n            x, pos_emb = x_input[0], x_input[1]\n        else:\n            x, pos_emb = x_input, None\n\n        # whether to use macaron style\n        if self.macaron_style:\n            residual = x\n            if self.normalize_before:\n                x = self.norm_ff_macaron(x)\n            x = residual + self.ff_scale * self.dropout(self.feed_forward_macaron(x))\n            if not self.normalize_before:\n                x = self.norm_ff_macaron(x)\n\n        # multi-headed self-attention module\n        residual = x\n        if self.normalize_before:\n            x = self.norm_mha(x)\n\n        if cache is None:\n            x_q = x\n        else:\n            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)\n            x_q = x[:, -1:, :]\n            residual = residual[:, -1:, :]\n            mask = None if mask is None else mask[:, -1:, :]\n\n        if pos_emb is not None:\n            x_att = self.self_attn(x_q, x, x, pos_emb, mask)\n        else:\n            x_att = self.self_attn(x_q, x, x, mask)\n\n        if self.concat_after:\n            x_concat = torch.cat((x, x_att), dim=-1)\n            x = residual + self.concat_linear(x_concat)\n        else:\n            x = residual + self.dropout(x_att)\n        if not self.normalize_before:\n            x = self.norm_mha(x)\n\n        # convolution module\n        if self.conv_module is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.norm_conv(x)\n            x = residual + self.dropout(self.conv_module(x))\n            if not self.normalize_before:\n                x = self.norm_conv(x)\n\n        # feed forward module\n        residual = x\n        if self.normalize_before:\n            x = self.norm_ff(x)\n        x = residual + self.ff_scale * self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm_ff(x)\n\n        if self.conv_module is not None:\n            x = self.norm_final(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        if pos_emb is not None:\n            return (x, pos_emb), mask\n        else:\n            return x, mask","metadata":{"id":"iuCP5OywkuKg","execution":{"iopub.status.busy":"2024-06-14T22:59:34.751328Z","iopub.execute_input":"2024-06-14T22:59:34.751697Z","iopub.status.idle":"2024-06-14T22:59:34.772574Z","shell.execute_reply.started":"2024-06-14T22:59:34.751668Z","shell.execute_reply":"2024-06-14T22:59:34.771504Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Encoder","metadata":{"id":"JDY4Nxr7dbzg"}},{"cell_type":"code","source":"\"\"\"Encoder definition.\"\"\"\n\n# import torch\n# from espnet.nets.pytorch_backend.backbones.conv1d_extractor import Conv1dResNet\n# from espnet.nets.pytorch_backend.backbones.conv3d_extractor import Conv3dResNet\n\n# from espnet.nets.pytorch_backend.nets_utils import rename_state_dict\n\n# from espnet.nets.pytorch_backend.transformer.attention import (\n#     MultiHeadedAttention,  # noqa: H301\n#     RelPositionMultiHeadedAttention,  # noqa: H301\n# )\n# from espnet.nets.pytorch_backend.transformer.convolution import ConvolutionModule\n# from espnet.nets.pytorch_backend.transformer.embedding import (\n#     PositionalEncoding,  # noqa: H301\n#     RelPositionalEncoding,  # noqa: H301\n# )\n# from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n# from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n# from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n\n# from espnet.nets.pytorch_backend.transformer.repeat import repeat\n\n\n# def _pre_hook(\n#     state_dict,\n#     prefix,\n#     local_metadata,\n#     strict,\n#     missing_keys,\n#     unexpected_keys,\n#     error_msgs,\n# ):\n#     # https://github.com/espnet/espnet/commit/21d70286c354c66c0350e65dc098d2ee236faccc#diff-bffb1396f038b317b2b64dd96e6d3563\n#     rename_state_dict(prefix + \"input_layer.\", prefix + \"embed.\", state_dict)\n#     # https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563\n#     rename_state_dict(prefix + \"norm.\", prefix + \"after_norm.\", state_dict)\n\n\nclass Encoder(torch.nn.Module):\n    \"\"\"Transformer encoder module.\n\n    :param int attention_dim: dimention of attention\n    :param int attention_heads: the number of heads of multi head attention\n    :param int linear_units: the number of units of position-wise feed forward\n    :param int num_blocks: the number of decoder blocks\n    :param float dropout_rate: dropout rate\n    :param float attention_dropout_rate: dropout rate in attention\n    :param float positional_dropout_rate: dropout rate after adding positional encoding\n    :param str or torch.nn.Module input_layer: input layer type\n    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    :param int positionwise_conv_kernel_size: kernel size of positionwise conv1d layer\n    :param str encoder_attn_layer_type: encoder attention layer type\n    :param bool macaron_style: whether to use macaron style for positionwise layer\n    :param bool use_cnn_module: whether to use convolution module\n    :param bool zero_triu: whether to zero the upper triangular part of attention matrix\n    :param int cnn_module_kernel: kernerl size of convolution module\n    :param int padding_idx: padding_idx for input_layer=embed\n    \"\"\"\n\n    def __init__(\n        self,\n        attention_dim=768,\n        attention_heads=12,\n        linear_units=3072,\n        num_blocks=12,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        attention_dropout_rate=0.0,\n        input_layer=\"conv2d\",\n        pos_enc_class=RelPositionalEncoding,\n        normalize_before=True,\n        concat_after=False,\n        positionwise_conv_kernel_size=1,\n        macaron_style=False,\n        encoder_attn_layer_type=\"mha\",\n        use_cnn_module=False,\n        zero_triu=False,\n        cnn_module_kernel=31,\n        padding_idx=-1,\n        relu_type=\"prelu\",\n        a_upsample_ratio=1,\n    ):\n        \"\"\"Construct an Encoder object.\"\"\"\n        super(Encoder, self).__init__()\n        #self._register_load_state_dict_pre_hook(_pre_hook)\n\n        # -- frontend module.\n        if input_layer == \"conv1d\":\n            self.frontend = Conv1dResNet(relu_type=relu_type, a_upsample_ratio=a_upsample_ratio)\n        elif input_layer == \"conv3d\":\n            self.frontend = Conv3dResNet(relu_type=relu_type)\n        else:\n            self.frontend = None\n\n        # -- Embedding\n        if encoder_attn_layer_type == \"rel_mha\":\n            pos_enc_class = RelPositionalEncoding\n        if input_layer in [\"conv1d\", \"conv3d\"]:\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(512, attention_dim),\n                pos_enc_class(attention_dim, positional_dropout_rate)\n                )\n        else:\n            raise NotImplementedError(\"Support only conv1d and conv3d\")\n\n        # -- backend module.\n        self.normalize_before = normalize_before\n        positionwise_layer = PositionwiseFeedForward\n        positionwise_layer_args = (attention_dim, linear_units, dropout_rate)\n\n        if encoder_attn_layer_type == \"mha\":\n            encoder_attn_layer = MultiHeadedAttention\n            encoder_attn_layer_args = (\n                attention_heads,\n                attention_dim,\n                attention_dropout_rate,\n            )\n        elif encoder_attn_layer_type == \"rel_mha\":\n            encoder_attn_layer = RelPositionMultiHeadedAttention\n            encoder_attn_layer_args = (\n                attention_heads,\n                attention_dim,\n                attention_dropout_rate,\n                zero_triu,\n            )\n        else:\n            raise ValueError(\"unknown encoder_attn_layer: \" + encoder_attn_layer)\n\n        convolution_layer = ConvolutionModule\n        convolution_layer_args = (attention_dim, cnn_module_kernel)\n\n        self.encoders = repeat(\n            num_blocks,\n            lambda: EncoderLayer(\n                attention_dim,\n                encoder_attn_layer(*encoder_attn_layer_args),\n                positionwise_layer(*positionwise_layer_args),\n                convolution_layer(*convolution_layer_args) if use_cnn_module else None,\n                dropout_rate,\n                normalize_before,\n                concat_after,\n                macaron_style,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n\n    def forward(self, xs, masks):\n        \"\"\"Encode input sequence.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :return: position embedded tensor and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        if isinstance(self.frontend, (Conv1dResNet, Conv3dResNet)):\n            xs = self.frontend(xs)\n\n        xs = self.embed(xs)\n        xs, masks = self.encoders(xs, masks)\n\n        if isinstance(xs, tuple):\n            xs = xs[0]\n\n        if self.normalize_before:\n            xs = self.after_norm(xs)\n\n        return xs, masks\n\n    def forward_one_step(self, xs, masks, cache=None):\n        \"\"\"Encode input frame.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :param List[torch.Tensor] cache: cache tensors\n        :return: position embedded tensor, mask and new cache\n        :rtype Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        \"\"\"\n        if isinstance(self.frontend, (Conv1dResNet, Conv3dResNet)):\n            xs = self.frontend(xs)\n\n        xs = self.embed(xs)\n\n        if cache is None:\n            cache = [None for _ in range(len(self.encoders))]\n        new_cache = []\n        for c, e in zip(cache, self.encoders):\n            xs, masks = e(xs, masks, cache=c)\n            new_cache.append(xs)\n        if self.normalize_before:\n            xs = self.after_norm(xs)\n        return xs, masks, new_cache","metadata":{"id":"cLRIFkwjdfBL","execution":{"iopub.status.busy":"2024-06-14T22:59:35.161570Z","iopub.execute_input":"2024-06-14T22:59:35.161896Z","iopub.status.idle":"2024-06-14T22:59:35.184007Z","shell.execute_reply.started":"2024-06-14T22:59:35.161872Z","shell.execute_reply":"2024-06-14T22:59:35.182922Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## MLPHead","metadata":{"id":"YvlnuzwKl8ya"}},{"cell_type":"code","source":"class MLPHead(torch.nn.Module):\n    def __init__(self, idim, hdim, odim, norm=\"batchnorm\"):\n        super(MLPHead, self).__init__()\n        self.norm = norm\n\n        self.fc1 = torch.nn.Linear(idim, hdim)\n        if norm == \"batchnorm\":\n            self.bn1 = torch.nn.BatchNorm1d(hdim)\n        elif norm == \"layernorm\":\n            self.norm1 = torch.nn.LayerNorm(hdim)\n        self.nonlin1 = torch.nn.ReLU(inplace=True)\n        self.fc2 = torch.nn.Linear(hdim, odim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        if self.norm == \"batchnorm\":\n            x = self.bn1(x.transpose(1, 2)).transpose(1, 2)\n        elif self.norm == \"layernorm\":\n            x = self.norm1(x)\n        x = self.nonlin1(x)\n        x = self.fc2(x)\n        return x","metadata":{"id":"5WS_YEjnl-73","execution":{"iopub.status.busy":"2024-06-14T22:59:35.561075Z","iopub.execute_input":"2024-06-14T22:59:35.562042Z","iopub.status.idle":"2024-06-14T22:59:35.570629Z","shell.execute_reply.started":"2024-06-14T22:59:35.562007Z","shell.execute_reply":"2024-06-14T22:59:35.569774Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## ScorerInterface","metadata":{"id":"JAA0ah8jnioL"}},{"cell_type":"code","source":"class ScorerInterface:\n    \"\"\"Scorer interface for beam search.\n\n    The scorer performs scoring of the all tokens in vocabulary.\n\n    Examples:\n        * Search heuristics\n            * :class:`espnet.nets.scorers.length_bonus.LengthBonus`\n        * Decoder networks of the sequence-to-sequence models\n            * :class:`espnet.nets.pytorch_backend.nets.transformer.decoder.Decoder`\n            * :class:`espnet.nets.pytorch_backend.nets.rnn.decoders.Decoder`\n        * Neural language models\n            * :class:`espnet.nets.pytorch_backend.lm.transformer.TransformerLM`\n            * :class:`espnet.nets.pytorch_backend.lm.default.DefaultRNNLM`\n            * :class:`espnet.nets.pytorch_backend.lm.seq_rnn.SequentialRNNLM`\n\n    \"\"\"\n\n    def init_state(self, x: torch.Tensor) -> Any:\n        \"\"\"Get an initial state for decoding (optional).\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"\n        return None\n\n    def select_state(self, state: Any, i: int, new_id: int = None) -> Any:\n        \"\"\"Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n            new_id (int): New label index to select a state if necessary\n\n        Returns:\n            state: pruned state\n\n        \"\"\"\n        return None if state is None else state[i]\n\n    def score(\n        self, y: torch.Tensor, state: Any, x: torch.Tensor\n    ) -> Tuple[torch.Tensor, Any]:\n        \"\"\"Score new token (required).\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): The encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                scores for next token that has a shape of `(n_vocab)`\n                and next state for ys\n\n        \"\"\"\n        raise NotImplementedError\n\n    def final_score(self, state: Any) -> float:\n        \"\"\"Score eos (optional).\n\n        Args:\n            state: Scorer state for prefix tokens\n\n        Returns:\n            float: final score\n\n        \"\"\"\n        return 0.0\n\n\nclass BatchScorerInterface(ScorerInterface):\n    \"\"\"Batch scorer interface.\"\"\"\n\n    def batch_init_state(self, x: torch.Tensor) -> Any:\n        \"\"\"Get an initial state for decoding (optional).\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"\n        return self.init_state(x)\n\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        \"\"\"Score new token batch (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        \"\"\"\n        warnings.warn(\n            \"{} batch score is implemented through for loop not parallelized\".format(\n                self.__class__.__name__\n            )\n        )\n        scores = list()\n        outstates = list()\n        for i, (y, state, x) in enumerate(zip(ys, states, xs)):\n            score, outstate = self.score(y, state, x)\n            outstates.append(outstate)\n            scores.append(score)\n        scores = torch.cat(scores, 0).view(ys.shape[0], -1)\n        return scores, outstates\n\n\nclass PartialScorerInterface(ScorerInterface):\n    \"\"\"Partial scorer interface for beam search.\n\n    The partial scorer performs scoring when non-partial scorer finished scoring,\n    and receives pre-pruned next tokens to score because it is too heavy to score\n    all the tokens.\n\n    Examples:\n         * Prefix search for connectionist-temporal-classification models\n             * :class:`espnet.nets.scorers.ctc.CTCPrefixScorer`\n\n    \"\"\"\n\n    def score_partial(\n        self, y: torch.Tensor, next_tokens: torch.Tensor, state: Any, x: torch.Tensor\n    ) -> Tuple[torch.Tensor, Any]:\n        \"\"\"Score new token (required).\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            next_tokens (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): The encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"\n        raise NotImplementedError\n\n\nclass BatchPartialScorerInterface(BatchScorerInterface, PartialScorerInterface):\n    \"\"\"Batch partial scorer interface for beam search.\"\"\"\n\n    def batch_score_partial(\n        self,\n        ys: torch.Tensor,\n        next_tokens: torch.Tensor,\n        states: List[Any],\n        xs: torch.Tensor,\n    ) -> Tuple[torch.Tensor, Any]:\n        \"\"\"Score new token (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            next_tokens (torch.Tensor): torch.int64 tokens to score (n_batch, n_token).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for ys that has a shape `(n_batch, n_vocab)`\n                and next states for ys\n        \"\"\"\n        raise NotImplementedError","metadata":{"id":"qCdIDkbinj3V","execution":{"iopub.status.busy":"2024-06-14T22:59:36.062411Z","iopub.execute_input":"2024-06-14T22:59:36.062784Z","iopub.status.idle":"2024-06-14T22:59:36.079510Z","shell.execute_reply.started":"2024-06-14T22:59:36.062754Z","shell.execute_reply":"2024-06-14T22:59:36.078585Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## PositionalEncoding","metadata":{"id":"apmMcHctoPrh"}},{"cell_type":"code","source":"class PositionalEncoding(torch.nn.Module):\n    \"\"\"Positional encoding.\n    Args:\n        d_model (int): Embedding dimension.\n        dropout_rate (float): Dropout rate.\n        max_len (int): Maximum input length.\n        reverse (bool): Whether to reverse the input position. Only for\n        the class LegacyRelPositionalEncoding. We remove it in the current\n        class RelPositionalEncoding.\n    \"\"\"\n\n    def __init__(self, d_model, dropout_rate, max_len=5000, reverse=False):\n        \"\"\"Construct an PositionalEncoding object.\"\"\"\n        super(PositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.reverse = reverse\n        self.xscale = math.sqrt(self.d_model)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.pe = None\n        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n        #self._register_load_state_dict_pre_hook(_pre_hook)\n\n    def extend_pe(self, x):\n        \"\"\"Reset the positional encodings.\"\"\"\n        if self.pe is not None:\n            if self.pe.size(1) >= x.size(1):\n                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n                return\n        pe = torch.zeros(x.size(1), self.d_model)\n        if self.reverse:\n            position = torch.arange(\n                x.size(1) - 1, -1, -1.0, dtype=torch.float32\n            ).unsqueeze(1)\n        else:\n            position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n            * -(math.log(10000.0) / self.d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.pe = pe.to(device=x.device, dtype=x.dtype)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Add positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n        \"\"\"\n        self.extend_pe(x)\n        x = x * self.xscale + self.pe[:, : x.size(1)]\n        return self.dropout(x)","metadata":{"id":"AvHCuslcoUZC","execution":{"iopub.status.busy":"2024-06-14T22:59:36.797902Z","iopub.execute_input":"2024-06-14T22:59:36.798271Z","iopub.status.idle":"2024-06-14T22:59:36.811754Z","shell.execute_reply.started":"2024-06-14T22:59:36.798241Z","shell.execute_reply":"2024-06-14T22:59:36.810675Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## DecoderLayer (Decoder del transformer)","metadata":{"id":"iE1DYbe_pAeg"}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    \"\"\"Single decoder layer module.\n    :param int size: input dim\n    :param espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\n        self_attn: self attention module\n    :param espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\n        src_attn: source attention module\n    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n        PositionwiseFeedForward feed_forward: feed forward layer module\n    :param float dropout_rate: dropout rate\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    \"\"\"\n\n    def __init__(\n        self,\n        size,\n        self_attn,\n        src_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        \"\"\"Construct an DecoderLayer object.\"\"\"\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.norm1 = LayerNorm(size)\n        self.norm2 = LayerNorm(size)\n        self.norm3 = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear1 = nn.Linear(size + size, size)\n            self.concat_linear2 = nn.Linear(size + size, size)\n\n    def forward(self, tgt, tgt_mask, memory, memory_mask, cache=None):\n        \"\"\"Compute decoded features.\n        Args:\n            tgt (torch.Tensor):\n                decoded previous target features (batch, max_time_out, size)\n            tgt_mask (torch.Tensor): mask for x (batch, max_time_out)\n            memory (torch.Tensor): encoded source features (batch, max_time_in, size)\n            memory_mask (torch.Tensor): mask for memory (batch, max_time_in)\n            cache (torch.Tensor): cached output (batch, max_time_out-1, size)\n        \"\"\"\n        residual = tgt\n        if self.normalize_before:\n            tgt = self.norm1(tgt)\n\n        if cache is None:\n            tgt_q = tgt\n            tgt_q_mask = tgt_mask\n        else:\n            # compute only the last frame query keeping dim: max_time_out -> 1\n            assert cache.shape == (\n                tgt.shape[0],\n                tgt.shape[1] - 1,\n                self.size,\n            ), f\"{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}\"\n            tgt_q = tgt[:, -1:, :]\n            residual = residual[:, -1:, :]\n            tgt_q_mask = None\n            if tgt_mask is not None:\n                tgt_q_mask = tgt_mask[:, -1:, :]\n\n        if self.concat_after:\n            tgt_concat = torch.cat(\n                (tgt_q, self.self_attn(tgt_q, tgt, tgt, tgt_q_mask)), dim=-1\n            )\n            x = residual + self.concat_linear1(tgt_concat)\n        else:\n            x = residual + self.dropout(self.self_attn(tgt_q, tgt, tgt, tgt_q_mask))\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        if self.concat_after:\n            x_concat = torch.cat(\n                (x, self.src_attn(x, memory, memory, memory_mask)), dim=-1\n            )\n            x = residual + self.concat_linear2(x_concat)\n        else:\n            x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))\n        if not self.normalize_before:\n            x = self.norm2(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm3(x)\n        x = residual + self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm3(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        return x, tgt_mask, memory, memory_mask","metadata":{"id":"Wuztt-WOpDdo","execution":{"iopub.status.busy":"2024-06-14T22:59:37.345720Z","iopub.execute_input":"2024-06-14T22:59:37.346074Z","iopub.status.idle":"2024-06-14T22:59:37.364190Z","shell.execute_reply.started":"2024-06-14T22:59:37.346046Z","shell.execute_reply":"2024-06-14T22:59:37.363052Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Mask","metadata":{"id":"dQAWTi24pfBa"}},{"cell_type":"code","source":"from distutils.version import LooseVersion\n\nis_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(\"1.2.0\")\n# LooseVersion('1.2.0') == LooseVersion(torch.__version__) can't include e.g. 1.2.0+aaa\nis_torch_1_2 = (\n    LooseVersion(\"1.3\") > LooseVersion(torch.__version__) >= LooseVersion(\"1.2\")\n)\ndatatype = torch.bool if is_torch_1_2_plus else torch.uint8\n\ndef subsequent_mask(size, device=\"cpu\", dtype=datatype):\n    \"\"\"Create mask for subsequent steps (1, size, size).\n\n    :param int size: size of mask\n    :param str device: \"cpu\" or \"cuda\" or torch.Tensor.device\n    :param torch.dtype dtype: result dtype\n    :rtype: torch.Tensor\n    >>> subsequent_mask(3)\n    [[1, 0, 0],\n     [1, 1, 0],\n     [1, 1, 1]]\n    \"\"\"\n    if is_torch_1_2 and dtype == torch.bool:\n        # torch=1.2 doesn't support tril for bool tensor\n        ret = torch.ones(size, size, device=device, dtype=torch.uint8)\n        return torch.tril(ret, out=ret).type(dtype)\n    else:\n        ret = torch.ones(size, size, device=device, dtype=dtype)\n        return torch.tril(ret, out=ret)\n\n\ndef target_mask(ys_in_pad, ignore_id):\n    \"\"\"Create mask for decoder self-attention.\n\n    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n    :param int ignore_id: index of padding\n    :param torch.dtype dtype: result dtype\n    :rtype: torch.Tensor\n    \"\"\"\n    ys_mask = ys_in_pad != ignore_id\n    m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)\n    return ys_mask.unsqueeze(-2) & m","metadata":{"id":"9nLsXl0cpgix","outputId":"2e9618aa-506c-4edf-fcc3-6e9fa0d74455","execution":{"iopub.status.busy":"2024-06-14T22:59:38.415274Z","iopub.execute_input":"2024-06-14T22:59:38.416133Z","iopub.status.idle":"2024-06-14T22:59:38.426639Z","shell.execute_reply.started":"2024-06-14T22:59:38.416099Z","shell.execute_reply":"2024-06-14T22:59:38.425773Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3006193323.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  is_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(\"1.2.0\")\n/tmp/ipykernel_34/3006193323.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  LooseVersion(\"1.3\") > LooseVersion(torch.__version__) >= LooseVersion(\"1.2\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Decoder","metadata":{"id":"Cf9JstLvmyhc"}},{"cell_type":"code","source":"\"\"\"Decoder definition.\"\"\"\n\n# from typing import Any, List, Tuple\n\n# import torch\n\n# from espnet.nets.pytorch_backend.nets_utils import rename_state_dict\n# from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n# from espnet.nets.pytorch_backend.transformer.decoder_layer import DecoderLayer\n# from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n# from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n# from espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\n# from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n#     PositionwiseFeedForward,  # noqa: H301\n# )\n# from espnet.nets.pytorch_backend.transformer.repeat import repeat\n# from espnet.nets.scorer_interface import BatchScorerInterface\n\n\n# def _pre_hook(\n#     state_dict,\n#     prefix,\n#     local_metadata,\n#     strict,\n#     missing_keys,\n#     unexpected_keys,\n#     error_msgs,\n# ):\n#     # https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563\n#     rename_state_dict(prefix + \"output_norm.\", prefix + \"after_norm.\", state_dict)\n\n\nclass Decoder(BatchScorerInterface, torch.nn.Module):\n    \"\"\"Transfomer decoder module.\n\n    :param int odim: output dim\n    :param int attention_dim: dimention of attention\n    :param int attention_heads: the number of heads of multi head attention\n    :param int linear_units: the number of units of position-wise feed forward\n    :param int num_blocks: the number of decoder blocks\n    :param float dropout_rate: dropout rate\n    :param float attention_dropout_rate: dropout rate for attention\n    :param str or torch.nn.Module input_layer: input layer type\n    :param bool use_output_layer: whether to use output layer\n    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer's input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    \"\"\"\n\n    def __init__(\n        self,\n        odim,\n        attention_dim=256,\n        attention_heads=4,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        self_attention_dropout_rate=0.0,\n        src_attention_dropout_rate=0.0,\n        input_layer=\"embed\",\n        use_output_layer=True,\n        pos_enc_class=PositionalEncoding,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        \"\"\"Construct an Decoder object.\"\"\"\n        torch.nn.Module.__init__(self)\n        #self._register_load_state_dict_pre_hook(_pre_hook)\n        if input_layer == \"embed\":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(odim, attention_dim),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        # elif input_layer == \"linear\":\n        #     self.embed = torch.nn.Sequential(\n        #         torch.nn.Linear(odim, attention_dim),\n        #         torch.nn.LayerNorm(attention_dim),\n        #         torch.nn.Dropout(dropout_rate),\n        #         torch.nn.ReLU(),\n        #         pos_enc_class(attention_dim, positional_dropout_rate),\n        #     )\n        # elif isinstance(input_layer, torch.nn.Module):\n        #     self.embed = torch.nn.Sequential(\n        #         input_layer, pos_enc_class(attention_dim, positional_dropout_rate)\n        #     )\n        else:\n            raise NotImplementedError(\"only `embed` or torch.nn.Module is supported.\")\n\n        self.normalize_before = normalize_before\n        self.decoders = repeat(\n            num_blocks,\n            lambda: DecoderLayer(\n                attention_dim,\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, self_attention_dropout_rate\n                ),\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, src_attention_dropout_rate\n                ),\n                PositionwiseFeedForward(attention_dim, linear_units, dropout_rate),\n                dropout_rate,\n                normalize_before,\n                concat_after,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n        if use_output_layer:\n            self.output_layer = torch.nn.Linear(attention_dim, odim)\n        else:\n            self.output_layer = None\n\n    def forward(self, tgt, tgt_mask, memory, memory_mask):\n        \"\"\"Forward decoder.\n        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n                                 if input_layer == \"embed\"\n                                 input tensor (batch, maxlen_out, #mels)\n                                 in the other cases\n        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n                                      dtype=torch.uint8 in PyTorch 1.2-\n                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n        :param torch.Tensor memory_mask: encoded memory mask,  (batch, maxlen_in)\n                                         dtype=torch.uint8 in PyTorch 1.2-\n                                         dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :return x: decoded token score before softmax (batch, maxlen_out, token)\n                   if use_output_layer is True,\n                   final block outputs (batch, maxlen_out, attention_dim)\n                   in the other cases\n        :rtype: torch.Tensor\n        :return tgt_mask: score mask before softmax (batch, maxlen_out)\n        :rtype: torch.Tensor\n        \"\"\"\n        x = self.embed(tgt)\n        x, tgt_mask, memory, memory_mask = self.decoders(\n            x, tgt_mask, memory, memory_mask\n        )\n        if self.normalize_before:\n            x = self.after_norm(x)\n        if self.output_layer is not None:\n            x = self.output_layer(x)\n        return x, tgt_mask\n\n    def forward_one_step(self, tgt, tgt_mask, memory, memory_mask=None, cache=None):\n        \"\"\"Forward one step.\n        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n                                      dtype=torch.uint8 in PyTorch 1.2-\n                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n        :param List[torch.Tensor] cache:\n            cached output list of (batch, max_time_out-1, size)\n        :return y, cache: NN output value and cache per `self.decoders`.\n            `y.shape` is (batch, maxlen_out, token)\n        :rtype: Tuple[torch.Tensor, List[torch.Tensor]]\n        \"\"\"\n        x = self.embed(tgt)\n        if cache is None:\n            cache = [None] * len(self.decoders)\n        new_cache = []\n        for c, decoder in zip(cache, self.decoders):\n            x, tgt_mask, memory, memory_mask = decoder(\n                x, tgt_mask, memory, memory_mask, cache=c\n            )\n            new_cache.append(x)\n\n        if self.normalize_before:\n            y = self.after_norm(x[:, -1])\n        else:\n            y = x[:, -1]\n        if self.output_layer is not None:\n            y = torch.log_softmax(self.output_layer(y), dim=-1)\n\n        return y, new_cache\n\n    # beam search API (see ScorerInterface)\n    def score(self, ys, state, x):\n        \"\"\"Score.\"\"\"\n        ys_mask = subsequent_mask(len(ys), device=x.device).unsqueeze(0)\n        logp, state = self.forward_one_step(\n            ys.unsqueeze(0), ys_mask, x.unsqueeze(0), cache=state\n        )\n        return logp.squeeze(0), state\n\n    # batch beam search API (see BatchScorerInterface)\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        \"\"\"Score new token batch (required).\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n        \"\"\"\n        # merge states\n        n_batch = len(ys)\n        n_layers = len(self.decoders)\n        if states[0] is None:\n            batch_state = None\n        else:\n            # transpose state of [batch, layer] into [layer, batch]\n            batch_state = [\n                torch.stack([states[b][l] for b in range(n_batch)])\n                for l in range(n_layers)\n            ]\n\n        # batch decoding\n        ys_mask = subsequent_mask(ys.size(-1), device=xs.device).unsqueeze(0)\n        logp, states = self.forward_one_step(ys, ys_mask, xs, cache=batch_state)\n\n        # transpose state of [layer, batch] into [batch, layer]\n        state_list = [[states[l][b] for l in range(n_layers)] for b in range(n_batch)]\n        return logp, state_list","metadata":{"id":"lCkqlATum0cr","execution":{"iopub.status.busy":"2024-06-14T22:59:43.752907Z","iopub.execute_input":"2024-06-14T22:59:43.753418Z","iopub.status.idle":"2024-06-14T22:59:43.778917Z","shell.execute_reply.started":"2024-06-14T22:59:43.753388Z","shell.execute_reply":"2024-06-14T22:59:43.777717Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{"id":"D6w-KSvPtOO3"}},{"cell_type":"code","source":"def to_device(m, x):\n    \"\"\"Send tensor into the device of the module.\n\n    Args:\n        m (torch.nn.Module): Torch module.\n        x (Tensor): Torch tensor.\n\n    Returns:\n        Tensor: Torch tensor located in the same place as torch module.\n\n    \"\"\"\n    if isinstance(m, torch.nn.Module):\n        device = next(m.parameters()).device\n    elif isinstance(m, torch.Tensor):\n        device = m.device\n    else:\n        raise TypeError(\n            \"Expected torch.nn.Module or torch.tensor, \" f\"bot got: {type(m)}\"\n        )\n    return x.to(device)\n\ndef pad_list(xs, pad_value):\n    \"\"\"Perform padding for the list of tensors.\n\n    Args:\n        xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)].\n        pad_value (float): Value for padding.\n\n    Returns:\n        Tensor: Padded tensor (B, Tmax, `*`).\n\n    Examples:\n        >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)]\n        >>> x\n        [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]\n        >>> pad_list(x, 0)\n        tensor([[1., 1., 1., 1.],\n                [1., 1., 0., 0.],\n                [1., 0., 0., 0.]])\n\n    \"\"\"\n    n_batch = len(xs)\n    max_len = max(x.size(0) for x in xs)\n    pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)\n\n    for i in range(n_batch):\n        pad[i, : xs[i].size(0)] = xs[i]\n\n    return pad\n\n\ndef make_pad_mask(lengths, xs=None, length_dim=-1, maxlen=None):\n    \"\"\"Make mask tensor containing indices of padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        Tensor: Mask tensor containing indices of padded part.\n                dtype=torch.uint8 in PyTorch 1.2-\n                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_pad_mask(lengths)\n        masks = [[0, 0, 0, 0 ,0],\n                 [0, 0, 0, 1, 1],\n                 [0, 0, 1, 1, 1]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0],\n                 [0, 0, 0, 0]],\n                [[0, 0, 0, 1],\n                 [0, 0, 0, 1]],\n                [[0, 0, 1, 1],\n                 [0, 0, 1, 1]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_pad_mask(lengths, xs, 1)\n        tensor([[[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n        >>> make_pad_mask(lengths, xs, 2)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n    \"\"\"\n    if length_dim == 0:\n        raise ValueError(\"length_dim cannot be 0: {}\".format(length_dim))\n\n    if not isinstance(lengths, list):\n        lengths = lengths.tolist()\n    bs = int(len(lengths))\n    if maxlen is None:\n        if xs is None:\n            maxlen = int(max(lengths))\n        else:\n            maxlen = xs.size(length_dim)\n    else:\n        assert xs is None\n        assert maxlen >= int(max(lengths))\n\n    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n    mask = seq_range_expand >= seq_length_expand\n\n    if xs is not None:\n        assert xs.size(0) == bs, (xs.size(0), bs)\n\n        if length_dim < 0:\n            length_dim = xs.dim() + length_dim\n        # ind = (:, None, ..., None, :, , None, ..., None)\n        ind = tuple(\n            slice(None) if i in (0, length_dim) else None for i in range(xs.dim())\n        )\n        mask = mask[ind].expand_as(xs).to(xs.device)\n    return mask\n\ndef make_non_pad_mask(lengths, xs=None, length_dim=-1):\n    \"\"\"Make mask tensor containing indices of non-padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        ByteTensor: mask tensor containing indices of padded part.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[1, 1, 1, 1 ,1],\n                 [1, 1, 1, 0, 0],\n                 [1, 1, 0, 0, 0]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1],\n                 [1, 1, 1, 1]],\n                [[1, 1, 1, 0],\n                 [1, 1, 1, 0]],\n                [[1, 1, 0, 0],\n                 [1, 1, 0, 0]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_non_pad_mask(lengths, xs, 1)\n        tensor([[[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)\n        >>> make_non_pad_mask(lengths, xs, 2)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n    \"\"\"\n    return ~make_pad_mask(lengths, xs, length_dim)\n\n\ndef th_accuracy(pad_outputs, pad_targets, ignore_label):\n    \"\"\"Calculate accuracy.\n\n    Args:\n        pad_outputs (Tensor): Prediction tensors (B * Lmax, D).\n        pad_targets (LongTensor): Target label tensors (B, Lmax, D).\n        ignore_label (int): Ignore label id.\n\n    Returns:\n        float: Accuracy value (0.0 - 1.0).\n\n    \"\"\"\n    pad_pred = pad_outputs.view(\n        pad_targets.size(0), pad_targets.size(1), pad_outputs.size(1)\n    ).argmax(2)\n    mask = pad_targets != ignore_label\n    numerator = torch.sum(\n        pad_pred.masked_select(mask) == pad_targets.masked_select(mask)\n    )\n    denominator = torch.sum(mask)\n    return float(numerator) / float(denominator)\n","metadata":{"id":"ESjiQFI8tQr9","execution":{"iopub.status.busy":"2024-06-14T22:59:45.460745Z","iopub.execute_input":"2024-06-14T22:59:45.461131Z","iopub.status.idle":"2024-06-14T22:59:45.484561Z","shell.execute_reply.started":"2024-06-14T22:59:45.461105Z","shell.execute_reply":"2024-06-14T22:59:45.483719Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## add_sos_eos","metadata":{"id":"nnas4FCNuK-3"}},{"cell_type":"code","source":"def add_sos_eos(ys_pad, sos, eos, ignore_id):\n    \"\"\"Add <sos> and <eos> labels.\n\n    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n    :param int sos: index of <sos>\n    :param int eos: index of <eeos>\n    :param int ignore_id: index of padding\n    :return: padded tensor (B, Lmax)\n    :rtype: torch.Tensor\n    :return: padded tensor (B, Lmax)\n    :rtype: torch.Tensor\n    \"\"\"\n    #from espnet.nets.pytorch_backend.nets_utils import pad_list\n\n    _sos = ys_pad.new([sos])\n    _eos = ys_pad.new([eos])\n    ys = [y[y != ignore_id] for y in ys_pad]  # parse padded ys\n    ys_in = [torch.cat([_sos, y], dim=0) for y in ys]\n    ys_out = [torch.cat([y, _eos], dim=0) for y in ys]\n    return pad_list(ys_in, eos), pad_list(ys_out, ignore_id)","metadata":{"id":"bhKNalMJuMWB","execution":{"iopub.status.busy":"2024-06-14T22:59:46.575709Z","iopub.execute_input":"2024-06-14T22:59:46.576396Z","iopub.status.idle":"2024-06-14T22:59:46.583052Z","shell.execute_reply.started":"2024-06-14T22:59:46.576367Z","shell.execute_reply":"2024-06-14T22:59:46.581967Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## LabelSmoothingLoss","metadata":{"id":"w-Y9I92RqrVT"}},{"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    \"\"\"Label-smoothing loss.\n\n    :param int size: the number of class\n    :param int padding_idx: ignored class id\n    :param float smoothing: smoothing rate (0.0 means the conventional CE)\n    :param bool normalize_length: normalize loss by sequence length if True\n    :param torch.nn.Module criterion: loss function to be smoothed\n    \"\"\"\n\n    def __init__(\n        self,\n        size,\n        padding_idx,\n        smoothing,\n        normalize_length=False,\n        criterion=nn.KLDivLoss(reduction=\"none\"),\n    ):\n        \"\"\"Construct an LabelSmoothingLoss object.\"\"\"\n        super(LabelSmoothingLoss, self).__init__()\n        self.criterion = criterion\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n        self.normalize_length = normalize_length\n\n    def forward(self, x, target):\n        \"\"\"Compute loss between x and target.\n\n        :param torch.Tensor x: prediction (batch, seqlen, class)\n        :param torch.Tensor target:\n            target signal masked with self.padding_id (batch, seqlen)\n        :return: scalar float value\n        :rtype torch.Tensor\n        \"\"\"\n        assert x.size(2) == self.size\n        batch_size = x.size(0)\n        x = x.view(-1, self.size)\n        target = target.view(-1)\n        with torch.no_grad():\n            true_dist = x.clone()\n            true_dist.fill_(self.smoothing / (self.size - 1))\n            ignore = target == self.padding_idx  # (B,)\n            total = len(target) - ignore.sum().item()\n            target = target.masked_fill(ignore, 0)  # avoid -1 index\n            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n        kl = self.criterion(torch.log_softmax(x, dim=1), true_dist)\n        denom = total if self.normalize_length else batch_size\n        return kl.masked_fill(ignore.unsqueeze(1), 0).sum() / denom","metadata":{"id":"HJCMkwOZq11b","execution":{"iopub.status.busy":"2024-06-14T22:59:47.656600Z","iopub.execute_input":"2024-06-14T22:59:47.657421Z","iopub.status.idle":"2024-06-14T22:59:47.667723Z","shell.execute_reply.started":"2024-06-14T22:59:47.657391Z","shell.execute_reply":"2024-06-14T22:59:47.666687Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## CTC","metadata":{"id":"P4_0WJ1RrLa0"}},{"cell_type":"code","source":"class CTC(torch.nn.Module):\n    \"\"\"CTC module\n\n    :param int odim: dimension of outputs\n    :param int eprojs: number of encoder projection units\n    :param float dropout_rate: dropout rate (0.0 ~ 1.0)\n    :param str ctc_type: builtin or warpctc\n    :param bool reduce: reduce the CTC loss into a scalar\n    \"\"\"\n\n    def __init__(self, odim, eprojs, dropout_rate, ctc_type=\"builtin\", reduce=True):\n        super().__init__()\n        self.dropout_rate = dropout_rate\n        self.loss = None\n        self.ctc_lo = torch.nn.Linear(eprojs, odim)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        self.probs = None  # for visualization\n\n        # In case of Pytorch >= 1.7.0, CTC will be always builtin\n        self.ctc_type = (\n            ctc_type\n            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")\n            else \"builtin\"\n        )\n\n        if ctc_type != self.ctc_type:\n            logging.debug(f\"CTC was set to {self.ctc_type} due to PyTorch version.\")\n\n        if self.ctc_type == \"builtin\":\n            reduction_type = \"sum\" if reduce else \"none\"\n            self.ctc_loss = torch.nn.CTCLoss(\n                reduction=reduction_type, zero_infinity=True\n            )\n        # elif self.ctc_type == \"cudnnctc\":\n        #     reduction_type = \"sum\" if reduce else \"none\"\n        #     self.ctc_loss = torch.nn.CTCLoss(reduction=reduction_type)\n        # elif self.ctc_type == \"warpctc\":\n        #     import warpctc_pytorch as warp_ctc\n\n        #     self.ctc_loss = warp_ctc.CTCLoss(size_average=True, reduce=reduce)\n        # elif self.ctc_type == \"gtnctc\":\n        #     from espnet.nets.pytorch_backend.gtn_ctc import GTNCTCLossFunction\n\n        #     self.ctc_loss = GTNCTCLossFunction.apply\n        else:\n            raise ValueError(\n                'ctc_type must be \"builtin\" or \"warpctc\": {}'.format(self.ctc_type)\n            )\n\n        self.ignore_id = -1\n        self.reduce = reduce\n\n    def loss_fn(self, th_pred, th_target, th_ilen, th_olen):\n        if self.ctc_type in [\"builtin\", \"cudnnctc\"]:\n            th_pred = th_pred.log_softmax(2)\n            # Use the deterministic CuDNN implementation of CTC loss to avoid\n            #  [issue#17798](https://github.com/pytorch/pytorch/issues/17798)\n            with torch.backends.cudnn.flags(deterministic=True):\n                loss = self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n            # Batch-size average\n            loss = loss / th_pred.size(1)\n            return loss\n        elif self.ctc_type == \"warpctc\":\n            return self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n        elif self.ctc_type == \"gtnctc\":\n            targets = [t.tolist() for t in th_target]\n            log_probs = torch.nn.functional.log_softmax(th_pred, dim=2)\n            return self.ctc_loss(log_probs, targets, th_ilen, 0, \"none\")\n        else:\n            raise NotImplementedError\n\n    def forward(self, hs_pad, hlens, ys_pad):\n        \"\"\"CTC forward\n\n        :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D)\n        :param torch.Tensor hlens: batch of lengths of hidden state sequences (B)\n        :param torch.Tensor ys_pad:\n            batch of padded character id sequence tensor (B, Lmax)\n        :return: ctc loss value\n        :rtype: torch.Tensor\n        \"\"\"\n        # TODO(kan-bayashi): need to make more smart way\n        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n\n        # zero padding for hs\n        ys_hat = self.ctc_lo(self.dropout(hs_pad))\n        if self.ctc_type != \"gtnctc\":\n            ys_hat = ys_hat.transpose(0, 1)\n\n        if self.ctc_type == \"builtin\":\n            olens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))\n            hlens = hlens.long()\n            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix\n            self.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)\n        else:\n            self.loss = None\n            hlens = torch.from_numpy(np.fromiter(hlens, dtype=np.int32))\n            olens = torch.from_numpy(\n                np.fromiter((x.size(0) for x in ys), dtype=np.int32)\n            )\n            # zero padding for ys\n            ys_true = torch.cat(ys).cpu().int()  # batch x olen\n            # get ctc loss\n            # expected shape of seqLength x batchSize x alphabet_size\n            dtype = ys_hat.dtype\n            if self.ctc_type == \"warpctc\" or dtype == torch.float16:\n                # warpctc only supports float32\n                # torch.ctc does not support float16 (#1751)\n                ys_hat = ys_hat.to(dtype=torch.float32)\n            if self.ctc_type == \"cudnnctc\":\n                # use GPU when using the cuDNN implementation\n                ys_true = to_device(hs_pad, ys_true)\n            if self.ctc_type == \"gtnctc\":\n                # keep as list for gtn\n                ys_true = ys\n            self.loss = to_device(\n                hs_pad, self.loss_fn(ys_hat, ys_true, hlens, olens)\n            ).to(dtype=dtype)\n\n        # get length info\n        \"\"\"\n        logging.debug(\n            self.__class__.__name__\n            + \" input lengths:  \"\n            + \"\".join(str(hlens).split(\"\\n\"))\n        )\n        logging.debug(\n            self.__class__.__name__\n            + \" output lengths: \"\n            + \"\".join(str(olens).split(\"\\n\"))\n        )\n        \"\"\"\n        if self.reduce:\n            # NOTE: sum() is needed to keep consistency\n            # since warpctc return as tensor w/ shape (1,)\n            # but builtin return as tensor w/o shape (scalar).\n            self.loss = self.loss.sum()\n            # logging.debug(\"ctc loss:\" + str(float(self.loss)))\n\n        return self.loss, ys_hat\n\n    def softmax(self, hs_pad):\n        \"\"\"softmax of frame activations\n\n        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: log softmax applied 3d tensor (B, Tmax, odim)\n        :rtype: torch.Tensor\n        \"\"\"\n        self.probs = F.softmax(self.ctc_lo(hs_pad), dim=-1)\n        return self.probs\n\n    def log_softmax(self, hs_pad):\n        \"\"\"log_softmax of frame activations\n\n        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: log softmax applied 3d tensor (B, Tmax, odim)\n        :rtype: torch.Tensor\n        \"\"\"\n        return F.log_softmax(self.ctc_lo(hs_pad), dim=-1)\n\n    def argmax(self, hs_pad):\n        \"\"\"argmax of frame activations\n\n        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: argmax applied 2d tensor (B, Tmax)\n        :rtype: torch.Tensor\n        \"\"\"\n        return torch.argmax(self.ctc_lo(hs_pad), dim=-1)\n\n    def forced_align(self, h, y, blank_id=0):\n        \"\"\"forced alignment.\n\n        :param torch.Tensor h: hidden state sequence, 2d tensor (T, D)\n        :param torch.Tensor y: id sequence tensor 1d tensor (L)\n        :param int y: blank symbol index\n        :return: best alignment results\n        :rtype: list\n        \"\"\"\n\n        def interpolate_blank(label, blank_id=0):\n            \"\"\"Insert blank token between every two label token.\"\"\"\n            label = np.expand_dims(label, 1)\n            blanks = np.zeros((label.shape[0], 1), dtype=np.int64) + blank_id\n            label = np.concatenate([blanks, label], axis=1)\n            label = label.reshape(-1)\n            label = np.append(label, label[0])\n            return label\n\n        lpz = self.log_softmax(h)\n        lpz = lpz.squeeze(0)\n\n        y_int = interpolate_blank(y, blank_id)\n\n        logdelta = np.zeros((lpz.size(0), len(y_int))) - 100000000000.0  # log of zero\n        state_path = (\n            np.zeros((lpz.size(0), len(y_int)), dtype=np.int16) - 1\n        )  # state path\n\n        logdelta[0, 0] = lpz[0][y_int[0]]\n        logdelta[0, 1] = lpz[0][y_int[1]]\n\n        for t in six.moves.range(1, lpz.size(0)):\n            for s in six.moves.range(len(y_int)):\n                if y_int[s] == blank_id or s < 2 or y_int[s] == y_int[s - 2]:\n                    candidates = np.array([logdelta[t - 1, s], logdelta[t - 1, s - 1]])\n                    prev_state = [s, s - 1]\n                else:\n                    candidates = np.array(\n                        [\n                            logdelta[t - 1, s],\n                            logdelta[t - 1, s - 1],\n                            logdelta[t - 1, s - 2],\n                        ]\n                    )\n                    prev_state = [s, s - 1, s - 2]\n                logdelta[t, s] = np.max(candidates) + lpz[t][y_int[s]]\n                state_path[t, s] = prev_state[np.argmax(candidates)]\n\n        state_seq = -1 * np.ones((lpz.size(0), 1), dtype=np.int16)\n\n        candidates = np.array(\n            [logdelta[-1, len(y_int) - 1], logdelta[-1, len(y_int) - 2]]\n        )\n        prev_state = [len(y_int) - 1, len(y_int) - 2]\n        state_seq[-1] = prev_state[np.argmax(candidates)]\n        for t in six.moves.range(lpz.size(0) - 2, -1, -1):\n            state_seq[t] = state_path[t + 1, state_seq[t + 1, 0]]\n\n        output_state_seq = []\n        for t in six.moves.range(0, lpz.size(0)):\n            output_state_seq.append(y_int[state_seq[t, 0]])\n\n        return output_state_seq\n\n    def forced_align_batch(self, hs_pad, ys_pad, ilens, blank_id=0):\n        \"\"\"forced alignment with batch processing.\n\n        :param torch.Tensor hs_pad: hidden state sequence, 3d tensor (T, B, D)\n        :param torch.Tensor ys_pad: id sequence tensor 2d tensor (B, L)\n        :param torch.Tensor ilens: Input length of each utterance (B,)\n        :param int blank_id: blank symbol index\n        :return: best alignment results\n        :rtype: list of numpy.array\n        \"\"\"\n\n        def interpolate_blank(label, olens_int):\n            \"\"\"Insert blank token between every two label token.\"\"\"\n            lab_len = label.shape[1] * 2 + 1\n            label_out = np.full((label.shape[0], lab_len), blank_id, dtype=np.int64)\n            label_out[:, 1::2] = label\n            for b in range(label.shape[0]):\n                label_out[b, olens_int[b] * 2 + 1 :] = self.ignore_id\n            return label_out\n\n        neginf = float(\"-inf\")  # log of zero\n        # lpz = self.log_softmax(hs_pad).cpu().detach().numpy()\n        # hs_pad = hs_pad.transpose(1,0)\n        lpz = F.log_softmax(hs_pad, dim=-1).cpu().detach().numpy()\n        ilens = ilens.cpu().detach().numpy()\n\n        ys_pad = ys_pad.cpu().detach().numpy()\n        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n        olens = np.array([len(s) for s in ys])\n        olens_int = olens * 2 + 1\n        ys_int = interpolate_blank(ys_pad, olens_int)\n\n        Tmax, B, _ = lpz.shape\n        Lmax = ys_int.shape[-1]\n        logdelta = np.full((Tmax, B, Lmax), neginf, dtype=lpz.dtype)\n        state_path = -np.ones(logdelta.shape, dtype=np.int16)  # state path\n\n        b_indx = np.arange(B, dtype=np.int64)\n        t_0 = np.zeros(B, dtype=np.int64)\n        logdelta[0, :, 0] = lpz[t_0, b_indx, ys_int[:, 0]]\n        logdelta[0, :, 1] = lpz[t_0, b_indx, ys_int[:, 1]]\n\n        s_indx_mat = np.arange(Lmax)[None, :].repeat(B, 0)\n        notignore_mat = ys_int != self.ignore_id\n        same_lab_mat = np.zeros((B, Lmax), dtype=np.bool)\n        same_lab_mat[:, 3::2] = ys_int[:, 3::2] == ys_int[:, 1:-2:2]\n        Lmin = olens_int.min()\n        for t in range(1, Tmax):\n            s_start = max(0, Lmin - (Tmax - t) * 2)\n            s_end = min(Lmax, t * 2 + 2)\n            candidates = np.full((B, Lmax, 3), neginf, dtype=logdelta.dtype)\n            candidates[:, :, 0] = logdelta[t - 1, :, :]\n            candidates[:, 1:, 1] = logdelta[t - 1, :, :-1]\n            candidates[:, 3::2, 2] = logdelta[t - 1, :, 1:-2:2]\n            candidates[same_lab_mat, 2] = neginf\n            candidates_ = candidates[:, s_start:s_end, :]\n            idx = candidates_.argmax(-1)\n            b_i, s_i = np.ogrid[:B, : idx.shape[-1]]\n            nignore = notignore_mat[:, s_start:s_end]\n            logdelta[t, :, s_start:s_end][nignore] = (\n                candidates_[b_i, s_i, idx][nignore]\n                + lpz[t, b_i, ys_int[:, s_start:s_end]][nignore]\n            )\n            s = s_indx_mat[:, s_start:s_end]\n            state_path[t, :, s_start:s_end][nignore] = (s - idx)[nignore]\n\n        alignments = []\n        prev_states = logdelta[\n            ilens[:, None] - 1,\n            b_indx[:, None],\n            np.stack([olens_int - 2, olens_int - 1], -1),\n        ].argmax(-1)\n        for b in range(B):\n            T, L = ilens[b], olens_int[b]\n            prev_state = prev_states[b] + L - 2\n            ali = np.empty(T, dtype=ys_int.dtype)\n            ali[T - 1] = ys_int[b, prev_state]\n            for t in range(T - 2, -1, -1):\n                prev_state = state_path[t + 1, b, prev_state]\n                ali[t] = ys_int[b, prev_state]\n            alignments.append(ali)\n\n        return alignments","metadata":{"id":"tVaK6pp-rNUq","execution":{"iopub.status.busy":"2024-06-14T22:59:49.852735Z","iopub.execute_input":"2024-06-14T22:59:49.853567Z","iopub.status.idle":"2024-06-14T22:59:49.907539Z","shell.execute_reply.started":"2024-06-14T22:59:49.853533Z","shell.execute_reply":"2024-06-14T22:59:49.906675Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## E2E AV","metadata":{"id":"6-ciO8kw0Wda"}},{"cell_type":"code","source":"\"\"\"Transformer speech recognition model (pytorch).\"\"\"\n\n# import logging\n# import numpy\n# import torch\n\n# from espnet.nets.pytorch_backend.ctc import CTC\n# from espnet.nets.pytorch_backend.nets_utils import (\n#     make_non_pad_mask,\n#     th_accuracy,\n# )\n# from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n# from espnet.nets.pytorch_backend.transformer.decoder import Decoder\n# from espnet.nets.pytorch_backend.transformer.encoder import Encoder\n# from espnet.nets.pytorch_backend.transformer.label_smoothing_loss import LabelSmoothingLoss\n# from espnet.nets.pytorch_backend.transformer.mask import target_mask\n# from espnet.nets.pytorch_backend.nets_utils import MLPHead\n\n\nclass E2EM(torch.nn.Module):\n    def __init__(self, odim, args, ignore_id=-1):\n        torch.nn.Module.__init__(self)\n\n        self.encoder = Encoder(\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=args.transformer_input_layer,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            attention_dropout_rate=args.transformer_attn_dropout_rate,\n            encoder_attn_layer_type=args.transformer_encoder_attn_layer_type,\n            macaron_style=args.macaron_style,\n            use_cnn_module=args.use_cnn_module,\n            cnn_module_kernel=args.cnn_module_kernel,\n            zero_triu=getattr(args, \"zero_triu\", False),\n            a_upsample_ratio=args.a_upsample_ratio,\n            relu_type=getattr(args, \"relu_type\", \"swish\"),\n        )\n\n        self.aux_encoder = Encoder(\n            attention_dim=args.aux_adim,\n            attention_heads=args.aux_aheads,\n            linear_units=args.aux_eunits,\n            num_blocks=args.aux_elayers,\n            input_layer=args.aux_transformer_input_layer,\n            dropout_rate=args.aux_dropout_rate,\n            positional_dropout_rate=args.aux_dropout_rate,\n            attention_dropout_rate=args.aux_transformer_attn_dropout_rate,\n            encoder_attn_layer_type=args.aux_transformer_encoder_attn_layer_type,\n            macaron_style=args.aux_macaron_style,\n            use_cnn_module=args.aux_use_cnn_module,\n            cnn_module_kernel=args.aux_cnn_module_kernel,\n            zero_triu=getattr(args, \"aux_zero_triu\", False),\n            a_upsample_ratio=args.aux_a_upsample_ratio,\n            relu_type=getattr(args, \"aux_relu_type\", \"swish\"),\n        )\n\n        self.transformer_input_layer = args.transformer_input_layer\n        self.a_upsample_ratio = args.a_upsample_ratio\n\n        self.fusion = MLPHead(\n            idim=args.adim + args.aux_adim,\n            hdim=args.fusion_hdim,\n            odim=args.adim,\n            norm=args.fusion_norm,\n        )\n\n        self.proj_decoder = None\n        if args.adim != args.ddim:\n            self.proj_decoder = torch.nn.Linear(args.adim, args.ddim)\n\n        if args.mtlalpha < 1:\n            self.decoder = Decoder(\n                odim=odim,\n                attention_dim=args.ddim,\n                attention_heads=args.dheads,\n                linear_units=args.dunits,\n                num_blocks=args.dlayers,\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n                src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n            )\n        else:\n            self.decoder = None\n\n        self.blank = 0\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.odim = odim\n        self.ignore_id = ignore_id\n\n        # self.lsm_weight = a\n        self.criterion = LabelSmoothingLoss(\n            self.odim,\n            self.ignore_id,\n            args.lsm_weight,\n            args.transformer_length_normalized_loss,\n        )\n\n        self.adim = args.adim\n        self.mtlalpha = args.mtlalpha\n        if args.mtlalpha > 0.0:\n            self.ctc = CTC(\n                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n            )\n        else:\n            self.ctc = None\n\n    def forward(self, video, audio, video_lengths, audio_lengths, label):\n        video_padding_mask = make_non_pad_mask(video_lengths).to(video.device).unsqueeze(-2)\n        video_feat, _ = self.encoder(video, video_padding_mask)\n\n        audio_lengths = torch.div(audio_lengths, 640, rounding_mode=\"trunc\")\n        audio_padding_mask = make_non_pad_mask(audio_lengths).to(video.device).unsqueeze(-2)\n        audio_feat, _ = self.aux_encoder(audio, audio_padding_mask)\n\n        x = self.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n\n        # ctc loss\n        loss_ctc, ys_hat = self.ctc(x, video_lengths, label)\n\n        if self.proj_decoder:\n            x = self.proj_decoder(x)\n\n        # decoder loss\n        ys_in_pad, ys_out_pad = add_sos_eos(label, self.sos, self.eos, self.ignore_id)\n        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n        pred_pad, _ = self.decoder(ys_in_pad, ys_mask, x, video_padding_mask)\n        loss_att = self.criterion(pred_pad, ys_out_pad)\n        loss = self.mtlalpha * loss_ctc + (1 - self.mtlalpha) * loss_att\n\n        acc = th_accuracy(\n            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n        )\n\n        return loss, loss_ctc, loss_att, acc","metadata":{"id":"RSYDWjNC0V0o","execution":{"iopub.status.busy":"2024-06-14T23:00:03.685362Z","iopub.execute_input":"2024-06-14T23:00:03.685746Z","iopub.status.idle":"2024-06-14T23:00:03.708527Z","shell.execute_reply.started":"2024-06-14T23:00:03.685716Z","shell.execute_reply":"2024-06-14T23:00:03.707616Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## E2E","metadata":{"id":"tYvrFGxC06jI"}},{"cell_type":"code","source":"\"\"\"Transformer speech recognition model (pytorch).\"\"\"\n\n# import logging\n# import numpy\n# import torch\n\n# from espnet.nets.pytorch_backend.ctc import CTC\n# from espnet.nets.pytorch_backend.nets_utils import (\n#     make_non_pad_mask,\n#     th_accuracy,\n# )\n# from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n# from espnet.nets.pytorch_backend.transformer.decoder import Decoder\n# from espnet.nets.pytorch_backend.transformer.encoder import Encoder\n# from espnet.nets.pytorch_backend.transformer.label_smoothing_loss import LabelSmoothingLoss\n# from espnet.nets.pytorch_backend.transformer.mask import target_mask\n\n\nclass E2E(torch.nn.Module):\n    def __init__(self, odim, args, ignore_id=-1):\n        torch.nn.Module.__init__(self)\n\n        self.encoder = Encoder(\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=args.transformer_input_layer,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            attention_dropout_rate=args.transformer_attn_dropout_rate,\n            encoder_attn_layer_type=args.transformer_encoder_attn_layer_type,\n            macaron_style=args.macaron_style,\n            use_cnn_module=args.use_cnn_module,\n            cnn_module_kernel=args.cnn_module_kernel,\n            zero_triu=getattr(args, \"zero_triu\", False),\n            a_upsample_ratio=args.a_upsample_ratio,\n            relu_type=getattr(args, \"relu_type\", \"swish\"),\n        )\n\n        self.transformer_input_layer = args.transformer_input_layer\n        self.a_upsample_ratio = args.a_upsample_ratio\n\n        self.proj_decoder = None\n        if args.adim != args.ddim:\n            self.proj_decoder = torch.nn.Linear(args.adim, args.ddim)\n\n        if args.mtlalpha < 1:\n            self.decoder = Decoder(\n                odim=odim,\n                attention_dim=args.ddim,\n                attention_heads=args.dheads,\n                linear_units=args.dunits,\n                num_blocks=args.dlayers,\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n                src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n            )\n        else:\n            self.decoder = None\n        self.blank = 0\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.odim = odim\n        self.ignore_id = ignore_id\n\n        # self.lsm_weight = a\n        self.criterion = LabelSmoothingLoss(\n            self.odim,\n            self.ignore_id,\n            args.lsm_weight,\n            args.transformer_length_normalized_loss,\n        )\n\n        self.adim = args.adim\n        self.mtlalpha = args.mtlalpha\n        if args.mtlalpha > 0.0:\n            self.ctc = CTC(\n                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n            )\n        else:\n            self.ctc = None\n\n    def forward(self, x, lengths, label):\n        if self.transformer_input_layer == \"conv1d\":\n            lengths = torch.div(lengths, 640, rounding_mode=\"trunc\")\n        padding_mask = make_non_pad_mask(lengths).to(x.device).unsqueeze(-2)\n\n        x, _ = self.encoder(x, padding_mask)\n\n        # ctc loss\n        loss_ctc, ys_hat = self.ctc(x, lengths, label)\n\n        if self.proj_decoder:\n            x = self.proj_decoder(x)\n\n        # decoder loss\n        ys_in_pad, ys_out_pad = add_sos_eos(label, self.sos, self.eos, self.ignore_id)\n        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n        pred_pad, _ = self.decoder(ys_in_pad, ys_mask, x, padding_mask)\n        loss_att = self.criterion(pred_pad, ys_out_pad)\n        loss = self.mtlalpha * loss_ctc + (1 - self.mtlalpha) * loss_att\n\n        acc = th_accuracy(\n            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n        )\n\n        return loss, loss_ctc, loss_att, acc","metadata":{"id":"aRhg5iO508ZX","execution":{"iopub.status.busy":"2024-06-14T23:00:04.972886Z","iopub.execute_input":"2024-06-14T23:00:04.973732Z","iopub.status.idle":"2024-06-14T23:00:04.990746Z","shell.execute_reply.started":"2024-06-14T23:00:04.973700Z","shell.execute_reply":"2024-06-14T23:00:04.989726Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Cosine","metadata":{"id":"3nQEmdFqNcjT"}},{"cell_type":"code","source":"class WarmupCosineScheduler(_LRScheduler):\n    def __init__(self, optimizer, warmup_epochs, num_epochs, iter_per_epoch):\n        self.base_lrs = {\n            param_group[\"name\"]: param_group[\"lr\"]\n            for param_group in optimizer.param_groups\n        }\n        self.warmup_iter = warmup_epochs * iter_per_epoch\n        self.total_iter = num_epochs * iter_per_epoch\n        self.optimizer = optimizer\n        self.iter = 0\n        self.current_lr = 0\n\n        self.init_lr()  # so that at first step we have the correct step size\n\n    def get_lr(self, base_lr):\n        if self.iter < self.warmup_iter:\n            return base_lr * self.iter / self.warmup_iter\n        else:\n            decay_iter = self.total_iter - self.warmup_iter\n            return (\n                0.5\n                * base_lr\n                * (1 + np.cos(np.pi * (self.iter - self.warmup_iter) / decay_iter))\n            )\n\n    def update_param_groups(self):\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = self.get_lr(self.base_lrs[param_group[\"name\"]])\n\n    def step(self):\n        self.update_param_groups()\n        self.iter += 1\n\n    def init_lr(self):\n        self.update_param_groups()","metadata":{"id":"SdB5dFVWNfH9","execution":{"iopub.status.busy":"2024-06-14T23:00:06.184113Z","iopub.execute_input":"2024-06-14T23:00:06.184764Z","iopub.status.idle":"2024-06-14T23:00:06.193811Z","shell.execute_reply.started":"2024-06-14T23:00:06.184733Z","shell.execute_reply":"2024-06-14T23:00:06.192737Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## CTCPrefixScorer","metadata":{"id":"5bvueiRMObJw"}},{"cell_type":"code","source":"class CTCPrefixScoreTH(object):\n    \"\"\"Batch processing of CTCPrefixScore\n\n    which is based on Algorithm 2 in WATANABE et al.\n    \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\"\n    but extended to efficiently compute the label probablities for multiple\n    hypotheses simultaneously\n    See also Seki et al. \"Vectorized Beam Search for CTC-Attention-Based\n    Speech Recognition,\" In INTERSPEECH (pp. 3825-3829), 2019.\n    \"\"\"\n\n    def __init__(self, x, xlens, blank, eos, margin=0):\n        \"\"\"Construct CTC prefix scorer\n\n        :param torch.Tensor x: input label posterior sequences (B, T, O)\n        :param torch.Tensor xlens: input lengths (B,)\n        :param int blank: blank label id\n        :param int eos: end-of-sequence id\n        :param int margin: margin parameter for windowing (0 means no windowing)\n        \"\"\"\n        # In the comment lines,\n        # we assume T: input_length, B: batch size, W: beam width, O: output dim.\n        self.logzero = -10000000000.0\n        self.blank = blank\n        self.eos = eos\n        self.batch = x.size(0)\n        self.input_length = x.size(1)\n        self.odim = x.size(2)\n        self.dtype = x.dtype\n        self.device = (\n            torch.device(\"cuda:%d\" % x.get_device())\n            if x.is_cuda\n            else torch.device(\"cpu\")\n        )\n        # Pad the rest of posteriors in the batch\n        # TODO(takaaki-hori): need a better way without for-loops\n        for i, l in enumerate(xlens):\n            if l < self.input_length:\n                x[i, l:, :] = self.logzero\n                x[i, l:, blank] = 0\n        # Reshape input x\n        xn = x.transpose(0, 1)  # (B, T, O) -> (T, B, O)\n        xb = xn[:, :, self.blank].unsqueeze(2).expand(-1, -1, self.odim)\n        self.x = torch.stack([xn, xb])  # (2, T, B, O)\n        self.end_frames = torch.as_tensor(xlens) - 1\n\n        # Setup CTC windowing\n        self.margin = margin\n        if margin > 0:\n            self.frame_ids = torch.arange(\n                self.input_length, dtype=self.dtype, device=self.device\n            )\n        # Base indices for index conversion\n        self.idx_bh = None\n        self.idx_b = torch.arange(self.batch, device=self.device)\n        self.idx_bo = (self.idx_b * self.odim).unsqueeze(1)\n\n    def __call__(self, y, state, scoring_ids=None, att_w=None):\n        \"\"\"Compute CTC prefix scores for next labels\n\n        :param list y: prefix label sequences\n        :param tuple state: previous CTC state\n        :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O)\n        :param torch.Tensor att_w: attention weights to decide CTC window\n        :return new_state, ctc_local_scores (BW, O)\n        \"\"\"\n        output_length = len(y[0]) - 1  # ignore sos\n        last_ids = [yi[-1] for yi in y]  # last output label ids\n        n_bh = len(last_ids)  # batch * hyps\n        n_hyps = n_bh // self.batch  # assuming each utterance has the same # of hyps\n        self.scoring_num = scoring_ids.size(-1) if scoring_ids is not None else 0\n        # prepare state info\n        if state is None:\n            r_prev = torch.full(\n                (self.input_length, 2, self.batch, n_hyps),\n                self.logzero,\n                dtype=self.dtype,\n                device=self.device,\n            )\n            r_prev[:, 1] = torch.cumsum(self.x[0, :, :, self.blank], 0).unsqueeze(2)\n            r_prev = r_prev.view(-1, 2, n_bh)\n            s_prev = 0.0\n            f_min_prev = 0\n            f_max_prev = 1\n        else:\n            r_prev, s_prev, f_min_prev, f_max_prev = state\n\n        # select input dimensions for scoring\n        if self.scoring_num > 0:\n            scoring_idmap = torch.full(\n                (n_bh, self.odim), -1, dtype=torch.long, device=self.device\n            )\n            snum = self.scoring_num\n            if self.idx_bh is None or n_bh > len(self.idx_bh):\n                self.idx_bh = torch.arange(n_bh, device=self.device).view(-1, 1)\n            scoring_idmap[self.idx_bh[:n_bh], scoring_ids] = torch.arange(\n                snum, device=self.device\n            )\n            scoring_idx = (\n                scoring_ids + self.idx_bo.repeat(1, n_hyps).view(-1, 1)\n            ).view(-1)\n            x_ = torch.index_select(\n                self.x.view(2, -1, self.batch * self.odim), 2, scoring_idx\n            ).view(2, -1, n_bh, snum)\n        else:\n            scoring_ids = None\n            scoring_idmap = None\n            snum = self.odim\n            x_ = self.x.unsqueeze(3).repeat(1, 1, 1, n_hyps, 1).view(2, -1, n_bh, snum)\n\n        # new CTC forward probs are prepared as a (T x 2 x BW x S) tensor\n        # that corresponds to r_t^n(h) and r_t^b(h) in a batch.\n        r = torch.full(\n            (self.input_length, 2, n_bh, snum),\n            self.logzero,\n            dtype=self.dtype,\n            device=self.device,\n        )\n        if output_length == 0:\n            r[0, 0] = x_[0, 0]\n\n        r_sum = torch.logsumexp(r_prev, 1)\n        log_phi = r_sum.unsqueeze(2).repeat(1, 1, snum)\n        if scoring_ids is not None:\n            for idx in range(n_bh):\n                pos = scoring_idmap[idx, last_ids[idx]]\n                if pos >= 0:\n                    log_phi[:, idx, pos] = r_prev[:, 1, idx]\n        else:\n            for idx in range(n_bh):\n                log_phi[:, idx, last_ids[idx]] = r_prev[:, 1, idx]\n\n        # decide start and end frames based on attention weights\n        if att_w is not None and self.margin > 0:\n            f_arg = torch.matmul(att_w, self.frame_ids)\n            f_min = max(int(f_arg.min().cpu()), f_min_prev)\n            f_max = max(int(f_arg.max().cpu()), f_max_prev)\n            start = min(f_max_prev, max(f_min - self.margin, output_length, 1))\n            end = min(f_max + self.margin, self.input_length)\n        else:\n            f_min = f_max = 0\n            start = max(output_length, 1)\n            end = self.input_length\n\n        # compute forward probabilities log(r_t^n(h)) and log(r_t^b(h))\n        for t in range(start, end):\n            rp = r[t - 1]\n            rr = torch.stack([rp[0], log_phi[t - 1], rp[0], rp[1]]).view(\n                2, 2, n_bh, snum\n            )\n            r[t] = torch.logsumexp(rr, 1) + x_[:, t]\n\n        # compute log prefix probabilities log(psi)\n        log_phi_x = torch.cat((log_phi[0].unsqueeze(0), log_phi[:-1]), dim=0) + x_[0]\n        if scoring_ids is not None:\n            log_psi = torch.full(\n                (n_bh, self.odim), self.logzero, dtype=self.dtype, device=self.device\n            )\n            log_psi_ = torch.logsumexp(\n                torch.cat((log_phi_x[start:end], r[start - 1, 0].unsqueeze(0)), dim=0),\n                dim=0,\n            )\n            for si in range(n_bh):\n                log_psi[si, scoring_ids[si]] = log_psi_[si]\n        else:\n            log_psi = torch.logsumexp(\n                torch.cat((log_phi_x[start:end], r[start - 1, 0].unsqueeze(0)), dim=0),\n                dim=0,\n            )\n\n        for si in range(n_bh):\n            log_psi[si, self.eos] = r_sum[self.end_frames[si // n_hyps], si]\n\n        # exclude blank probs\n        log_psi[:, self.blank] = self.logzero\n\n        return (log_psi - s_prev), (r, log_psi, f_min, f_max, scoring_idmap)\n\n    def index_select_state(self, state, best_ids):\n        \"\"\"Select CTC states according to best ids\n\n        :param state    : CTC state\n        :param best_ids : index numbers selected by beam pruning (B, W)\n        :return selected_state\n        \"\"\"\n        r, s, f_min, f_max, scoring_idmap = state\n        # convert ids to BHO space\n        n_bh = len(s)\n        n_hyps = n_bh // self.batch\n        vidx = (best_ids + (self.idx_b * (n_hyps * self.odim)).view(-1, 1)).view(-1)\n        # select hypothesis scores\n        s_new = torch.index_select(s.view(-1), 0, vidx)\n        s_new = s_new.view(-1, 1).repeat(1, self.odim).view(n_bh, self.odim)\n        # convert ids to BHS space (S: scoring_num)\n        if scoring_idmap is not None:\n            snum = self.scoring_num\n            hyp_idx = (best_ids // self.odim + (self.idx_b * n_hyps).view(-1, 1)).view(\n                -1\n            )\n            label_ids = torch.fmod(best_ids, self.odim).view(-1)\n            score_idx = scoring_idmap[hyp_idx, label_ids]\n            score_idx[score_idx == -1] = 0\n            vidx = score_idx + hyp_idx * snum\n        else:\n            snum = self.odim\n        # select forward probabilities\n        r_new = torch.index_select(r.view(-1, 2, n_bh * snum), 2, vidx).view(\n            -1, 2, n_bh\n        )\n        return r_new, s_new, f_min, f_max\n\n    def extend_prob(self, x):\n        \"\"\"Extend CTC prob.\n\n        :param torch.Tensor x: input label posterior sequences (B, T, O)\n        \"\"\"\n\n        if self.x.shape[1] < x.shape[1]:  # self.x (2,T,B,O); x (B,T,O)\n            # Pad the rest of posteriors in the batch\n            # TODO(takaaki-hori): need a better way without for-loops\n            xlens = [x.size(1)]\n            for i, l in enumerate(xlens):\n                if l < self.input_length:\n                    x[i, l:, :] = self.logzero\n                    x[i, l:, self.blank] = 0\n            tmp_x = self.x\n            xn = x.transpose(0, 1)  # (B, T, O) -> (T, B, O)\n            xb = xn[:, :, self.blank].unsqueeze(2).expand(-1, -1, self.odim)\n            self.x = torch.stack([xn, xb])  # (2, T, B, O)\n            self.x[:, : tmp_x.shape[1], :, :] = tmp_x\n            self.input_length = x.size(1)\n            self.end_frames = torch.as_tensor(xlens) - 1\n\n    def extend_state(self, state):\n        \"\"\"Compute CTC prefix state.\n\n\n        :param state    : CTC state\n        :return ctc_state\n        \"\"\"\n\n        if state is None:\n            # nothing to do\n            return state\n        else:\n            r_prev, s_prev, f_min_prev, f_max_prev = state\n\n            r_prev_new = torch.full(\n                (self.input_length, 2),\n                self.logzero,\n                dtype=self.dtype,\n                device=self.device,\n            )\n            start = max(r_prev.shape[0], 1)\n            r_prev_new[0:start] = r_prev\n            for t in six.moves.range(start, self.input_length):\n                r_prev_new[t, 1] = r_prev_new[t - 1, 1] + self.x[0, t, :, self.blank]\n\n            return (r_prev_new, s_prev, f_min_prev, f_max_prev)\n\n\nclass CTCPrefixScore(object):\n    \"\"\"Compute CTC label sequence scores\n\n    which is based on Algorithm 2 in WATANABE et al.\n    \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\"\n    but extended to efficiently compute the probablities of multiple labels\n    simultaneously\n    \"\"\"\n\n    def __init__(self, x, blank, eos, xp):\n        self.xp = xp\n        self.logzero = -10000000000.0\n        self.blank = blank\n        self.eos = eos\n        self.input_length = len(x)\n        self.x = x\n\n    def initial_state(self):\n        \"\"\"Obtain an initial CTC state\n\n        :return: CTC state\n        \"\"\"\n        # initial CTC state is made of a frame x 2 tensor that corresponds to\n        # r_t^n(<sos>) and r_t^b(<sos>), where 0 and 1 of axis=1 represent\n        # superscripts n and b (non-blank and blank), respectively.\n        r = self.xp.full((self.input_length, 2), self.logzero, dtype=np.float32)\n        r[0, 1] = self.x[0, self.blank]\n        for i in six.moves.range(1, self.input_length):\n            r[i, 1] = r[i - 1, 1] + self.x[i, self.blank]\n        return r\n\n    def __call__(self, y, cs, r_prev):\n        \"\"\"Compute CTC prefix scores for next labels\n\n        :param y     : prefix label sequence\n        :param cs    : array of next labels\n        :param r_prev: previous CTC state\n        :return ctc_scores, ctc_states\n        \"\"\"\n        # initialize CTC states\n        output_length = len(y) - 1  # ignore sos\n        # new CTC states are prepared as a frame x (n or b) x n_labels tensor\n        # that corresponds to r_t^n(h) and r_t^b(h).\n        r = self.xp.ndarray((self.input_length, 2, len(cs)), dtype=np.float32)\n        xs = self.x[:, cs]\n        if output_length == 0:\n            r[0, 0] = xs[0]\n            r[0, 1] = self.logzero\n        else:\n            r[output_length - 1] = self.logzero\n\n        # prepare forward probabilities for the last label\n        r_sum = self.xp.logaddexp(\n            r_prev[:, 0], r_prev[:, 1]\n        )  # log(r_t^n(g) + r_t^b(g))\n        last = y[-1]\n        if output_length > 0 and last in cs:\n            log_phi = self.xp.ndarray((self.input_length, len(cs)), dtype=np.float32)\n            for i in six.moves.range(len(cs)):\n                log_phi[:, i] = r_sum if cs[i] != last else r_prev[:, 1]\n        else:\n            log_phi = r_sum\n\n        # compute forward probabilities log(r_t^n(h)), log(r_t^b(h)),\n        # and log prefix probabilities log(psi)\n        start = max(output_length, 1)\n        log_psi = r[start - 1, 0]\n        for t in six.moves.range(start, self.input_length):\n            r[t, 0] = self.xp.logaddexp(r[t - 1, 0], log_phi[t - 1]) + xs[t]\n            r[t, 1] = (\n                self.xp.logaddexp(r[t - 1, 0], r[t - 1, 1]) + self.x[t, self.blank]\n            )\n            log_psi = self.xp.logaddexp(log_psi, log_phi[t - 1] + xs[t])\n\n        # get P(...eos|X) that ends with the prefix itself\n        eos_pos = self.xp.where(cs == self.eos)[0]\n        if len(eos_pos) > 0:\n            log_psi[eos_pos] = r_sum[-1]  # log(r_T^n(g) + r_T^b(g))\n\n        # exclude blank probs\n        blank_pos = self.xp.where(cs == self.blank)[0]\n        if len(blank_pos) > 0:\n            log_psi[blank_pos] = self.logzero\n\n        # return the log prefix probability and CTC states, where the label axis\n        # of the CTC states is moved to the first axis to slice it easily\n        return log_psi, self.xp.rollaxis(r, 2)","metadata":{"id":"pyvw0k0AQCTS","execution":{"iopub.status.busy":"2024-06-14T23:00:07.448592Z","iopub.execute_input":"2024-06-14T23:00:07.448931Z","iopub.status.idle":"2024-06-14T23:00:07.507795Z","shell.execute_reply.started":"2024-06-14T23:00:07.448907Z","shell.execute_reply":"2024-06-14T23:00:07.506641Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class CTCPrefixScorer(BatchPartialScorerInterface):\n    \"\"\"Decoder interface wrapper for CTCPrefixScore.\"\"\"\n\n    def __init__(self, ctc: torch.nn.Module, eos: int):\n        \"\"\"Initialize class.\n\n        Args:\n            ctc (torch.nn.Module): The CTC implementation.\n                For example, :class:`espnet.nets.pytorch_backend.ctc.CTC`\n            eos (int): The end-of-sequence id.\n\n        \"\"\"\n        self.ctc = ctc\n        self.eos = eos\n        self.impl = None\n\n    def init_state(self, x: torch.Tensor):\n        \"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"\n        logp = self.ctc.log_softmax(x.unsqueeze(0)).detach().squeeze(0).cpu().numpy()\n        # TODO(karita): use CTCPrefixScoreTH\n        self.impl = CTCPrefixScore(logp, 0, self.eos, np)\n        return 0, self.impl.initial_state()\n\n    def select_state(self, state, i, new_id=None):\n        \"\"\"Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n            new_id (int): New label id to select a state if necessary\n\n        Returns:\n            state: pruned state\n\n        \"\"\"\n        if type(state) == tuple:\n            if len(state) == 2:  # for CTCPrefixScore\n                sc, st = state\n                return sc[i], st[i]\n            else:  # for CTCPrefixScoreTH (need new_id > 0)\n                r, log_psi, f_min, f_max, scoring_idmap = state\n                s = log_psi[i, new_id].expand(log_psi.size(1))\n                if scoring_idmap is not None:\n                    return r[:, :, i, scoring_idmap[i, new_id]], s, f_min, f_max\n                else:\n                    return r[:, :, i, new_id], s, f_min, f_max\n        return None if state is None else state[i]\n\n    def score_partial(self, y, ids, state, x):\n        \"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            next_tokens (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"\n        prev_score, state = state\n        presub_score, new_st = self.impl(y.cpu(), ids.cpu(), state)\n        tscore = torch.as_tensor(\n            presub_score - prev_score, device=x.device, dtype=x.dtype\n        )\n        return tscore, (presub_score, new_st)\n\n    def batch_init_state(self, x: torch.Tensor):\n        \"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"\n        logp = self.ctc.log_softmax(x.unsqueeze(0))  # assuming batch_size = 1\n        xlen = torch.tensor([logp.size(1)])\n        self.impl = CTCPrefixScoreTH(logp, xlen, 0, self.eos)\n        return None\n\n    def batch_score_partial(self, y, ids, state, x):\n        \"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            ids (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"\n        batch_state = (\n            (\n                torch.stack([s[0] for s in state], dim=2),\n                torch.stack([s[1] for s in state]),\n                state[0][2],\n                state[0][3],\n            )\n            if state[0] is not None\n            else None\n        )\n        return self.impl(y, batch_state, ids)\n\n    def extend_prob(self, x: torch.Tensor):\n        \"\"\"Extend probs for decoding.\n\n        This extension is for streaming decoding\n        as in Eq (14) in https://arxiv.org/abs/2006.14941\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        \"\"\"\n        logp = self.ctc.log_softmax(x.unsqueeze(0))\n        self.impl.extend_prob(logp)\n\n    def extend_state(self, state):\n        \"\"\"Extend state for decoding.\n\n        This extension is for streaming decoding\n        as in Eq (14) in https://arxiv.org/abs/2006.14941\n\n        Args:\n            state: The states of hyps\n\n        Returns: exteded state\n\n        \"\"\"\n        new_state = []\n        for s in state:\n            new_state.append(self.impl.extend_state(s))\n\n        return new_state","metadata":{"id":"V0LFm_JLOcD4","execution":{"iopub.status.busy":"2024-06-14T23:00:08.211122Z","iopub.execute_input":"2024-06-14T23:00:08.211858Z","iopub.status.idle":"2024-06-14T23:00:08.229841Z","shell.execute_reply.started":"2024-06-14T23:00:08.211829Z","shell.execute_reply":"2024-06-14T23:00:08.228743Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## LengthBonus","metadata":{"id":"gZ980wJWQULi"}},{"cell_type":"code","source":"class LengthBonus(BatchScorerInterface):\n    \"\"\"Length bonus in beam search.\"\"\"\n\n    def __init__(self, n_vocab: int):\n        \"\"\"Initialize class.\n\n        Args:\n            n_vocab (int): The number of tokens in vocabulary for beam search\n\n        \"\"\"\n        self.n = n_vocab\n\n    def score(self, y, state, x):\n        \"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and None\n\n        \"\"\"\n        return torch.tensor([1.0], device=x.device, dtype=x.dtype).expand(self.n), None\n\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        \"\"\"Score new token batch.\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        \"\"\"\n        return (\n            torch.tensor([1.0], device=xs.device, dtype=xs.dtype).expand(\n                ys.shape[0], self.n\n            ),\n            None,\n        )","metadata":{"id":"qE_RPRxTQVED","execution":{"iopub.status.busy":"2024-06-14T23:00:09.950245Z","iopub.execute_input":"2024-06-14T23:00:09.950673Z","iopub.status.idle":"2024-06-14T23:00:09.959339Z","shell.execute_reply.started":"2024-06-14T23:00:09.950640Z","shell.execute_reply":"2024-06-14T23:00:09.958274Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## BeamSearch","metadata":{"id":"0nHmNDnWQxcu"}},{"cell_type":"code","source":"def end_detect(ended_hyps, i, M=3, D_end=np.log(1 * np.exp(-10))):\n    \"\"\"End detection.\n\n    described in Eq. (50) of S. Watanabe et al\n    \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\"\n\n    :param ended_hyps:\n    :param i:\n    :param M:\n    :param D_end:\n    :return:\n    \"\"\"\n    if len(ended_hyps) == 0:\n        return False\n    count = 0\n    best_hyp = sorted(ended_hyps, key=lambda x: x[\"score\"], reverse=True)[0]\n    for m in range(M):\n        # get ended_hyps with their length is i - m\n        hyp_length = i - m\n        hyps_same_length = [x for x in ended_hyps if len(x[\"yseq\"]) == hyp_length]\n        if len(hyps_same_length) > 0:\n            best_hyp_same_length = sorted(\n                hyps_same_length, key=lambda x: x[\"score\"], reverse=True\n            )[0]\n            if best_hyp_same_length[\"score\"] - best_hyp[\"score\"] < D_end:\n                count += 1\n\n    if count == M:\n        return True\n    else:\n        return False\n\n\nclass Hypothesis(NamedTuple):\n    \"\"\"Hypothesis data type.\"\"\"\n\n    yseq: torch.Tensor\n    score: Union[float, torch.Tensor] = 0\n    scores: Dict[str, Union[float, torch.Tensor]] = dict()\n    states: Dict[str, Any] = dict()\n\n    def asdict(self) -> dict:\n        \"\"\"Convert data to JSON-friendly dict.\"\"\"\n        return self._replace(\n            yseq=self.yseq.tolist(),\n            score=float(self.score),\n            scores={k: float(v) for k, v in self.scores.items()},\n        )._asdict()\n\n\nclass BeamSearch(torch.nn.Module):\n    \"\"\"Beam search implementation.\"\"\"\n\n    def __init__(\n        self,\n        scorers: Dict[str, ScorerInterface],\n        weights: Dict[str, float],\n        beam_size: int,\n        vocab_size: int,\n        sos: int,\n        eos: int,\n        token_list: List[str] = None,\n        pre_beam_ratio: float = 1.5,\n        pre_beam_score_key: str = None,\n    ):\n        \"\"\"Initialize beam search.\n\n        Args:\n            scorers (dict[str, ScorerInterface]): Dict of decoder modules\n                e.g., Decoder, CTCPrefixScorer, LM\n                The scorer will be ignored if it is `None`\n            weights (dict[str, float]): Dict of weights for each scorers\n                The scorer will be ignored if its weight is 0\n            beam_size (int): The number of hypotheses kept during search\n            vocab_size (int): The number of vocabulary\n            sos (int): Start of sequence id\n            eos (int): End of sequence id\n            token_list (list[str]): List of tokens for debug log\n            pre_beam_score_key (str): key of scores to perform pre-beam search\n            pre_beam_ratio (float): beam size in the pre-beam search\n                will be `int(pre_beam_ratio * beam_size)`\n\n        \"\"\"\n        super().__init__()\n        # set scorers\n        self.weights = weights\n        self.scorers = dict()\n        self.full_scorers = dict()\n        self.part_scorers = dict()\n        # this module dict is required for recursive cast\n        # `self.to(device, dtype)` in `recog.py`\n        self.nn_dict = torch.nn.ModuleDict()\n        for k, v in scorers.items():\n            w = weights.get(k, 0)\n            if w == 0 or v is None:\n                continue\n            assert isinstance(\n                v, ScorerInterface\n            ), f\"{k} ({type(v)}) does not implement ScorerInterface\"\n            self.scorers[k] = v\n            if isinstance(v, PartialScorerInterface):\n                self.part_scorers[k] = v\n            else:\n                self.full_scorers[k] = v\n            if isinstance(v, torch.nn.Module):\n                self.nn_dict[k] = v\n\n        # set configurations\n        self.sos = sos\n        self.eos = eos\n        self.token_list = token_list\n        self.pre_beam_size = int(pre_beam_ratio * beam_size)\n        self.beam_size = beam_size\n        self.n_vocab = vocab_size\n        if (\n            pre_beam_score_key is not None\n            and pre_beam_score_key != \"full\"\n            and pre_beam_score_key not in self.full_scorers\n        ):\n            raise KeyError(f\"{pre_beam_score_key} is not found in {self.full_scorers}\")\n        self.pre_beam_score_key = pre_beam_score_key\n        self.do_pre_beam = (\n            self.pre_beam_score_key is not None\n            and self.pre_beam_size < self.n_vocab\n            and len(self.part_scorers) > 0\n        )\n\n    def init_hyp(self, x: torch.Tensor) -> List[Hypothesis]:\n        \"\"\"Get an initial hypothesis data.\n\n        Args:\n            x (torch.Tensor): The encoder output feature\n\n        Returns:\n            Hypothesis: The initial hypothesis.\n\n        \"\"\"\n        init_states = dict()\n        init_scores = dict()\n        for k, d in self.scorers.items():\n            init_states[k] = d.init_state(x)\n            init_scores[k] = 0.0\n        return [\n            Hypothesis(\n                score=0.0,\n                scores=init_scores,\n                states=init_states,\n                yseq=torch.tensor([self.sos], device=x.device),\n            )\n        ]\n\n    @staticmethod\n    def append_token(xs: torch.Tensor, x: int) -> torch.Tensor:\n        \"\"\"Append new token to prefix tokens.\n\n        Args:\n            xs (torch.Tensor): The prefix token\n            x (int): The new token to append\n\n        Returns:\n            torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device\n\n        \"\"\"\n        x = torch.tensor([x], dtype=xs.dtype, device=xs.device)\n        return torch.cat((xs, x))\n\n    def score_full(\n        self, hyp: Hypothesis, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        \"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"\n        scores = dict()\n        states = dict()\n        for k, d in self.full_scorers.items():\n            scores[k], states[k] = d.score(hyp.yseq, hyp.states[k], x)\n        return scores, states\n\n    def score_partial(\n        self, hyp: Hypothesis, ids: torch.Tensor, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        \"\"\"Score new hypothesis by `self.part_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            ids (torch.Tensor): 1D tensor of new partial tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.part_scorers`\n                and tensor score values of shape: `(len(ids),)`,\n                and state dict that has string keys\n                and state values of `self.part_scorers`\n\n        \"\"\"\n        scores = dict()\n        states = dict()\n        for k, d in self.part_scorers.items():\n            scores[k], states[k] = d.score_partial(hyp.yseq, ids, hyp.states[k], x)\n        return scores, states\n\n    def beam(\n        self, weighted_scores: torch.Tensor, ids: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute topk full token ids and partial token ids.\n\n        Args:\n            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n            Its shape is `(self.n_vocab,)`.\n            ids (torch.Tensor): The partial token ids to compute topk\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]:\n                The topk full token ids and partial token ids.\n                Their shapes are `(self.beam_size,)`\n\n        \"\"\"\n        # no pre beam performed\n        if weighted_scores.size(0) == ids.size(0):\n            top_ids = weighted_scores.topk(self.beam_size)[1]\n            return top_ids, top_ids\n\n        # mask pruned in pre-beam not to select in topk\n        tmp = weighted_scores[ids]\n        weighted_scores[:] = -float(\"inf\")\n        weighted_scores[ids] = tmp\n        top_ids = weighted_scores.topk(self.beam_size)[1]\n        local_ids = weighted_scores[ids].topk(self.beam_size)[1]\n        return top_ids, local_ids\n\n    @staticmethod\n    def merge_scores(\n        prev_scores: Dict[str, float],\n        next_full_scores: Dict[str, torch.Tensor],\n        full_idx: int,\n        next_part_scores: Dict[str, torch.Tensor],\n        part_idx: int,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Merge scores for new hypothesis.\n\n        Args:\n            prev_scores (Dict[str, float]):\n                The previous hypothesis scores by `self.scorers`\n            next_full_scores (Dict[str, torch.Tensor]): scores by `self.full_scorers`\n            full_idx (int): The next token id for `next_full_scores`\n            next_part_scores (Dict[str, torch.Tensor]):\n                scores of partial tokens by `self.part_scorers`\n            part_idx (int): The new token id for `next_part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are scalar tensors by the scorers.\n\n        \"\"\"\n        new_scores = dict()\n        for k, v in next_full_scores.items():\n            new_scores[k] = prev_scores[k] + v[full_idx]\n        for k, v in next_part_scores.items():\n            new_scores[k] = prev_scores[k] + v[part_idx]\n        return new_scores\n\n    def merge_states(self, states: Any, part_states: Any, part_idx: int) -> Any:\n        \"\"\"Merge states for new hypothesis.\n\n        Args:\n            states: states of `self.full_scorers`\n            part_states: states of `self.part_scorers`\n            part_idx (int): The new token id for `part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are states of the scorers.\n\n        \"\"\"\n        new_states = dict()\n        for k, v in states.items():\n            new_states[k] = v\n        for k, d in self.part_scorers.items():\n            new_states[k] = d.select_state(part_states[k], part_idx)\n        return new_states\n\n    def search(\n        self, running_hyps: List[Hypothesis], x: torch.Tensor\n    ) -> List[Hypothesis]:\n        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n\n        Args:\n            running_hyps (List[Hypothesis]): Running hypotheses on beam\n            x (torch.Tensor): Encoded speech feature (T, D)\n\n        Returns:\n            List[Hypotheses]: Best sorted hypotheses\n\n        \"\"\"\n        best_hyps = []\n        part_ids = torch.arange(self.n_vocab, device=x.device)  # no pre-beam\n        for hyp in running_hyps:\n            # scoring\n            weighted_scores = torch.zeros(self.n_vocab, dtype=x.dtype, device=x.device)\n            scores, states = self.score_full(hyp, x)\n            for k in self.full_scorers:\n                weighted_scores += self.weights[k] * scores[k]\n            # partial scoring\n            if self.do_pre_beam:\n                pre_beam_scores = (\n                    weighted_scores\n                    if self.pre_beam_score_key == \"full\"\n                    else scores[self.pre_beam_score_key]\n                )\n                part_ids = torch.topk(pre_beam_scores, self.pre_beam_size)[1]\n            part_scores, part_states = self.score_partial(hyp, part_ids, x)\n            for k in self.part_scorers:\n                weighted_scores[part_ids] += self.weights[k] * part_scores[k]\n            # add previous hyp score\n            weighted_scores += hyp.score\n\n            # update hyps\n            for j, part_j in zip(*self.beam(weighted_scores, part_ids)):\n                # will be (2 x beam at most)\n                best_hyps.append(\n                    Hypothesis(\n                        score=weighted_scores[j],\n                        yseq=self.append_token(hyp.yseq, j),\n                        scores=self.merge_scores(\n                            hyp.scores, scores, j, part_scores, part_j\n                        ),\n                        states=self.merge_states(states, part_states, part_j),\n                    )\n                )\n\n            # sort and prune 2 x beam -> beam\n            best_hyps = sorted(best_hyps, key=lambda x: x.score, reverse=True)[\n                : min(len(best_hyps), self.beam_size)\n            ]\n        return best_hyps\n\n    def forward(\n        self, x: torch.Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0\n    ) -> List[Hypothesis]:\n        \"\"\"Perform beam search.\n\n        Args:\n            x (torch.Tensor): Encoded speech feature (T, D)\n            maxlenratio (float): Input length ratio to obtain max output length.\n                If maxlenratio=0.0 (default), it uses a end-detect function\n                to automatically find maximum hypothesis lengths\n                If maxlenratio<0.0, its absolute value is interpreted\n                as a constant max output length.\n            minlenratio (float): Input length ratio to obtain min output length.\n\n        Returns:\n            list[Hypothesis]: N-best decoding results\n\n        \"\"\"\n        # set length bounds\n        if maxlenratio == 0:\n            maxlen = x.shape[0]\n        elif maxlenratio < 0:\n            maxlen = -1 * int(maxlenratio)\n        else:\n            maxlen = max(1, int(maxlenratio * x.size(0)))\n        minlen = int(minlenratio * x.size(0))\n        logging.debug(\"decoder input length: \" + str(x.shape[0]))\n        logging.debug(\"max output length: \" + str(maxlen))\n        logging.debug(\"min output length: \" + str(minlen))\n\n        # main loop of prefix search\n        running_hyps = self.init_hyp(x)\n        ended_hyps = []\n        for i in range(maxlen):\n            logging.debug(\"position \" + str(i))\n            best = self.search(running_hyps, x)\n            # post process of one iteration\n            running_hyps = self.post_process(i, maxlen, maxlenratio, best, ended_hyps)\n            # end detection\n            if maxlenratio == 0.0 and end_detect([h.asdict() for h in ended_hyps], i):\n                logging.debug(f\"end detected at {i}\")\n                break\n            if len(running_hyps) == 0:\n                logging.debug(\"no hypothesis. Finish decoding.\")\n                break\n            else:\n                logging.debug(f\"remained hypotheses: {len(running_hyps)}\")\n\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x.score, reverse=True)\n        # check the number of hypotheses reaching to eos\n        if len(nbest_hyps) == 0:\n            logging.warning(\n                \"there is no N-best results, perform recognition \"\n                \"again with smaller minlenratio.\"\n            )\n            return (\n                []\n                if minlenratio < 0.1\n                else self.forward(x, maxlenratio, max(0.0, minlenratio - 0.1))\n            )\n\n        # report the best result\n        best = nbest_hyps[0]\n        for k, v in best.scores.items():\n            logging.debug(\n                f\"{v:6.2f} * {self.weights[k]:3} = {v * self.weights[k]:6.2f} for {k}\"\n            )\n        logging.debug(f\"total log probability: {best.score:.2f}\")\n        logging.debug(f\"normalized log probability: {best.score / len(best.yseq):.2f}\")\n        logging.debug(f\"total number of ended hypotheses: {len(nbest_hyps)}\")\n        if self.token_list is not None:\n            logging.debug(\n                \"best hypo: \"\n                + \"\".join([self.token_list[x] for x in best.yseq[1:-1]])\n                + \"\\n\"\n            )\n        return nbest_hyps\n\n    def post_process(\n        self,\n        i: int,\n        maxlen: int,\n        maxlenratio: float,\n        running_hyps: List[Hypothesis],\n        ended_hyps: List[Hypothesis],\n    ) -> List[Hypothesis]:\n        \"\"\"Perform post-processing of beam search iterations.\n\n        Args:\n            i (int): The length of hypothesis tokens.\n            maxlen (int): The maximum length of tokens in beam search.\n            maxlenratio (int): The maximum length ratio in beam search.\n            running_hyps (List[Hypothesis]): The running hypotheses in beam search.\n            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n\n        Returns:\n            List[Hypothesis]: The new running hypotheses.\n\n        \"\"\"\n        logging.debug(f\"the number of running hypotheses: {len(running_hyps)}\")\n        if self.token_list is not None:\n            logging.debug(\n                \"best hypo: \"\n                + \"\".join([self.token_list[x] for x in running_hyps[0].yseq[1:]])\n            )\n        # add eos in the final loop to avoid that there are no ended hyps\n        if i == maxlen - 1:\n            logging.debug(\"adding <eos> in the last position in the loop\")\n            running_hyps = [\n                h._replace(yseq=self.append_token(h.yseq, self.eos))\n                for h in running_hyps\n            ]\n\n        # add ended hypotheses to a final list, and removed them from current hypotheses\n        # (this will be a problem, number of hyps < beam)\n        remained_hyps = []\n        for hyp in running_hyps:\n            if hyp.yseq[-1] == self.eos:\n                # e.g., Word LM needs to add final <eos> score\n                for k, d in chain(self.full_scorers.items(), self.part_scorers.items()):\n                    s = d.final_score(hyp.states[k])\n                    hyp.scores[k] += s\n                    hyp = hyp._replace(score=hyp.score + self.weights[k] * s)\n                ended_hyps.append(hyp)\n            else:\n                remained_hyps.append(hyp)\n        return remained_hyps","metadata":{"id":"OJ9U-Ge2QyfD","execution":{"iopub.status.busy":"2024-06-14T23:00:11.080056Z","iopub.execute_input":"2024-06-14T23:00:11.080391Z","iopub.status.idle":"2024-06-14T23:00:11.140399Z","shell.execute_reply.started":"2024-06-14T23:00:11.080365Z","shell.execute_reply":"2024-06-14T23:00:11.139310Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## BatchBeamSearch","metadata":{"id":"9KU4cv09Qrkz"}},{"cell_type":"code","source":"class BatchHypothesis(NamedTuple):\n    \"\"\"Batchfied/Vectorized hypothesis data type.\"\"\"\n\n    yseq: torch.Tensor = torch.tensor([])  # (batch, maxlen)\n    score: torch.Tensor = torch.tensor([])  # (batch,)\n    length: torch.Tensor = torch.tensor([])  # (batch,)\n    scores: Dict[str, torch.Tensor] = dict()  # values: (batch,)\n    states: Dict[str, Dict] = dict()\n\n    def __len__(self) -> int:\n        \"\"\"Return a batch size.\"\"\"\n        return len(self.length)\n\n\nclass BatchBeamSearch(BeamSearch):\n    \"\"\"Batch beam search implementation.\"\"\"\n\n    def batchfy(self, hyps: List[Hypothesis]) -> BatchHypothesis:\n        \"\"\"Convert list to batch.\"\"\"\n        if len(hyps) == 0:\n            return BatchHypothesis()\n        yseq = pad_sequence(\n            [h.yseq for h in hyps], batch_first=True, padding_value=self.eos\n        )\n        return BatchHypothesis(\n            yseq=yseq,\n            length=torch.tensor(\n                [len(h.yseq) for h in hyps], dtype=torch.int64, device=yseq.device\n            ),\n            score=torch.tensor([h.score for h in hyps]).to(yseq.device),\n            scores={\n                k: torch.tensor([h.scores[k] for h in hyps], device=yseq.device)\n                for k in self.scorers\n            },\n            states={k: [h.states[k] for h in hyps] for k in self.scorers},\n        )\n\n    def _batch_select(self, hyps: BatchHypothesis, ids: List[int]) -> BatchHypothesis:\n        return BatchHypothesis(\n            yseq=hyps.yseq[ids],\n            score=hyps.score[ids],\n            length=hyps.length[ids],\n            scores={k: v[ids] for k, v in hyps.scores.items()},\n            states={\n                k: [self.scorers[k].select_state(v, i) for i in ids]\n                for k, v in hyps.states.items()\n            },\n        )\n\n    def _select(self, hyps: BatchHypothesis, i: int) -> Hypothesis:\n        return Hypothesis(\n            yseq=hyps.yseq[i, : hyps.length[i]],\n            score=hyps.score[i],\n            scores={k: v[i] for k, v in hyps.scores.items()},\n            states={\n                k: self.scorers[k].select_state(v, i) for k, v in hyps.states.items()\n            },\n        )\n\n    def unbatchfy(self, batch_hyps: BatchHypothesis) -> List[Hypothesis]:\n        \"\"\"Revert batch to list.\"\"\"\n        return [\n            Hypothesis(\n                yseq=batch_hyps.yseq[i][: batch_hyps.length[i]],\n                score=batch_hyps.score[i],\n                scores={k: batch_hyps.scores[k][i] for k in self.scorers},\n                states={\n                    k: v.select_state(batch_hyps.states[k], i)\n                    for k, v in self.scorers.items()\n                },\n            )\n            for i in range(len(batch_hyps.length))\n        ]\n\n    def batch_beam(\n        self, weighted_scores: torch.Tensor, ids: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Batch-compute topk full token ids and partial token ids.\n\n        Args:\n            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n                Its shape is `(n_beam, self.vocab_size)`.\n            ids (torch.Tensor): The partial token ids to compute topk.\n                Its shape is `(n_beam, self.pre_beam_size)`.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n                The topk full (prev_hyp, new_token) ids\n                and partial (prev_hyp, new_token) ids.\n                Their shapes are all `(self.beam_size,)`\n\n        \"\"\"\n        top_ids = weighted_scores.view(-1).topk(self.beam_size)[1]\n        # Because of the flatten above, `top_ids` is organized as:\n        # [hyp1 * V + token1, hyp2 * V + token2, ..., hypK * V + tokenK],\n        # where V is `self.n_vocab` and K is `self.beam_size`\n        prev_hyp_ids = torch.div(top_ids, self.n_vocab, rounding_mode=\"trunc\")\n        new_token_ids = top_ids % self.n_vocab\n        return prev_hyp_ids, new_token_ids, prev_hyp_ids, new_token_ids\n\n    def init_hyp(self, x: torch.Tensor) -> BatchHypothesis:\n        \"\"\"Get an initial hypothesis data.\n\n        Args:\n            x (torch.Tensor): The encoder output feature\n\n        Returns:\n            Hypothesis: The initial hypothesis.\n\n        \"\"\"\n        init_states = dict()\n        init_scores = dict()\n        for k, d in self.scorers.items():\n            init_states[k] = d.batch_init_state(x)\n            init_scores[k] = 0.0\n        return self.batchfy(\n            [\n                Hypothesis(\n                    score=0.0,\n                    scores=init_scores,\n                    states=init_states,\n                    yseq=torch.tensor([self.sos], device=x.device),\n                )\n            ]\n        )\n\n    def score_full(\n        self, hyp: BatchHypothesis, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        \"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"\n        scores = dict()\n        states = dict()\n        for k, d in self.full_scorers.items():\n            scores[k], states[k] = d.batch_score(hyp.yseq, hyp.states[k], x)\n        return scores, states\n\n    def score_partial(\n        self, hyp: BatchHypothesis, ids: torch.Tensor, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        \"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            ids (torch.Tensor): 2D tensor of new partial tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"\n        scores = dict()\n        states = dict()\n        for k, d in self.part_scorers.items():\n            scores[k], states[k] = d.batch_score_partial(\n                hyp.yseq, ids, hyp.states[k], x\n            )\n        return scores, states\n\n    def merge_states(self, states: Any, part_states: Any, part_idx: int) -> Any:\n        \"\"\"Merge states for new hypothesis.\n\n        Args:\n            states: states of `self.full_scorers`\n            part_states: states of `self.part_scorers`\n            part_idx (int): The new token id for `part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are states of the scorers.\n\n        \"\"\"\n        new_states = dict()\n        for k, v in states.items():\n            new_states[k] = v\n        for k, v in part_states.items():\n            new_states[k] = v\n        return new_states\n\n    def search(self, running_hyps: BatchHypothesis, x: torch.Tensor) -> BatchHypothesis:\n        \"\"\"Search new tokens for running hypotheses and encoded speech x.\n\n        Args:\n            running_hyps (BatchHypothesis): Running hypotheses on beam\n            x (torch.Tensor): Encoded speech feature (T, D)\n\n        Returns:\n            BatchHypothesis: Best sorted hypotheses\n\n        \"\"\"\n        n_batch = len(running_hyps)\n        part_ids = None  # no pre-beam\n        # batch scoring\n        weighted_scores = torch.zeros(\n            n_batch, self.n_vocab, dtype=x.dtype, device=x.device\n        )\n        scores, states = self.score_full(running_hyps, x.expand(n_batch, *x.shape))\n        for k in self.full_scorers:\n            weighted_scores += self.weights[k] * scores[k]\n        # partial scoring\n        if self.do_pre_beam:\n            pre_beam_scores = (\n                weighted_scores\n                if self.pre_beam_score_key == \"full\"\n                else scores[self.pre_beam_score_key]\n            )\n            part_ids = torch.topk(pre_beam_scores, self.pre_beam_size, dim=-1)[1]\n        # NOTE(takaaki-hori): Unlike BeamSearch, we assume that score_partial returns\n        # full-size score matrices, which has non-zero scores for part_ids and zeros\n        # for others.\n        part_scores, part_states = self.score_partial(running_hyps, part_ids, x)\n        for k in self.part_scorers:\n            weighted_scores += self.weights[k] * part_scores[k]\n        # add previous hyp scores\n        weighted_scores += running_hyps.score.to(\n            dtype=x.dtype, device=x.device\n        ).unsqueeze(1)\n\n        # TODO(karita): do not use list. use batch instead\n        # see also https://github.com/espnet/espnet/pull/1402#discussion_r354561029\n        # update hyps\n        best_hyps = []\n        prev_hyps = self.unbatchfy(running_hyps)\n        for (\n            full_prev_hyp_id,\n            full_new_token_id,\n            part_prev_hyp_id,\n            part_new_token_id,\n        ) in zip(*self.batch_beam(weighted_scores, part_ids)):\n            prev_hyp = prev_hyps[full_prev_hyp_id]\n            best_hyps.append(\n                Hypothesis(\n                    score=weighted_scores[full_prev_hyp_id, full_new_token_id],\n                    yseq=self.append_token(prev_hyp.yseq, full_new_token_id),\n                    scores=self.merge_scores(\n                        prev_hyp.scores,\n                        {k: v[full_prev_hyp_id] for k, v in scores.items()},\n                        full_new_token_id,\n                        {k: v[part_prev_hyp_id] for k, v in part_scores.items()},\n                        part_new_token_id,\n                    ),\n                    states=self.merge_states(\n                        {\n                            k: self.full_scorers[k].select_state(v, full_prev_hyp_id)\n                            for k, v in states.items()\n                        },\n                        {\n                            k: self.part_scorers[k].select_state(\n                                v, part_prev_hyp_id, part_new_token_id\n                            )\n                            for k, v in part_states.items()\n                        },\n                        part_new_token_id,\n                    ),\n                )\n            )\n        return self.batchfy(best_hyps)\n\n    def post_process(\n        self,\n        i: int,\n        maxlen: int,\n        maxlenratio: float,\n        running_hyps: BatchHypothesis,\n        ended_hyps: List[Hypothesis],\n    ) -> BatchHypothesis:\n        \"\"\"Perform post-processing of beam search iterations.\n\n        Args:\n            i (int): The length of hypothesis tokens.\n            maxlen (int): The maximum length of tokens in beam search.\n            maxlenratio (int): The maximum length ratio in beam search.\n            running_hyps (BatchHypothesis): The running hypotheses in beam search.\n            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n\n        Returns:\n            BatchHypothesis: The new running hypotheses.\n\n        \"\"\"\n        n_batch = running_hyps.yseq.shape[0]\n        logging.debug(f\"the number of running hypothes: {n_batch}\")\n        if self.token_list is not None:\n            logging.debug(\n                \"best hypo: \"\n                + \"\".join(\n                    [\n                        self.token_list[x]\n                        for x in running_hyps.yseq[0, 1 : running_hyps.length[0]]\n                    ]\n                )\n            )\n        # add eos in the final loop to avoid that there are no ended hyps\n        if i == maxlen - 1:\n            logging.debug(\"adding <eos> in the last position in the loop\")\n            yseq_eos = torch.cat(\n                (\n                    running_hyps.yseq,\n                    torch.full(\n                        (n_batch, 1),\n                        self.eos,\n                        device=running_hyps.yseq.device,\n                        dtype=torch.int64,\n                    ),\n                ),\n                1,\n            )\n            running_hyps.yseq.resize_as_(yseq_eos)\n            running_hyps.yseq[:] = yseq_eos\n            running_hyps.length[:] = yseq_eos.shape[1]\n\n        # add ended hypotheses to a final list, and removed them from current hypotheses\n        # (this will be a probmlem, number of hyps < beam)\n        is_eos = (\n            running_hyps.yseq[torch.arange(n_batch), running_hyps.length - 1]\n            == self.eos\n        )\n        for b in torch.nonzero(is_eos, as_tuple=False).view(-1):\n            hyp = self._select(running_hyps, b)\n            ended_hyps.append(hyp)\n        remained_ids = torch.nonzero(is_eos == 0, as_tuple=False).view(-1)\n        return self._batch_select(running_hyps, remained_ids)","metadata":{"id":"e4xkn6M3QsgW","execution":{"iopub.status.busy":"2024-06-14T23:00:12.132739Z","iopub.execute_input":"2024-06-14T23:00:12.133102Z","iopub.status.idle":"2024-06-14T23:00:12.179046Z","shell.execute_reply.started":"2024-06-14T23:00:12.133073Z","shell.execute_reply":"2024-06-14T23:00:12.178157Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## ensemble","metadata":{"id":"kJJTggAkS5Pe"}},{"cell_type":"code","source":"def average_checkpoints(last):\n    avg = None\n    for path in last:\n        states = torch.load(path, map_location=lambda storage, loc: storage)[\n            \"state_dict\"\n        ]\n        states = {k[6:]: v for k, v in states.items() if k.startswith(\"model.\")}\n        if avg is None:\n            avg = states\n        else:\n            for k in avg.keys():\n                avg[k] += states[k]\n    # average\n    for k in avg.keys():\n        if avg[k] is not None:\n            if avg[k].is_floating_point():\n                avg[k] /= len(last)\n            else:\n                avg[k] //= len(last)\n    return avg\n\n\ndef ensemble(args):\n    last = [\n        os.path.join(args.exp_dir, args.exp_name, f\"epoch={n}.ckpt\")\n        for n in range(\n            args.trainer.max_epochs - 5,\n            args.trainer.max_epochs,\n        )\n    ]\n    model_path = os.path.join(\n        args.exp_dir, args.exp_name, f\"model_avg_5.pth\"\n    )\n    torch.save(average_checkpoints(last), model_path)\n    return model_path","metadata":{"id":"HLnnKKLOS6NK","execution":{"iopub.status.busy":"2024-06-14T23:00:13.243315Z","iopub.execute_input":"2024-06-14T23:00:13.243979Z","iopub.status.idle":"2024-06-14T23:00:13.252947Z","shell.execute_reply.started":"2024-06-14T23:00:13.243949Z","shell.execute_reply":"2024-06-14T23:00:13.251958Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## AVSR multimodal(Audio-video)","metadata":{"id":"TXA40fu0zMBS"}},{"cell_type":"code","source":"# import torch\n# import torchaudio\n# from cosine import WarmupCosineScheduler\n# from datamodule.transforms import TextTransform\n\n# from pytorch_lightning import LightningModule\n# from espnet.nets.batch_beam_search import BatchBeamSearch\n# from espnet.nets.pytorch_backend.e2e_asr_conformer_av import E2E\n# from espnet.nets.scorers.length_bonus import LengthBonus\n# from espnet.nets.scorers.ctc import CTCPrefixScorer\n\n\ndef compute_word_level_distance(seq1, seq2):\n    return torchaudio.functional.edit_distance(seq1.lower().split(), seq2.lower().split())\n\n\nclass ModelModuleM(LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.save_hyperparameters(cfg)\n        self.cfg = cfg\n        self.backbone_args = self.cfg.model.audiovisual_backbone\n\n        self.text_transform = TextTransform()\n        self.token_list = self.text_transform.token_list\n        self.model = E2EM(len(self.token_list), self.backbone_args)\n\n        # -- initialise\n        if self.cfg.pretrained_model_path:\n            print('Transfer learning')\n            ckpt = torch.load(self.cfg.pretrained_model_path, map_location=lambda storage, loc: storage)\n            if self.cfg.transfer_frontend:\n                #tmp_ckpt = {k:v for k, v in ckpt.items() if k.startswith(\"trunk.\") or k.startswith(\"frontend.\")}\n                tmp_ckpt = {k.replace(\"encoder.frontend.\", \"\"):v for k, v in ckpt.items() if k.startswith(\"encoder.frontend.\")}\n                #print(tmp_ckpt.keys())\n                #print('----------------------------------------------')\n                #print(self.model.encoder.frontend.state_dict().keys())\n                self.model.encoder.frontend.load_state_dict(ckpt)\n                print('Frontend cargado')\n                \n            if self.cfg.transfer_encoder:\n                tmp_ckpt = {k.replace(\"encoder.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"encoder.\")}\n                self.model.encoder.load_state_dict(tmp_ckpt, strict=True)\n                print('Encoder cargado')\n                \n            if self.cfg.transfer_decoder:\n                tmp_ckpt = {k.replace(\"decoder.decoders.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"decoder.decoders\")}\n                self.model.decoder.decoders.load_state_dict(tmp_ckpt, strict=True)\n                print('Decoder cargado')\n            \n            if self.cfg.transfer_all:\n                self.model.load_state_dict(ckpt)\n                print('Todo el modelo cargado')\n        else:\n            print('Entrenamiento de cero')\n        \n        # Congelar encoder y decoder\n        if self.cfg.freeze_encoder:\n            print('Freeze encoder')\n            for param in self.model.encoder.parameters():\n                param.requires_grad = False\n            for param in self.model.aux_encoder.parameters():\n                param.requires_grad = False\n        if self.cfg.freeze_decoder:\n            print('Freeze decoder')\n            for param in self.model.decoder.decoders.parameters():\n                param.requires_grad = False\n                \n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW([{\"name\": \"model\", \"params\": self.model.parameters(), \"lr\": self.cfg.optimizer.lr}], weight_decay=self.cfg.optimizer.weight_decay, betas=(0.9, 0.98))\n        scheduler = WarmupCosineScheduler(optimizer, self.cfg.optimizer.warmup_epochs, self.cfg.trainer.max_epochs, len(self.trainer.datamodule.train_dataloader()))\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]\n\n    def forward(self, video, audio):\n        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n        video_feat, _ = self.model.encoder(video.unsqueeze(0).to(self.device), None)\n        audio_feat, _ = self.model.aux_encoder(audio.unsqueeze(0).to(self.device), None)\n        audiovisual_feat = self.model.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n\n        audiovisual_feat = audiovisual_feat.squeeze(0)\n\n        nbest_hyps = self.beam_search(audiovisual_feat)\n        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n        return predicted\n\n    def training_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(self, sample, sample_idx):\n        video_feat, _ = self.model.encoder(sample[\"video\"].unsqueeze(0).to(self.device), None)\n        audio_feat, _ = self.model.aux_encoder(sample[\"audio\"].unsqueeze(0).to(self.device), None)\n        audiovisual_feat = self.model.fusion(torch.cat((video_feat, audio_feat), dim=-1))\n        audiovisual_feat = audiovisual_feat.squeeze(0)\n\n        nbest_hyps = self.beam_search(audiovisual_feat)\n        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n\n        token_id = sample[\"target\"]\n        actual = self.text_transform.post_process(token_id)\n\n        self.total_edit_distance += compute_word_level_distance(actual, predicted)\n        self.total_length += len(actual.split())\n        return\n\n    def _step(self, batch, batch_idx, step_type):\n        loss, loss_ctc, loss_att, acc = self.model(batch[\"videos\"], batch[\"audios\"], batch[\"video_lengths\"],\n                                                   batch[\"audio_lengths\"], batch[\"targets\"])\n        batch_size = len(batch[\"videos\"])\n\n        if step_type == \"train\":\n            self.log(\"loss\", loss, on_step=True, on_epoch=True, batch_size=batch_size)\n            self.log(\"loss_ctc\", loss_ctc, on_step=False, on_epoch=True, batch_size=batch_size)\n            self.log(\"loss_att\", loss_att, on_step=False, on_epoch=True, batch_size=batch_size)\n            self.log(\"decoder_acc\", acc, on_step=True, on_epoch=True, batch_size=batch_size)\n        else:\n            self.log(\"loss_val\", loss, batch_size=batch_size)\n            self.log(\"loss_ctc_val\", loss_ctc, batch_size=batch_size)\n            self.log(\"loss_att_val\", loss_att, batch_size=batch_size)\n            self.log(\"decoder_acc_val\", acc, batch_size=batch_size)\n\n        if step_type == \"train\":\n            self.log(\"monitoring_step\", torch.tensor(self.global_step, dtype=torch.float32))\n\n        return loss\n\n    def on_train_epoch_start(self):\n        if self.current_epoch == 3 and self.cfg.pretrained_model_path:\n            print('Unfreeze decoder')\n            for param in self.model.decoder.decoders.parameters():\n                param.requires_grad = True \n        if self.current_epoch in [3, 7, 11]:\n            for param in self.model.aux_encoder.parameters():\n                param.requires_grad = True\n            for param in self.model.encoder.parameters():\n                param.requires_grad = False\n        if self.current_epoch in [5, 9, 13]:\n            for param in self.model.aux_encoder.parameters():\n                param.requires_grad = False\n            for param in self.model.encoder.parameters():\n                param.requires_grad = True\n        #sampler = self.trainer.train_dataloader.loaders.batch_sampler\n        sampler = self.trainer.train_dataloader.batch_sampler\n        if hasattr(sampler, \"set_epoch\"):\n            sampler.set_epoch(self.current_epoch)\n        return super().on_train_epoch_start()\n\n    def on_test_epoch_start(self):\n        self.total_length = 0\n        self.total_edit_distance = 0\n        self.text_transform = TextTransform()\n        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n\n    def on_test_epoch_end(self):\n        self.log(\"wer\", self.total_edit_distance / self.total_length)\n\n\ndef get_beam_search_decoder(model, token_list, ctc_weight=0.1, beam_size=40):\n    scorers = {\n        \"decoder\": model.decoder,\n        \"ctc\": CTCPrefixScorer(model.ctc, model.eos),\n        \"length_bonus\": LengthBonus(len(token_list)),\n        \"lm\": None\n    }\n\n    weights = {\n        \"decoder\": 1.0 - ctc_weight,\n        \"ctc\": ctc_weight,\n        \"lm\": 0.0,\n        \"length_bonus\": 0.0,\n    }\n\n    return BatchBeamSearch(\n        beam_size=beam_size,\n        vocab_size=len(token_list),\n        weights=weights,\n        scorers=scorers,\n        sos=model.sos,\n        eos=model.eos,\n        token_list=token_list,\n        pre_beam_score_key=None if ctc_weight == 1.0 else \"decoder\",\n    )","metadata":{"id":"hYkyP2H4zcZE","execution":{"iopub.status.busy":"2024-06-14T23:03:10.913877Z","iopub.execute_input":"2024-06-14T23:03:10.914472Z","iopub.status.idle":"2024-06-14T23:03:10.953254Z","shell.execute_reply.started":"2024-06-14T23:03:10.914427Z","shell.execute_reply":"2024-06-14T23:03:10.952260Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"## AVSR Base","metadata":{"id":"zEdCfDfmzabz"}},{"cell_type":"code","source":"# import torch\n# import torchaudio\n# from cosine import WarmupCosineScheduler\n# from datamodule.transforms import TextTransform\n\n# from pytorch_lightning import LightningModule\n# from espnet.nets.batch_beam_search import BatchBeamSearch\n# from espnet.nets.pytorch_backend.e2e_asr_conformer import E2E\n# from espnet.nets.scorers.length_bonus import LengthBonus\n# from espnet.nets.scorers.ctc import CTCPrefixScorer\n\n\ndef compute_word_level_distance(seq1, seq2):\n    return torchaudio.functional.edit_distance(seq1.lower().split(), seq2.lower().split())\n\n\nclass ModelModule(LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.save_hyperparameters(cfg)\n        self.cfg = cfg\n        if self.cfg.data.modality == \"audio\":\n            self.backbone_args = self.cfg.model.audio_backbone\n        elif self.cfg.data.modality == \"video\":\n            self.backbone_args = self.cfg.model.visual_backbone\n\n        self.text_transform = TextTransform()\n        self.token_list = self.text_transform.token_list\n        self.model = E2E(len(self.token_list), self.backbone_args)\n        # -- initialise\n        if self.cfg.pretrained_model_path:\n            print('Transfer learning')\n            ckpt = torch.load(self.cfg.pretrained_model_path, map_location=lambda storage, loc: storage)\n            if self.cfg.transfer_frontend:\n                #tmp_ckpt = {k:v for k, v in ckpt.items() if k.startswith(\"trunk.\") or k.startswith(\"frontend.\")}\n                tmp_ckpt = {k.replace(\"encoder.frontend.\", \"\"):v for k, v in ckpt.items() if k.startswith(\"encoder.frontend.\")}\n                #print(tmp_ckpt.keys())\n                #print('----------------------------------------------')\n                #print(self.model.encoder.frontend.state_dict().keys())\n                self.model.encoder.frontend.load_state_dict(ckpt)\n                print('Frontend cargado')\n                \n            if self.cfg.transfer_encoder:\n                tmp_ckpt = {k.replace(\"encoder.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"encoder.\")}\n                self.model.encoder.load_state_dict(tmp_ckpt, strict=True)\n                print('Encoder cargado')\n                \n            if self.cfg.transfer_decoder:\n                tmp_ckpt = {k.replace(\"decoder.decoders.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"decoder.decoders\")}\n                self.model.decoder.decoders.load_state_dict(tmp_ckpt, strict=True)\n                print('Decoder cargado')\n            \n            if self.cfg.transfer_all:\n                self.model.load_state_dict(ckpt)\n                print('Todo el modelo cargado')\n    \n            # Congelar encoder y decoder\n            if self.cfg.freeze_encoder:\n                print('Freeze encoder')\n                for param in self.model.encoder.parameters():\n                    param.requires_grad = False\n            if self.cfg.freeze_decoder:\n                print('Freeze decoder')\n                for param in self.model.decoder.decoders.parameters():\n                    param.requires_grad = False\n        else:\n            print('Entrenamiento de cero')\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW([{\"name\": \"model\", \"params\": self.model.parameters(), \"lr\": self.cfg.optimizer.lr}], weight_decay=self.cfg.optimizer.weight_decay, betas=(0.9, 0.98))\n        scheduler = WarmupCosineScheduler(optimizer, self.cfg.optimizer.warmup_epochs, self.cfg.trainer.max_epochs, len(self.trainer.datamodule.train_dataloader()))\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]\n\n    def forward(self, sample):\n        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n        enc_feat, _ = self.model.encoder(sample.unsqueeze(0).to(self.device), None)\n        enc_feat = enc_feat.squeeze(0)\n\n        nbest_hyps = self.beam_search(enc_feat)\n        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n        return predicted\n\n    def training_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, step_type=\"train\")\n\n    def validation_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, step_type=\"val\")\n\n    def test_step(self, sample, sample_idx):\n        enc_feat, _ = self.model.encoder(sample[\"input\"].unsqueeze(0).to(self.device), None)\n        enc_feat = enc_feat.squeeze(0)\n\n        nbest_hyps = self.beam_search(enc_feat)\n        nbest_hyps = [h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), 1)]]\n        predicted_token_id = torch.tensor(list(map(int, nbest_hyps[0][\"yseq\"][1:])))\n        predicted = self.text_transform.post_process(predicted_token_id).replace(\"<eos>\", \"\")\n\n        token_id = sample[\"target\"]\n        actual = self.text_transform.post_process(token_id)\n\n        self.total_edit_distance += compute_word_level_distance(actual, predicted)\n        self.total_length += len(actual.split())\n        return\n\n    def _step(self, batch, batch_idx, step_type):\n        loss, loss_ctc, loss_att, acc = self.model(batch[\"inputs\"], batch[\"input_lengths\"], batch[\"targets\"])\n        batch_size = len(batch[\"inputs\"])\n\n        if step_type == \"train\":\n            self.log(\"loss\", loss, on_step=True, on_epoch=True, batch_size=batch_size)\n            self.log(\"loss_ctc\", loss_ctc, on_step=False, on_epoch=True, batch_size=batch_size)\n            self.log(\"loss_att\", loss_att, on_step=False, on_epoch=True, batch_size=batch_size)\n            self.log(\"decoder_acc\", acc, on_step=True, on_epoch=True, batch_size=batch_size)\n        else:\n            self.log(\"loss_val\", loss, batch_size=batch_size)\n            self.log(\"loss_ctc_val\", loss_ctc, batch_size=batch_size)\n            self.log(\"loss_att_val\", loss_att, batch_size=batch_size)\n            self.log(\"decoder_acc_val\", acc, batch_size=batch_size)\n\n        if step_type == \"train\":\n            self.log(\"monitoring_step\", torch.tensor(self.global_step, dtype=torch.float32))\n\n        return loss\n\n    def on_train_epoch_start(self):\n        if self.current_epoch == 20 and self.cfg.pretrained_model_path:\n            print('Unfreeze')\n            for param in self.model.encoder.parameters():\n                param.requires_grad = True\n            for param in self.model.decoder.decoders.parameters():\n                param.requires_grad = True\n        #sampler = self.trainer.train_dataloader.loaders.batch_sampler\n        sampler = self.trainer.train_dataloader.batch_sampler\n        if hasattr(sampler, \"set_epoch\"):\n            sampler.set_epoch(self.current_epoch)\n        return super().on_train_epoch_start()\n\n    def on_test_epoch_start(self):\n        self.total_length = 0\n        self.total_edit_distance = 0\n        self.text_transform = TextTransform()\n        self.beam_search = get_beam_search_decoder(self.model, self.token_list)\n\n    def on_test_epoch_end(self):\n        self.log(\"wer\", self.total_edit_distance / self.total_length)\n\n\ndef get_beam_search_decoder(model, token_list, ctc_weight=0.1, beam_size=40):\n    scorers = {\n        \"decoder\": model.decoder,\n        \"ctc\": CTCPrefixScorer(model.ctc, model.eos),\n        \"length_bonus\": LengthBonus(len(token_list)),\n        \"lm\": None\n    }\n\n    weights = {\n        \"decoder\": 1.0 - ctc_weight,\n        \"ctc\": ctc_weight,\n        \"lm\": 0.0,\n        \"length_bonus\": 0.0,\n    }\n\n    return BatchBeamSearch(\n        beam_size=beam_size,\n        vocab_size=len(token_list),\n        weights=weights,\n        scorers=scorers,\n        sos=model.sos,\n        eos=model.eos,\n        token_list=token_list,\n        pre_beam_score_key=None if ctc_weight == 1.0 else \"decoder\",\n    )","metadata":{"id":"Li8yCFQ4zmxS","execution":{"iopub.status.busy":"2024-06-14T23:03:11.952708Z","iopub.execute_input":"2024-06-14T23:03:11.953078Z","iopub.status.idle":"2024-06-14T23:03:11.987981Z","shell.execute_reply.started":"2024-06-14T23:03:11.953048Z","shell.execute_reply":"2024-06-14T23:03:11.987023Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# Prueba ModuleModel","metadata":{"id":"gVcdlgZ5umU1"}},{"cell_type":"code","source":"modelmodule = ModelModuleM(cfg)","metadata":{"id":"Vss4d9VJuujz","outputId":"57590734-ab73-456b-f8da-b784435030d2","execution":{"iopub.status.busy":"2024-06-13T16:29:45.690855Z","iopub.execute_input":"2024-06-13T16:29:45.691662Z","iopub.status.idle":"2024-06-13T16:29:48.487969Z","shell.execute_reply.started":"2024-06-13T16:29:45.691620Z","shell.execute_reply":"2024-06-13T16:29:48.486949Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Entrenamiento de cero\nFreeze encoder\nFreeze decoder\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/3492487820.py:22: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ModelSummary(modelmodule))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:29:49.545484Z","iopub.execute_input":"2024-06-13T16:29:49.546351Z","iopub.status.idle":"2024-06-13T16:29:49.609637Z","shell.execute_reply.started":"2024-06-13T16:29:49.546305Z","shell.execute_reply":"2024-06-13T16:29:49.608545Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"  | Name  | Type | Params\n-------------------------------\n0 | model | E2EM | 432 M \n-------------------------------\n19.2 M    Trainable params\n413 M     Non-trainable params\n432 M     Total params\n1,730.351 Total estimated model params size (MB)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Número de parámetros:\", sum(p.numel() for p in list(modelmodule.parameters())))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:27:40.845121Z","iopub.execute_input":"2024-05-15T19:27:40.845478Z","iopub.status.idle":"2024-05-15T19:27:40.853637Z","shell.execute_reply.started":"2024-05-15T19:27:40.845438Z","shell.execute_reply":"2024-05-15T19:27:40.852714Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Número de parámetros: 45909612\n","output_type":"stream"}]},{"cell_type":"code","source":"modelmodule","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelmodule.cuda()","metadata":{"id":"s3mEHaZswlJW","outputId":"30e3bbc9-c047-444e-afe0-1a2eeb128953","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelmodule.device","metadata":{"id":"Cl6LPN4dzmBf","outputId":"36f7747d-14d8-4241-9f7a-4e2de78de6ca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelmodule.cpu()","metadata":{"id":"Axiyc2r-0Hwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.memory_allocated() / (1024 ** 2)","metadata":{"id":"qNM20dUR0FKZ","outputId":"f2b150e9-1171-40dc-cac6-a6aedd76f97e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{"id":"CERfWWMgWOcr"}},{"cell_type":"code","source":"!mkdir model_vsr","metadata":{"execution":{"iopub.status.busy":"2024-06-14T23:03:17.531837Z","iopub.execute_input":"2024-06-14T23:03:17.532479Z","iopub.status.idle":"2024-06-14T23:03:18.503970Z","shell.execute_reply.started":"2024-06-14T23:03:17.532420Z","shell.execute_reply":"2024-06-14T23:03:18.502770Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"cfg = DictConfig({'exp_dir': '/kaggle/working/model_vsr/', 'exp_name': 'models', 'gpus': None, 'slurm_job_id': None, 'file_path': None,\n                  'ckpt_path': '/kaggleu/working/', \n                  'pretrained_model_path': \"/kaggle/input/avsr_trlrwlrs2lrs3vox2avsp_base/pytorch/v1/1/avsr_trlrwlrs2lrs3vox2avsp_base.pth\",\n                  'transfer_all': False,\n                  'transfer_frontend': False,\n                  'transfer_encoder': True,\n                  'transfer_decoder': True,\n                  'freeze_encoder': True,\n                  'freeze_decoder': True,\n                  'data': {'modality': 'audiovisual', 'use_audio_normalise': True, 'max_frames': 1800,\n                           'max_frames_val': 1800,\n                           'dataset': {'root_dir': '/kaggle/input/vphbusfx/VPHB_USFX_preprocessing', 'label_dir': 'labels',\n                                       'train_file': 'VPHBUSFX_unigram100_train.csv',\n                                       'val_file': 'VPHBUSFX_unigram100_val.csv'}},\n                  'model': {'visual_backbone': {'adim': 256, 'aheads': 4, 'eunits': 2048,\n                                                'elayers': 12, 'transformer_input_layer': 'conv3d',\n                                                'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1,\n                                                'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                'macaron_style': 1, 'use_cnn_module': 1,\n                                                'cnn_module_kernel': 31, 'zero_triu': 0,\n                                                'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': 256,\n                                                'dheads': 4, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                'ctc_type': 'builtin', 'rel_pos_type': 'latest'},\n                            'audio_backbone': {'adim': 256, 'aheads': 4, 'eunits': 2048, 'elayers': 12,\n                                               'transformer_input_layer': 'conv1d', 'dropout_rate': 0.1,\n                                               'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                               'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                               'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                               'ddim': 256, 'dheads': 4, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                               'transformer_length_normalized_loss': False, 'mtlalpha': 0.1, 'ctc_type': 'builtin',\n                                               'rel_pos_type': 'latest'},\n                            'audiovisual_backbone': {'adim': 768, 'aheads': 12, 'eunits': 3072, 'elayers': 12,\n                                                     'transformer_input_layer': 'conv3d', 'dropout_rate': 0.1,\n                                                     'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                     'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                                     'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                                     'ddim': 768, 'dheads': 12, 'dunits': 3072, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                     'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                     'ctc_type': 'builtin', 'rel_pos_type': 'latest', 'aux_adim': 768,\n                                                     'aux_aheads': 12, 'aux_eunits': 3072, 'aux_elayers': 12,\n                                                     'aux_transformer_input_layer': 'conv1d', 'aux_dropout_rate': 0.1,\n                                                     'aux_transformer_attn_dropout_rate': 0.1,\n                                                     'aux_transformer_encoder_attn_layer_type': 'rel_mha', 'aux_macaron_style': True,\n                                                     'aux_use_cnn_module': True, 'aux_cnn_module_kernel': 31, 'aux_zero_triu': False,\n                                                     'aux_a_upsample_ratio': 1, 'aux_relu_type': 'swish', 'aux_dunits': 3072,\n                                                     'aux_dlayers': 6, 'aux_lsm_weight': 0.1,\n                                                     'aux_transformer_length_normalized_loss': False, 'aux_mtlalpha': 0.1,\n                                                     'aux_ctc_type': 'builtin', 'aux_rel_pos_type': 'latest', 'fusion_hdim': 8192,\n                                                     'fusion_norm': 'batchnorm'}},\n                  'optimizer': {'name': 'adamw', 'lr': 0.001, 'warmup_epochs': 5, 'weight_decay': 0.03},\n                  'trainer': {'precision': '16-mixed', 'max_epochs': 75, 'num_nodes': 1, 'sync_batchnorm': True,\n                              'default_root_dir': '${exp_dir}', 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1,\n                              'gradient_clip_val': 5.0},\n                  'decode': {'name': 'default', 'snr_target': 999999}})","metadata":{"id":"TcX8RNfWGe6U","execution":{"iopub.status.busy":"2024-06-14T23:03:25.712736Z","iopub.execute_input":"2024-06-14T23:03:25.713087Z","iopub.status.idle":"2024-06-14T23:03:25.746427Z","shell.execute_reply.started":"2024-06-14T23:03:25.713059Z","shell.execute_reply":"2024-06-14T23:03:25.745683Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"seed_everything(42, workers=True)\ncfg.gpus = torch.cuda.device_count()\n\ncheckpoint = ModelCheckpoint(\n    monitor=\"monitoring_step\",\n    mode=\"max\",\n    dirpath=os.path.join(cfg.exp_dir, cfg.exp_name) if cfg.exp_dir else None,\n    save_last=False,\n    filename=\"{epoch}\",\n    save_top_k=5,\n)\nlr_monitor = LearningRateMonitor(logging_interval=\"step\")\ncallbacks = [checkpoint, lr_monitor]\n\ndatamodule = DataModule(cfg)\nmodelmodule = ModelModuleM(cfg)\nprint(ModelSummary(modelmodule))\ncomet_logger = CometLogger(\n    api_key=\"g7gTJs7ovTZcgJKboOt03uQPI\",\n    project_name=\"VSR\",\n    experiment_name='model-unigram100_avsr_v2',\n)\ntrainer = Trainer(\n    **cfg.trainer,\n    logger=comet_logger,\n    callbacks=callbacks,\n)\nprint('Cantidad de memoria ocupada:',torch.cuda.memory_allocated() / (1024 ** 2))\ntrainer.fit(model=modelmodule, datamodule=datamodule)\nensemble(cfg)","metadata":{"id":"BEacjNzBY2NW","outputId":"296f21e9-fec4-4252-e0c4-5be1f476e831","execution":{"iopub.status.busy":"2024-06-14T23:04:32.795293Z","iopub.execute_input":"2024-06-14T23:04:32.796134Z","iopub.status.idle":"2024-06-14T23:37:21.679994Z","shell.execute_reply.started":"2024-06-14T23:04:32.796099Z","shell.execute_reply":"2024-06-14T23:37:21.677427Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Numero de gpus 1\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/3492487820.py:22: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")\n","output_type":"stream"},{"name":"stdout","text":"Transfer learning\nEncoder cargado\nDecoder cargado\nFreeze encoder\nFreeze decoder\n  | Name  | Type | Params\n-------------------------------\n0 | model | E2EM | 432 M \n-------------------------------\n19.2 M    Trainable params\n413 M     Non-trainable params\n432 M     Total params\n1,730.351 Total estimated model params size (MB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","output_type":"stream"},{"name":"stdout","text":"Cantidad de memoria ocupada: 0.0\n","output_type":"stream"},{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/daniel314/vsr/5de16130f184484080e62811ad59d83a\n\n","output_type":"stream"},{"name":"stdout","text":"num_train:1128\nnum_train:1128\nnum_val:99\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bf18698fa754fe0a61698b4d6353525"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Unfreeze decoder\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : model-unigram100_avsr_v2\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/daniel314/vsr/5de16130f184484080e62811ad59d83a\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decoder_acc_epoch [5] : (0.015007225796580315, 0.16397513449192047)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decoder_acc_step [14] : (0.0, 0.18140244483947754)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decoder_acc_val [5]   : (0.0, 0.17186814546585083)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [88]             : (73.77410888671875, 599.6728515625)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_att [5]          : (219.07693481445312, 253.59083557128906)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_att_val [5]      : (213.60659790039062, 234.86282348632812)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_ctc [5]          : (302.1233825683594, 363.86614990234375)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_ctc_val [5]      : (301.2148132324219, 329.9643859863281)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_epoch [5]        : (227.99122619628906, 264.4687194824219)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_step [14]        : (73.77410888671875, 478.8145446777344)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_val [5]          : (222.78944396972656, 241.86524963378906)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr-AdamW/model [14]   : (6.620689655172413e-05, 0.0009627586206896552)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     monitoring_step [14]  : (49.0, 699.0)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : model-unigram100_avsr_v2\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ckpt_path                                                          : /kaggleu/working/\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/dataset/label_dir                                             : labels\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/dataset/root_dir                                              : /kaggle/input/vphbusfx/VPHB_USFX_preprocessing\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/dataset/train_file                                            : VPHBUSFX_unigram100_train.csv\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/dataset/val_file                                              : VPHBUSFX_unigram100_val.csv\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/max_frames                                                    : 1800\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/max_frames_val                                                : 1800\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/modality                                                      : audiovisual\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data/use_audio_normalise                                           : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decode/name                                                        : default\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decode/snr_target                                                  : 999999\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exp_dir                                                            : /kaggle/working/model_vsr/\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exp_name                                                           : models\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     file_path                                                          : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze_decoder                                                     : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze_encoder                                                     : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gpus                                                               : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/a_upsample_ratio                              : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/adim                                          : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/aheads                                        : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/cnn_module_kernel                             : 31\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/ctc_type                                      : builtin\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/ddim                                          : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/dheads                                        : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/dlayers                                       : 6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/dropout_rate                                  : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/dunits                                        : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/elayers                                       : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/eunits                                        : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/lsm_weight                                    : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/macaron_style                                 : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/mtlalpha                                      : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/rel_pos_type                                  : latest\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/relu_type                                     : swish\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/transformer_attn_dropout_rate                 : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/transformer_encoder_attn_layer_type           : rel_mha\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/transformer_input_layer                       : conv1d\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/transformer_length_normalized_loss            : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/use_cnn_module                                : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audio_backbone/zero_triu                                     : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/a_upsample_ratio                        : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/adim                                    : 768\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aheads                                  : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_a_upsample_ratio                    : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_adim                                : 768\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_aheads                              : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_cnn_module_kernel                   : 31\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_ctc_type                            : builtin\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_dlayers                             : 6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_dropout_rate                        : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_dunits                              : 3072\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_elayers                             : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_eunits                              : 3072\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_lsm_weight                          : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_macaron_style                       : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_mtlalpha                            : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_rel_pos_type                        : latest\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_relu_type                           : swish\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_transformer_attn_dropout_rate       : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_transformer_encoder_attn_layer_type : rel_mha\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_transformer_input_layer             : conv1d\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_transformer_length_normalized_loss  : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_use_cnn_module                      : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/aux_zero_triu                           : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/cnn_module_kernel                       : 31\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/ctc_type                                : builtin\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/ddim                                    : 768\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/dheads                                  : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/dlayers                                 : 6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/dropout_rate                            : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/dunits                                  : 3072\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/elayers                                 : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/eunits                                  : 3072\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/fusion_hdim                             : 8192\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/fusion_norm                             : batchnorm\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/lsm_weight                              : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/macaron_style                           : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/mtlalpha                                : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/rel_pos_type                            : latest\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/relu_type                               : swish\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/transformer_attn_dropout_rate           : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/transformer_encoder_attn_layer_type     : rel_mha\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/transformer_input_layer                 : conv3d\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/transformer_length_normalized_loss      : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/use_cnn_module                          : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/audiovisual_backbone/zero_triu                               : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/a_upsample_ratio                             : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/adim                                         : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/aheads                                       : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/cnn_module_kernel                            : 31\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/ctc_type                                     : builtin\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/ddim                                         : 256\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/dheads                                       : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/dlayers                                      : 6\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/dropout_rate                                 : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/dunits                                       : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/elayers                                      : 12\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/eunits                                       : 2048\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/lsm_weight                                   : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/macaron_style                                : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/mtlalpha                                     : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/rel_pos_type                                 : latest\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/relu_type                                    : swish\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/transformer_attn_dropout_rate                : 0.1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/transformer_encoder_attn_layer_type          : rel_mha\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/transformer_input_layer                      : conv3d\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/transformer_length_normalized_loss           : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/use_cnn_module                               : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/visual_backbone/zero_triu                                    : 0\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer/lr                                                       : 0.001\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer/name                                                     : adamw\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer/warmup_epochs                                            : 5\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer/weight_decay                                             : 0.03\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pretrained_model_path                                              : /kaggle/input/avsr_trlrwlrs2lrs3vox2avsp_base/pytorch/v1/1/avsr_trlrwlrs2lrs3vox2avsp_base.pth\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     slurm_job_id                                                       : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/accumulate_grad_batches                                    : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/default_root_dir                                           : /kaggle/working/model_vsr/\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/gradient_clip_val                                          : 5.0\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/max_epochs                                                 : 75\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/num_nodes                                                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/num_sanity_val_steps                                       : 0\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/precision                                                  : 16-mixed\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     trainer/sync_batchnorm                                             : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transfer_all                                                       : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transfer_decoder                                                   : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transfer_encoder                                                   : True\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transfer_frontend                                                  : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg\u001b[38;5;241m.\u001b[39mtrainer,\n\u001b[1;32m     25\u001b[0m     logger\u001b[38;5;241m=\u001b[39mcomet_logger,\n\u001b[1;32m     26\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCantidad de memoria ocupada:\u001b[39m\u001b[38;5;124m'\u001b[39m,torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m ensemble(cfg)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1303\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:152\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/amp.py:80\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n\u001b[1;32m     83\u001b[0m skip_unscaling \u001b[38;5;241m=\u001b[39m closure_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:318\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[47], line 91\u001b[0m, in \u001b[0;36mModelModuleM.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[47], line 115\u001b[0m, in \u001b[0;36mModelModuleM._step\u001b[0;34m(self, batch, batch_idx, step_type)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx, step_type):\n\u001b[0;32m--> 115\u001b[0m     loss, loss_ctc, loss_att, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudios\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo_lengths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio_lengths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[37], line 131\u001b[0m, in \u001b[0;36mE2EM.forward\u001b[0;34m(self, video, audio, video_lengths, audio_lengths, label)\u001b[0m\n\u001b[1;32m    129\u001b[0m ys_in_pad, ys_out_pad \u001b[38;5;241m=\u001b[39m add_sos_eos(label, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_id)\n\u001b[1;32m    130\u001b[0m ys_mask \u001b[38;5;241m=\u001b[39m target_mask(ys_in_pad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_id)\n\u001b[0;32m--> 131\u001b[0m pred_pad, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys_in_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m loss_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(pred_pad, ys_out_pad)\n\u001b[1;32m    133\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmtlalpha \u001b[38;5;241m*\u001b[39m loss_ctc \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmtlalpha) \u001b[38;5;241m*\u001b[39m loss_att\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[32], line 139\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, tgt, tgt_mask, memory, memory_mask)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward decoder.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m:param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m                         if input_layer == \"embed\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m:rtype: torch.Tensor\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(tgt)\n\u001b[0;32m--> 139\u001b[0m x, tgt_mask, memory, memory_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_norm(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[23], line 7\u001b[0m, in \u001b[0;36mMultiSequential.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Repeat.\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[30], line 80\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, tgt, tgt_mask, memory, memory_mask, cache)\u001b[0m\n\u001b[1;32m     78\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_linear1(tgt_concat)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_q_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n\u001b[1;32m     82\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[20], line 90\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward\u001b[0;34m(self, query, key, value, mask, rtn_attn)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, value, mask, rtn_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute scaled dot product attention.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        query (torch.Tensor): Query tensor (#batch, time1, size).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output tensor (#batch, time1, d_model).\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_attention(v, scores, mask, rtn_attn)\n","Cell \u001b[0;32mIn[20], line 35\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward_qkv\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform query, key and value.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    query (torch.Tensor): Query tensor (#batch, time1, size).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m n_batch \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(n_batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m     36\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_k(key)\u001b[38;5;241m.\u001b[39mview(n_batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m     37\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_v(value)\u001b[38;5;241m.\u001b[39mview(n_batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 20.12 MiB is free. Process 4030 has 15.87 GiB memory in use. Of the allocated memory 13.42 GiB is allocated by PyTorch, and 2.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 20.12 MiB is free. Process 4030 has 15.87 GiB memory in use. Of the allocated memory 13.42 GiB is allocated by PyTorch, and 2.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"markdown","source":"### Evaluacion","metadata":{}},{"cell_type":"code","source":"cfg = DictConfig({'exp_dir': '/kaggle/working/model_vsr/', 'exp_name': 'models', 'gpus': None, 'slurm_job_id': None, 'file_path': None,\n                  'ckpt_path': '/kaggle/working/', \n                  'pretrained_model_path': '/kaggle/working/model_vsr/models/model_avg_10.pth', \n                  'transfer_frontend': None,\n                  'transfer_encoder': None,\n                  'data': {'modality': 'video', 'use_audio_normalise': False, 'max_frames': 1800,\n                           'max_frames_val': 1800,\n                           'dataset': {'root_dir': '/kaggle/input/vphbusfx/VPHB_USFX_preprocessing', 'label_dir': 'labels',\n                                       'train_file': 'VPHBUSFX_unigram1000_train.csv',\n                                       'val_file': 'VPHBUSFX_unigram1000_val.csv',\n                                       'test_file': 'VPHBUSFX_unigram1000_val.csv'}},\n                  'model': {'visual_backbone': {'adim': 256, 'aheads': 4, 'eunits': 2048,\n                                                'elayers': 12, 'transformer_input_layer': 'conv3d',\n                                                'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1,\n                                                'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                'macaron_style': 1, 'use_cnn_module': 1,\n                                                'cnn_module_kernel': 31, 'zero_triu': 0,\n                                                'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': 256,\n                                                'dheads': 4, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                'ctc_type': 'builtin', 'rel_pos_type': 'latest'},\n                            'audio_backbone': {'adim': 256, 'aheads': 12, 'eunits': 2048, 'elayers': 12,\n                                               'transformer_input_layer': 'conv1d', 'dropout_rate': 0.1,\n                                               'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                               'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                               'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                               'ddim': 256, 'dheads': 30, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                               'transformer_length_normalized_loss': False, 'mtlalpha': 0.1, 'ctc_type': 'builtin',\n                                               'rel_pos_type': 'latest'},\n                            'audiovisual_backbone': {'adim': 256, 'aheads': 12, 'eunits': 2048, 'elayers': 12,\n                                                     'transformer_input_layer': 'conv3d', 'dropout_rate': 0.1,\n                                                     'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha',\n                                                     'macaron_style': True, 'use_cnn_module': True, 'cnn_module_kernel': 31,\n                                                     'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish',\n                                                     'ddim': 256, 'dheads': 12, 'dunits': 2048, 'dlayers': 6, 'lsm_weight': 0.1,\n                                                     'transformer_length_normalized_loss': False, 'mtlalpha': 0.1,\n                                                     'ctc_type': 'builtin', 'rel_pos_type': 'latest', 'aux_adim': 256,\n                                                     'aux_aheads': 12, 'aux_eunits': 2048, 'aux_elayers': 12,\n                                                     'aux_transformer_input_layer': 'conv1d', 'aux_dropout_rate': 0.1,\n                                                     'aux_transformer_attn_dropout_rate': 0.1,\n                                                     'aux_transformer_encoder_attn_layer_type': 'rel_mha', 'aux_macaron_style': True,\n                                                     'aux_use_cnn_module': True, 'aux_cnn_module_kernel': 31, 'aux_zero_triu': False,\n                                                     'aux_a_upsample_ratio': 1, 'aux_relu_type': 'swish', 'aux_dunits': 2048,\n                                                     'aux_dlayers': 6, 'aux_lsm_weight': 0.1,\n                                                     'aux_transformer_length_normalized_loss': False, 'aux_mtlalpha': 0.1,\n                                                     'aux_ctc_type': 'builtin', 'aux_rel_pos_type': 'latest', 'fusion_hdim': 8192,\n                                                     'fusion_norm': 'batchnorm'}},\n                  'optimizer': {'name': 'adamw', 'lr': 0.001, 'warmup_epochs': 5, 'weight_decay': 0.03},\n                  'trainer': {'precision': 16, 'max_epochs': 20, 'num_nodes': 1, 'sync_batchnorm': True,\n                              'default_root_dir': '${exp_dir}', 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1,\n                              'gradient_clip_val': 5.0},\n                  'decode': {'name': 'default', 'snr_target': 999999}})","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:35:54.828020Z","iopub.execute_input":"2024-06-13T16:35:54.828432Z","iopub.status.idle":"2024-06-13T16:35:54.863527Z","shell.execute_reply.started":"2024-06-13T16:35:54.828394Z","shell.execute_reply":"2024-06-13T16:35:54.862495Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"# Unigram1000","metadata":{}},{"cell_type":"code","source":"modelmodule = ModelModule(cfg)\ndatamodule = DataModule(cfg)\ntrainer = Trainer(num_nodes=1)\n# Training and testing\nmodelmodule.model.load_state_dict(torch.load(cfg.pretrained_model_path, map_location=lambda storage, loc: storage))\ntrainer.test(model=modelmodule, datamodule=datamodule)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}