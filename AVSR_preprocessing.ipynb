{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1jLgxJNjUvK0",
        "2rIk5Xf7Urtz",
        "zAvtziY3UFbe",
        "CiL6IrYoYdOy",
        "Wq50mSe0jOLG",
        "2Vf5mUY66pRJ",
        "bu4mQFgBzwGY",
        "E3pA7PK_-z6c"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZgCs7dGcNVGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7ETCcaQGNNWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63325a6-e343-4ba7-d5bc-6b031020a618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting av\n",
            "  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg-python\n",
        "!pip install av"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "XZlrG64BUNYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchvision\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import transform as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xREsj1urUPos"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MediaPipe"
      ],
      "metadata": {
        "id": "1jLgxJNjUvK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "6iuJF2_VXmWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp"
      ],
      "metadata": {
        "id": "XkGKjc61XbJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self):\n",
        "        self.mp_face_detection = mp.solutions.face_detection\n",
        "        self.short_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=0)\n",
        "        self.full_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=1)\n",
        "\n",
        "    def __call__(self, video_frames):\n",
        "        landmarks = self.detect(video_frames, self.full_range_detector)\n",
        "        if all(element is None for element in landmarks):\n",
        "            landmarks = self.detect(video_frames, self.short_range_detector)\n",
        "            assert any(l is not None for l in landmarks), \"Cannot detect any frames in the video\"\n",
        "        return landmarks\n",
        "\n",
        "    def detect(self, video_frames, detector):\n",
        "        landmarks = []\n",
        "        for frame in video_frames:\n",
        "            results = detector.process(frame)\n",
        "            if not results.detections:\n",
        "                landmarks.append(None)\n",
        "                continue\n",
        "            face_points = []\n",
        "            for idx, detected_faces in enumerate(results.detections):\n",
        "                max_id, max_size = 0, 0\n",
        "                bboxC = detected_faces.location_data.relative_bounding_box\n",
        "                ih, iw, ic = frame.shape\n",
        "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
        "                bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "                if bbox_size > max_size:\n",
        "                    max_id, max_size = idx, bbox_size\n",
        "                lmx = [\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].y * ih)],\n",
        "                    ]\n",
        "                face_points.append(lmx)\n",
        "            landmarks.append(np.array(face_points[max_id]))\n",
        "        return landmarks"
      ],
      "metadata": {
        "id": "7TAzoxo8XVxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = np.array([[ 70.92383848,  97.13757949],[ 72.62515288, 114.90361188],\n",
        "  [ 75.98941827, 131.07402473],[ 79.26959318, 146.2116596 ],[ 83.61348894, 163.26020701],\n",
        "  [ 91.33134186, 177.44535288],[100.27169245, 187.0885567 ],[112.12435016, 196.00353535],\n",
        "  [130.74175801, 200.52998862],[149.8553027 , 195.31198065],[163.101687  , 186.44060881],\n",
        "  [173.65334172, 176.57250158],[182.43614459, 162.27992572],[187.16738556, 145.09391978],\n",
        "  [190.22905333, 129.72418731],[193.02118502, 113.45923358],[194.43863372,  95.5795392 ],\n",
        "  [ 81.33095967,  80.79544511],[ 87.75906556,  75.27980275],[ 96.22692544,  73.83857497],\n",
        "  [104.55524335,  74.74029382],[112.23186144,  76.97670954],[144.49576387,  76.42387471],\n",
        "  [152.34799901,  73.83329748],[161.13054079,  72.63570385],[170.58715674,  73.84785054],\n",
        "  [178.21409885,  79.43802857],[128.7337425 ,  95.35962566],[128.48854473, 106.92459506],\n",
        "  [128.24475936, 118.27285086],[128.26596547, 127.69870727],[118.76000113, 135.19357677],\n",
        "  [122.96307973, 136.14619774],[128.87017961, 137.30253356],[134.9428314 , 135.99720543],\n",
        "  [139.48259748, 134.87763793],[ 92.52245553,  94.36876014],[ 97.58518219,  90.95977781],\n",
        "  [105.41368273,  90.91345887],[112.77241724,  94.9436087 ],[106.103635  ,  97.08485693],\n",
        "  [ 98.04628565,  97.36335869],[145.52511509,  94.53499862],[152.58953438,  90.21485666],\n",
        "  [160.61170666,  90.19938514],[166.67710071,  93.56562296],[160.55971572,  96.48125958],\n",
        "  [152.20465993,  96.47281336],[107.16760614, 157.19606764],[114.47611216, 152.12006957],\n",
        "  [123.84852759, 148.51863199],[128.97628288, 149.41552527],[134.14360703, 148.42628211],\n",
        "  [144.17717842, 151.79343262],[152.19284005, 156.98711116],[143.85966895, 164.00347101],\n",
        "  [136.7441507 , 167.9430006 ],[129.15278278, 168.81853366],[121.79511074, 168.02271929],\n",
        "  [115.27508573, 164.15159355],[109.23088653, 157.00172103],[122.50270762, 154.40733649],\n",
        "  [129.02862236, 154.12104227],[135.83648069, 154.31214998],[150.75782809, 156.79506004],\n",
        "  [135.66204627, 160.62976732],[128.95218547, 161.28762709],[122.48775432, 160.50878431]])\n",
        "\n",
        "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
        "    start_landmarks = landmarks[start_idx]\n",
        "    stop_landmarks = landmarks[stop_idx]\n",
        "    delta = stop_landmarks - start_landmarks\n",
        "    for idx in range(1, stop_idx - start_idx):\n",
        "        landmarks[start_idx + idx] = (\n",
        "            start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
        "        )\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def warp_img(src, dst, img, std_size):\n",
        "    tform = tf.estimate_transform(\"similarity\", src, dst)\n",
        "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped, tform\n",
        "\n",
        "\n",
        "def apply_transform(transform, img, std_size):\n",
        "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped\n",
        "\n",
        "\n",
        "def cut_patch(img, landmarks, height, width, threshold=5):\n",
        "    center_x, center_y = np.mean(landmarks, axis=0)\n",
        "    # Check for too much bias in height and width\n",
        "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
        "        raise OverflowError(\"too much bias in height\")\n",
        "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
        "        raise OverflowError(\"too much bias in width\")\n",
        "    # Calculate bounding box coordinates\n",
        "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
        "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
        "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
        "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
        "    # Cut the image\n",
        "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
        "    return cutted_img\n",
        "\n",
        "\n",
        "class VideoProcess:\n",
        "    def __init__(\n",
        "        self,\n",
        "        mean_face_path=\"20words_mean_face.npy\",\n",
        "        crop_width=96,\n",
        "        crop_height=96,\n",
        "        start_idx=3,\n",
        "        stop_idx=4,\n",
        "        window_margin=12,\n",
        "        convert_gray=True,\n",
        "    ):\n",
        "        # self.reference = np.load(\n",
        "        #     os.path.join(os.path.dirname(__file__), mean_face_path)\n",
        "        # )\n",
        "        self.reference = reference\n",
        "        self.crop_width = crop_width\n",
        "        self.crop_height = crop_height\n",
        "        self.start_idx = start_idx\n",
        "        self.stop_idx = stop_idx\n",
        "        self.window_margin = window_margin\n",
        "        self.convert_gray = convert_gray\n",
        "\n",
        "    def __call__(self, video, landmarks):\n",
        "        # Pre-process landmarks: interpolate frames that are not detected\n",
        "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
        "        # Exclude corner cases: no landmark in all frames\n",
        "        if not preprocessed_landmarks:\n",
        "            return\n",
        "        # Affine transformation and crop patch\n",
        "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
        "        assert sequence is not None, \"crop an empty patch.\"\n",
        "        return sequence\n",
        "\n",
        "    def crop_patch(self, video, landmarks):\n",
        "        sequence = []\n",
        "        for frame_idx, frame in enumerate(video):\n",
        "            window_margin = min(\n",
        "                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx\n",
        "            )\n",
        "            smoothed_landmarks = np.mean(\n",
        "                [\n",
        "                    landmarks[x]\n",
        "                    for x in range(\n",
        "                        frame_idx - window_margin, frame_idx + window_margin + 1\n",
        "                    )\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "            smoothed_landmarks += landmarks[frame_idx].mean(\n",
        "                axis=0\n",
        "            ) - smoothed_landmarks.mean(axis=0)\n",
        "            transformed_frame, transformed_landmarks = self.affine_transform(\n",
        "                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray\n",
        "            )\n",
        "            patch = cut_patch(\n",
        "                transformed_frame,\n",
        "                transformed_landmarks[self.start_idx : self.stop_idx],\n",
        "                self.crop_height // 2,\n",
        "                self.crop_width // 2,\n",
        "            )\n",
        "            sequence.append(patch)\n",
        "        return np.array(sequence)\n",
        "\n",
        "    def interpolate_landmarks(self, landmarks):\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        if not valid_frames_idx:\n",
        "            return None\n",
        "\n",
        "        for idx in range(1, len(valid_frames_idx)):\n",
        "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
        "                landmarks = linear_interpolate(\n",
        "                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]\n",
        "                )\n",
        "\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
        "        if valid_frames_idx:\n",
        "            landmarks[: valid_frames_idx[0]] = [\n",
        "                landmarks[valid_frames_idx[0]]\n",
        "            ] * valid_frames_idx[0]\n",
        "            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (\n",
        "                len(landmarks) - valid_frames_idx[-1]\n",
        "            )\n",
        "\n",
        "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    def affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        reference,\n",
        "        grayscale=False,\n",
        "        target_size=(256, 256),\n",
        "        reference_size=(256, 256),\n",
        "        stable_points=(0, 1, 2, 3),\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=0,\n",
        "    ):\n",
        "        if grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        stable_reference = self.get_stable_reference(\n",
        "            reference, reference_size, target_size\n",
        "        )\n",
        "        transform = self.estimate_affine_transform(\n",
        "            landmarks, stable_points, stable_reference\n",
        "        )\n",
        "        transformed_frame, transformed_landmarks = self.apply_affine_transform(\n",
        "            frame,\n",
        "            landmarks,\n",
        "            transform,\n",
        "            target_size,\n",
        "            interpolation,\n",
        "            border_mode,\n",
        "            border_value,\n",
        "        )\n",
        "\n",
        "        return transformed_frame, transformed_landmarks\n",
        "\n",
        "    def get_stable_reference(self, reference, reference_size, target_size):\n",
        "        # -- right eye, left eye, nose tip, mouth center\n",
        "        stable_reference = np.vstack(\n",
        "            [\n",
        "                np.mean(reference[36:42], axis=0),\n",
        "                np.mean(reference[42:48], axis=0),\n",
        "                np.mean(reference[31:36], axis=0),\n",
        "                np.mean(reference[48:68], axis=0),\n",
        "            ]\n",
        "        )\n",
        "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
        "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
        "        return stable_reference\n",
        "\n",
        "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
        "        return cv2.estimateAffinePartial2D(\n",
        "            np.vstack([landmarks[x] for x in stable_points]),\n",
        "            stable_reference,\n",
        "            method=cv2.LMEDS,\n",
        "        )[0]\n",
        "\n",
        "    def apply_affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        transform,\n",
        "        target_size,\n",
        "        interpolation,\n",
        "        border_mode,\n",
        "        border_value,\n",
        "    ):\n",
        "        transformed_frame = cv2.warpAffine(\n",
        "            frame,\n",
        "            transform,\n",
        "            dsize=(target_size[0], target_size[1]),\n",
        "            flags=interpolation,\n",
        "            borderMode=border_mode,\n",
        "            borderValue=border_value,\n",
        "        )\n",
        "        transformed_landmarks = (\n",
        "            np.matmul(landmarks, transform[:, :2].transpose())\n",
        "            + transform[:, 2].transpose()\n",
        "        )\n",
        "        return transformed_frame, transformed_landmarks"
      ],
      "metadata": {
        "id": "k6ahhX7gXohu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetinaFace and FAN"
      ],
      "metadata": {
        "id": "2rIk5Xf7Urtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hhj1897/face_alignment.git\n",
        "!git clone https://github.com/hhj1897/face_detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE9lkZSTV0YI",
        "outputId": "07fc8c53-282a-4d7b-a591-acc323b98ca5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face_alignment'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 190 (delta 27), reused 27 (delta 26), pack-reused 158\u001b[K\n",
            "Receiving objects: 100% (190/190), 213.82 MiB | 22.03 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Updating files: 100% (14/14), done.\n",
            "Cloning into 'face_detection'...\n",
            "remote: Enumerating objects: 300, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 300 (delta 41), reused 39 (delta 39), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (300/300), 81.19 MiB | 18.81 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "Updating files: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from face_detection.ibug.face_detection import RetinaFacePredictor\n",
        "from face_alignment.ibug.face_alignment import FANPredictor"
      ],
      "metadata": {
        "id": "p_IOSBvMV4ox"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self, device=\"cuda:0\", model_name=\"resnet50\"):\n",
        "        self.face_detector = RetinaFacePredictor(\n",
        "            device=device,\n",
        "            threshold=0.8,\n",
        "            model=RetinaFacePredictor.get_model(model_name),\n",
        "        )\n",
        "        self.landmark_detector = FANPredictor(device=device, model=None)\n",
        "\n",
        "    def __call__(self, video_frames):\n",
        "        landmarks = []\n",
        "        for frame in video_frames:\n",
        "            detected_faces = self.face_detector(frame, rgb=False)\n",
        "            face_points, _ = self.landmark_detector(frame, detected_faces, rgb=True)\n",
        "            if len(detected_faces) == 0:\n",
        "                landmarks.append(None)\n",
        "            else:\n",
        "                max_id, max_size = 0, 0\n",
        "                for idx, bbox in enumerate(detected_faces):\n",
        "                    bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "                    if bbox_size > max_size:\n",
        "                        max_id, max_size = idx, bbox_size\n",
        "                landmarks.append(face_points[max_id])\n",
        "        return landmarks"
      ],
      "metadata": {
        "id": "u2_Tl0YFUu2N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = np.array([[ 70.92383848,  97.13757949],[ 72.62515288, 114.90361188],\n",
        "  [ 75.98941827, 131.07402473],[ 79.26959318, 146.2116596 ],[ 83.61348894, 163.26020701],\n",
        "  [ 91.33134186, 177.44535288],[100.27169245, 187.0885567 ],[112.12435016, 196.00353535],\n",
        "  [130.74175801, 200.52998862],[149.8553027 , 195.31198065],[163.101687  , 186.44060881],\n",
        "  [173.65334172, 176.57250158],[182.43614459, 162.27992572],[187.16738556, 145.09391978],\n",
        "  [190.22905333, 129.72418731],[193.02118502, 113.45923358],[194.43863372,  95.5795392 ],\n",
        "  [ 81.33095967,  80.79544511],[ 87.75906556,  75.27980275],[ 96.22692544,  73.83857497],\n",
        "  [104.55524335,  74.74029382],[112.23186144,  76.97670954],[144.49576387,  76.42387471],\n",
        "  [152.34799901,  73.83329748],[161.13054079,  72.63570385],[170.58715674,  73.84785054],\n",
        "  [178.21409885,  79.43802857],[128.7337425 ,  95.35962566],[128.48854473, 106.92459506],\n",
        "  [128.24475936, 118.27285086],[128.26596547, 127.69870727],[118.76000113, 135.19357677],\n",
        "  [122.96307973, 136.14619774],[128.87017961, 137.30253356],[134.9428314 , 135.99720543],\n",
        "  [139.48259748, 134.87763793],[ 92.52245553,  94.36876014],[ 97.58518219,  90.95977781],\n",
        "  [105.41368273,  90.91345887],[112.77241724,  94.9436087 ],[106.103635  ,  97.08485693],\n",
        "  [ 98.04628565,  97.36335869],[145.52511509,  94.53499862],[152.58953438,  90.21485666],\n",
        "  [160.61170666,  90.19938514],[166.67710071,  93.56562296],[160.55971572,  96.48125958],\n",
        "  [152.20465993,  96.47281336],[107.16760614, 157.19606764],[114.47611216, 152.12006957],\n",
        "  [123.84852759, 148.51863199],[128.97628288, 149.41552527],[134.14360703, 148.42628211],\n",
        "  [144.17717842, 151.79343262],[152.19284005, 156.98711116],[143.85966895, 164.00347101],\n",
        "  [136.7441507 , 167.9430006 ],[129.15278278, 168.81853366],[121.79511074, 168.02271929],\n",
        "  [115.27508573, 164.15159355],[109.23088653, 157.00172103],[122.50270762, 154.40733649],\n",
        "  [129.02862236, 154.12104227],[135.83648069, 154.31214998],[150.75782809, 156.79506004],\n",
        "  [135.66204627, 160.62976732],[128.95218547, 161.28762709],[122.48775432, 160.50878431]])\n",
        "\n",
        "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
        "    start_landmarks = landmarks[start_idx]\n",
        "    stop_landmarks = landmarks[stop_idx]\n",
        "    delta = stop_landmarks - start_landmarks\n",
        "    for idx in range(1, stop_idx - start_idx):\n",
        "        landmarks[start_idx + idx] = (\n",
        "            start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
        "        )\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def warp_img(src, dst, img, std_size):\n",
        "    tform = tf.estimate_transform(\"similarity\", src, dst)\n",
        "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped, tform\n",
        "\n",
        "\n",
        "def apply_transform(transform, img, std_size):\n",
        "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped\n",
        "\n",
        "\n",
        "def cut_patch(img, landmarks, height, width, threshold=5):\n",
        "    center_x, center_y = np.mean(landmarks, axis=0)\n",
        "    # Check for too much bias in height and width\n",
        "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
        "        raise OverflowError(\"too much bias in height\")\n",
        "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
        "        raise OverflowError(\"too much bias in width\")\n",
        "    # Calculate bounding box coordinates\n",
        "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
        "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
        "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
        "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
        "    # Cut the image\n",
        "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
        "    return cutted_img\n",
        "\n",
        "\n",
        "class VideoProcess:\n",
        "    def __init__(\n",
        "        self,\n",
        "        mean_face_path=\"20words_mean_face.npy\",\n",
        "        crop_width=96,\n",
        "        crop_height=96,\n",
        "        start_idx=48,\n",
        "        stop_idx=68,\n",
        "        window_margin=12,\n",
        "        convert_gray=True,\n",
        "    ):\n",
        "        # self.reference = np.load(\n",
        "        #     os.path.join(os.path.dirname(__file__), mean_face_path)\n",
        "        # )\n",
        "        self.reference = reference\n",
        "        self.crop_width = crop_width\n",
        "        self.crop_height = crop_height\n",
        "        self.start_idx = start_idx\n",
        "        self.stop_idx = stop_idx\n",
        "        self.window_margin = window_margin\n",
        "        self.convert_gray = convert_gray\n",
        "\n",
        "    def __call__(self, video, landmarks):\n",
        "        # Pre-process landmarks: interpolate frames that are not detected\n",
        "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
        "        # Exclude corner cases: no landmark in all frames or number of frames is less than window length\n",
        "        if (\n",
        "            not preprocessed_landmarks\n",
        "            or len(preprocessed_landmarks) < self.window_margin\n",
        "        ):\n",
        "            return\n",
        "        # Affine transformation and crop patch\n",
        "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
        "        assert sequence is not None, \"crop an empty patch.\"\n",
        "        return sequence\n",
        "\n",
        "    def crop_patch(self, video, landmarks):\n",
        "        sequence = []\n",
        "        for frame_idx, frame in enumerate(video):\n",
        "            window_margin = min(\n",
        "                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx\n",
        "            )\n",
        "            smoothed_landmarks = np.mean(\n",
        "                [\n",
        "                    landmarks[x]\n",
        "                    for x in range(\n",
        "                        frame_idx - window_margin, frame_idx + window_margin + 1\n",
        "                    )\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "            smoothed_landmarks += landmarks[frame_idx].mean(\n",
        "                axis=0\n",
        "            ) - smoothed_landmarks.mean(axis=0)\n",
        "            transformed_frame, transformed_landmarks = self.affine_transform(\n",
        "                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray\n",
        "            )\n",
        "            patch = cut_patch(\n",
        "                transformed_frame,\n",
        "                transformed_landmarks[self.start_idx : self.stop_idx],\n",
        "                self.crop_height // 2,\n",
        "                self.crop_width // 2,\n",
        "            )\n",
        "            sequence.append(patch)\n",
        "        return np.array(sequence)\n",
        "\n",
        "    def interpolate_landmarks(self, landmarks):\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        if not valid_frames_idx:\n",
        "            return None\n",
        "\n",
        "        for idx in range(1, len(valid_frames_idx)):\n",
        "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
        "                landmarks = linear_interpolate(\n",
        "                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]\n",
        "                )\n",
        "\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
        "        if valid_frames_idx:\n",
        "            landmarks[: valid_frames_idx[0]] = [\n",
        "                landmarks[valid_frames_idx[0]]\n",
        "            ] * valid_frames_idx[0]\n",
        "            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (\n",
        "                len(landmarks) - valid_frames_idx[-1]\n",
        "            )\n",
        "\n",
        "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    def affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        reference,\n",
        "        grayscale=True,\n",
        "        target_size=(256, 256),\n",
        "        reference_size=(256, 256),\n",
        "        stable_points=(28, 33, 36, 39, 42, 45, 48, 54),\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=0,\n",
        "    ):\n",
        "        if grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        stable_reference = self.get_stable_reference(\n",
        "            reference, stable_points, reference_size, target_size\n",
        "        )\n",
        "        transform = self.estimate_affine_transform(\n",
        "            landmarks, stable_points, stable_reference\n",
        "        )\n",
        "        transformed_frame, transformed_landmarks = self.apply_affine_transform(\n",
        "            frame,\n",
        "            landmarks,\n",
        "            transform,\n",
        "            target_size,\n",
        "            interpolation,\n",
        "            border_mode,\n",
        "            border_value,\n",
        "        )\n",
        "\n",
        "        return transformed_frame, transformed_landmarks\n",
        "\n",
        "    def get_stable_reference(\n",
        "        self, reference, stable_points, reference_size, target_size\n",
        "    ):\n",
        "        stable_reference = np.vstack([reference[x] for x in stable_points])\n",
        "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
        "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
        "        return stable_reference\n",
        "\n",
        "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
        "        return cv2.estimateAffinePartial2D(\n",
        "            np.vstack([landmarks[x] for x in stable_points]),\n",
        "            stable_reference,\n",
        "            method=cv2.LMEDS,\n",
        "        )[0]\n",
        "\n",
        "    def apply_affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        transform,\n",
        "        target_size,\n",
        "        interpolation,\n",
        "        border_mode,\n",
        "        border_value,\n",
        "    ):\n",
        "        transformed_frame = cv2.warpAffine(\n",
        "            frame,\n",
        "            transform,\n",
        "            dsize=(target_size[0], target_size[1]),\n",
        "            flags=interpolation,\n",
        "            borderMode=border_mode,\n",
        "            borderValue=border_value,\n",
        "        )\n",
        "        transformed_landmarks = (\n",
        "            np.matmul(landmarks, transform[:, :2].transpose())\n",
        "            + transform[:, 2].transpose()\n",
        "        )\n",
        "        return transformed_frame, transformed_landmarks"
      ],
      "metadata": {
        "id": "DAVsQMOLWsGp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AVSRDataLoader"
      ],
      "metadata": {
        "id": "zAvtziY3UFbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AVSRDataLoader:\n",
        "    def __init__(self, modality, detector=\"retinaface\", convert_gray=True):\n",
        "        self.modality = modality\n",
        "        if modality == \"video\":\n",
        "            # if detector == \"retinaface\":\n",
        "            #     from detectors.retinaface.detector import LandmarksDetector\n",
        "            #     from detectors.retinaface.video_process import VideoProcess\n",
        "            #     self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
        "            #     self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "\n",
        "            # if detector == \"mediapipe\":\n",
        "            #     from detectors.mediapipe.detector import LandmarksDetector\n",
        "            #     from detectors.mediapipe.video_process import VideoProcess\n",
        "            #     self.landmarks_detector = LandmarksDetector()\n",
        "            #     self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "            self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
        "            self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "\n",
        "    def load_data(self, data_filename, landmarks=None, transform=True):\n",
        "        if self.modality == \"audio\":\n",
        "            audio, sample_rate = self.load_audio(data_filename)\n",
        "            audio = self.audio_process(audio, sample_rate)\n",
        "            return audio\n",
        "        if self.modality == \"video\":\n",
        "            video = self.load_video(data_filename)\n",
        "            if not landmarks:\n",
        "                landmarks = self.landmarks_detector(video)\n",
        "            video = self.video_process(video, landmarks)\n",
        "            if video is None:\n",
        "                raise TypeError(\"video cannot be None\")\n",
        "            video = torch.tensor(video)\n",
        "            return video\n",
        "\n",
        "    def load_audio(self, data_filename):\n",
        "        waveform, sample_rate = torchaudio.load(data_filename, normalize=True)\n",
        "        return waveform, sample_rate\n",
        "\n",
        "    def load_video(self, data_filename):\n",
        "        return torchvision.io.read_video(data_filename, pts_unit=\"sec\")[0].numpy()\n",
        "\n",
        "    def audio_process(self, waveform, sample_rate, target_sample_rate=16000):\n",
        "        if sample_rate != target_sample_rate:\n",
        "            waveform = torchaudio.functional.resample(\n",
        "                waveform, sample_rate, target_sample_rate\n",
        "            )\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        return waveform"
      ],
      "metadata": {
        "id": "8jQdMjnlNd9e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util"
      ],
      "metadata": {
        "id": "CiL6IrYoYdOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_file(filename, max_frames=600, fps=25.0):\n",
        "\n",
        "    lines = open(filename).read().splitlines()\n",
        "\n",
        "    flag = 0\n",
        "    stack = []\n",
        "    res = []\n",
        "\n",
        "    tmp = 0\n",
        "    start_timestamp = 0.0\n",
        "\n",
        "    threshold = max_frames / fps\n",
        "\n",
        "    for line in lines:\n",
        "        if \"WORD START END ASDSCORE\" in line:\n",
        "            flag = 1\n",
        "            continue\n",
        "        if flag:\n",
        "            word, start, end, score = line.split(\" \")\n",
        "            start, end, score = float(start), float(end), float(score)\n",
        "            if end < tmp + threshold:\n",
        "                stack.append(word)\n",
        "                last_timestamp = end\n",
        "            else:\n",
        "                res.append(\n",
        "                    [\n",
        "                        \" \".join(stack),\n",
        "                        start_timestamp,\n",
        "                        last_timestamp,\n",
        "                        last_timestamp - start_timestamp,\n",
        "                    ]\n",
        "                )\n",
        "                tmp = start\n",
        "                start_timestamp = start\n",
        "                stack = [word]\n",
        "    if stack:\n",
        "        res.append([\" \".join(stack), start_timestamp, end, end - start_timestamp])\n",
        "    return res\n",
        "\n",
        "\n",
        "def save_vid_txt(\n",
        "    dst_vid_filename, dst_txt_filename, trim_video_data, content, video_fps=25\n",
        "):\n",
        "    # -- save video\n",
        "    save2vid(dst_vid_filename, trim_video_data, video_fps)\n",
        "    # -- save text\n",
        "    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)\n",
        "    f = open(dst_txt_filename, \"w\", encoding=\"utf-8\")\n",
        "    f.write(f\"{content}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def save_vid_aud(\n",
        "    dst_vid_filename,\n",
        "    dst_aud_filename,\n",
        "    trim_vid_data,\n",
        "    trim_aud_data,\n",
        "    video_fps=25,\n",
        "    audio_sample_rate=16000,\n",
        "):\n",
        "    # -- save video\n",
        "    save2vid(dst_vid_filename, trim_vid_data, video_fps)\n",
        "    # -- save audio\n",
        "    save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)\n",
        "\n",
        "\n",
        "def save_vid_aud_txt(\n",
        "    dst_vid_filename,\n",
        "    dst_aud_filename,\n",
        "    dst_txt_filename,\n",
        "    trim_vid_data,\n",
        "    trim_aud_data,\n",
        "    content,\n",
        "    video_fps=25,\n",
        "    audio_sample_rate=16000,\n",
        "):\n",
        "    # -- save video\n",
        "    if dst_vid_filename is not None:\n",
        "        save2vid(dst_vid_filename, trim_vid_data, video_fps)\n",
        "    # -- save audio\n",
        "    if dst_aud_filename is not None:\n",
        "        save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)\n",
        "    # -- save text\n",
        "    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)\n",
        "    f = open(dst_txt_filename, \"w\", encoding=\"utf-8\")\n",
        "    f.write(f\"{content}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def save2vid(filename, vid, frames_per_second):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchvision.io.write_video(filename, vid, frames_per_second)\n",
        "\n",
        "\n",
        "def save2aud(filename, aud, sample_rate):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchaudio.save(filename, aud, sample_rate)"
      ],
      "metadata": {
        "id": "zA3biEp1UXj2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SJuNFb_Ycq0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper"
      ],
      "metadata": {
        "id": "Wq50mSe0jOLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfH5SDeijQTL",
        "outputId": "eeda44f0-ccd1-4990-fda7-2df096acef3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-3w99zz3g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-3w99zz3g\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=1aece26ef1b891b68839785be3e73f6a0f45304c6f0f9ec939c58d5ce0973a88\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r5moxb1c/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from moviepy.editor import VideoFileClip"
      ],
      "metadata": {
        "id": "YQNyCAtujbq6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_whisper = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "id": "8w4WWs7okcK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60a79f6-c062-4ef4-f256-0802fc146ed2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:22<00:00, 68.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_texto(ruta, name):\n",
        "    video = VideoFileClip(ruta)\n",
        "    video.audio.write_audiofile(f'{name}.mp3', logger=None)\n",
        "    i = 0\n",
        "    while True:\n",
        "        result = model_whisper.transcribe(f'{name}.mp3')\n",
        "        temperature = result['segments'][0]['temperature']\n",
        "        compression_ratio = result['segments'][0]['compression_ratio']\n",
        "        no_speech_prob = result['segments'][0]['no_speech_prob']\n",
        "        #print(result)\n",
        "        if temperature == 0.0 and compression_ratio > 0.8:\n",
        "            text = result['text']\n",
        "            break\n",
        "        elif temperature != 1.0 and temperature > 0.2 and compression_ratio > 1.0:\n",
        "            text = result['text']\n",
        "            break\n",
        "        i += 1\n",
        "        if i>3:\n",
        "            print(result)\n",
        "\n",
        "    text = text[1:]\n",
        "    print(text)\n",
        "    print(f'temperature={temperature}')\n",
        "    print(f'compression ratio={compression_ratio}')\n",
        "    print(f'no_speech_prob={no_speech_prob}')\n",
        "    text = text.replace(',','')\n",
        "    text = text.replace('.','')\n",
        "    text = text.replace('¿','')\n",
        "    text = text.replace('?','')\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# def obtener_texto(ruta, audio):\n",
        "#     video = VideoFileClip(ruta)\n",
        "#     video.audio.write_audiofile(f'{audio}.mp3')\n",
        "#     # load audio and pad/trim it to fit 30 seconds\n",
        "#     audio = whisper.load_audio(f'{audio}.mp3')\n",
        "#     audio = whisper.pad_or_trim(audio)\n",
        "#     # make log-Mel spectrogram and move to the same device as the model\n",
        "#     mel = whisper.log_mel_spectrogram(audio).to(model_whisper.device)\n",
        "#     # detect the spoken language\n",
        "#     # _, probs = model.detect_language(mel)\n",
        "#     # print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "#     # decode the audio\n",
        "#     options = whisper.DecodingOptions()\n",
        "#     result = whisper.decode(model_whisper, mel, options)\n",
        "#     # recognized text\n",
        "#     return result.text"
      ],
      "metadata": {
        "id": "BHh8cPc_jumK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generacion de video procesado, y obtencion de texto"
      ],
      "metadata": {
        "id": "yPwPvqS9Y2Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_videos = '/content/drive/MyDrive/vsr/VPHB_USFX/videos'\n",
        "ruta_dataset = '/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing'\n",
        "list_videos_fa = os.listdir(ruta_videos)\n",
        "list_videos_pre = os.listdir(f'{ruta_dataset}/VPHBUSFX/video')\n",
        "list_videos = [v for v in list_videos_fa if v not in list_videos_pre]"
      ],
      "metadata": {
        "id": "aJZtCRYc81Fq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_videos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLCa_a0j85YD",
        "outputId": "94b0e641-c884-4008-954e-02f7c6c1b605"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "162"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_dataloader = AVSRDataLoader(modality=\"video\", detector=\"retinaface\", convert_gray=False)\n",
        "audio_dataloader = AVSRDataLoader(modality=\"audio\")\n",
        "\n",
        "for data_filename in tqdm(list_videos[2:100]):\n",
        "    print(data_filename)\n",
        "    name = data_filename.split('.')[0]\n",
        "    ruta = f'{ruta_videos}/{data_filename}'\n",
        "    # Obtener video y texto\n",
        "    text_data = obtener_texto(ruta, name)\n",
        "    print(text_data)\n",
        "    video_data = video_dataloader.load_data(ruta)\n",
        "    audio_data = audio_dataloader.load_data(ruta)\n",
        "    # Guardar texto y video\n",
        "    output_video_path = f'{ruta_dataset}/VPHBUSFX/video/{name}.mp4'\n",
        "    output_audio_path = f'{ruta_dataset}/VPHBUSFX/video/{name}.wav'\n",
        "    output_text_path = f'{ruta_dataset}/VPHBUSFX/text/{name}.txt'\n",
        "    # save_vid_txt(output_video_path, output_text_path,\n",
        "    #              video_data, text_data)\n",
        "    save_vid_aud_txt(output_video_path, output_audio_path, output_text_path,\n",
        "                    video_data, audio_data, text_data,\n",
        "                    video_fps=25, audio_sample_rate=16000)\n",
        "    print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrOUrv8lY4RF",
        "outputId": "03859ecd-48e7-4826-8a8c-df1e3358b7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/98 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2jhsGornklM_V1-0003.mp4\n",
            "La Fuerza Especial de Lucha contra el Crimen ya se encuentra analizando las imágenes de las cámaras.\n",
            "temperature=0.0\n",
            "compression ratio=1.096774193548387\n",
            "no_speech_prob=0.08419062197208405\n",
            "la fuerza especial de lucha contra el crimen ya se encuentra analizando las imágenes de las cámaras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/98 [00:21<35:14, 21.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "r0vLKEt1dVQ_V1-0001.mp4\n",
            "el movimiento de las lanchas a motor puesto que este ha sido una de las causas para que pueda identificarse la muerte de peces en la represa de hongostura el deslame de aceites\n",
            "temperature=0.0\n",
            "compression ratio=1.5172413793103448\n",
            "no_speech_prob=0.17104732990264893\n",
            "el movimiento de las lanchas a motor puesto que este ha sido una de las causas para que pueda identificarse la muerte de peces en la represa de hongostura el deslame de aceites\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/98 [01:03<53:23, 33.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "r0vLKEt1dVQ_V1-0002.mp4\n",
            "que se pueda realizarse un estudio técnico científico que pueda identificar cuánto volumen de agua\n",
            "temperature=0.0\n",
            "compression ratio=1.1348314606741574\n",
            "no_speech_prob=0.4325334131717682\n",
            "que se pueda realizarse un estudio técnico científico que pueda identificar cuánto volumen de agua\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/98 [01:23<43:41, 27.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "SaKZhaT47Yg_V1-0001.mp4\n",
            "Y a diferentes unidades policiales de otras regiones del país han llegado hasta la paz para sumarse justamente a este trabajo de rastrillaje. Se ha ampliado el perímetro de búsqueda, es lo último que sabemos.\n",
            "temperature=0.0\n",
            "compression ratio=1.3766233766233766\n",
            "no_speech_prob=0.4673718512058258\n",
            "y a diferentes unidades policiales de otras regiones del país han llegado hasta la paz para sumarse justamente a este trabajo de rastrillaje se ha ampliado el perímetro de búsqueda es lo último que sabemos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/98 [02:04<51:17, 32.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "SaKZhaT47Yg_V1-0002.mp4\n",
            "Entonces en ese lugar armamos una línea de descenso de unos 150 metros para poder observar con claridad más abajo.\n",
            "temperature=0.0\n",
            "compression ratio=1.1153846153846154\n",
            "no_speech_prob=0.20894531905651093\n",
            "entonces en ese lugar armamos una línea de descenso de unos 150 metros para poder observar con claridad más abajo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ly65uzLSD_rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5jVbsPynfIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextTransform"
      ],
      "metadata": {
        "id": "2Vf5mUY66pRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece"
      ],
      "metadata": {
        "id": "n_Mh7W8j66-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SP_MODEL_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"unigram\",\n",
        "    \"unigram1000.model\",\n",
        ")\n",
        "\n",
        "DICT_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"unigram\",\n",
        "    \"unigram1000_units.txt\",\n",
        ")\n",
        "\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Mapping Dictionary Class for SentencePiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sp_model_path=SP_MODEL_PATH,\n",
        "        dict_path=DICT_PATH,\n",
        "    ):\n",
        "\n",
        "        # Load SentencePiece model\n",
        "        self.spm = sentencepiece.SentencePieceProcessor(model_file=sp_model_path)\n",
        "\n",
        "        # Load units and create dictionary\n",
        "        units = open(dict_path, encoding='utf8').read().splitlines()\n",
        "        self.hashmap = {unit.split()[0]: unit.split()[-1] for unit in units}\n",
        "        # 0 will be used for \"blank\" in CTC\n",
        "        self.token_list = [\"<blank>\"] + list(self.hashmap.keys()) + [\"<eos>\"]\n",
        "        self.ignore_id = -1\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = self.spm.EncodeAsPieces(text)\n",
        "        token_ids = [self.hashmap.get(token, self.hashmap[\"<unk>\"]) for token in tokens]\n",
        "        return torch.tensor(list(map(int, token_ids)))\n",
        "\n",
        "    def post_process(self, token_ids):\n",
        "        token_ids = token_ids[token_ids != -1]\n",
        "        text = self._ids_to_str(token_ids, self.token_list)\n",
        "        text = text.replace(\"\\u2581\", \" \").strip()\n",
        "        return text\n",
        "\n",
        "    def _ids_to_str(self, token_ids, char_list):\n",
        "        token_as_list = [char_list[idx] for idx in token_ids]\n",
        "        return \"\".join(token_as_list).replace(\"<space>\", \" \")"
      ],
      "metadata": {
        "id": "U2pjDXiA6o0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer y generacion de csv"
      ],
      "metadata": {
        "id": "bu4mQFgBzwGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_transform = TextTransform()"
      ],
      "metadata": {
        "id": "z2lOzc6P6Xio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = \"/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing\""
      ],
      "metadata": {
        "id": "c0EKk85mnfFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_video_text = [[f'{ruta}/VPHBUSFX/video/{name}.mp4',f'{ruta}/VPHBUSFX/text/{name}.txt'] for name in [n.split('.')[0] for n in os.listdir(f'{ruta}/VPHBUSFX/video')]]\n",
        "list_csv = []\n",
        "for r_vid, r_text in tqdm(list_video_text):\n",
        "    video = torchvision.io.read_video(r_vid, pts_unit=\"sec\")[0].numpy()\n",
        "    text = open(r_text, 'r').read()\n",
        "    token_id_str = \" \".join(\n",
        "            map(str, [_.item() for _ in text_transform.tokenize(text)])\n",
        "    )\n",
        "    name_video = r_vid.split('/')[-1]\n",
        "    list_csv.append(['VPHBUSFX', f'video/{name_video}', video.shape[0], token_id_str])\n",
        "df_csv = pd.DataFrame(list_csv, columns=['Dataset','ruta','input_length','token_id'])\n",
        "df_csv.to_csv(f'{ruta}/labels/VPHBUSFX_unigram1000.csv')\n",
        "df_csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Yl3NGeGd0_Ev",
        "outputId": "3a75551e-2b27-4e4b-8880-5d4b6e8aa8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:35<00:00,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Dataset                           ruta  input_length  \\\n",
              "0    VPHBUSFX  video/4yquiL-r1J0_V1-0001.mp4           298   \n",
              "1    VPHBUSFX  video/4yquiL-r1J0_V1-0002.mp4           280   \n",
              "2    VPHBUSFX  video/4yquiL-r1J0_V1-0003.mp4           360   \n",
              "3    VPHBUSFX  video/4yquiL-r1J0_V1-0004.mp4           241   \n",
              "4    VPHBUSFX  video/4yquiL-r1J0_V1-0005.mp4           419   \n",
              "..        ...                            ...           ...   \n",
              "295  VPHBUSFX  video/44Ee_Oa6dYc_V1-0007.mp4           422   \n",
              "296  VPHBUSFX  video/44Ee_Oa6dYc_V1-0008.mp4           207   \n",
              "297  VPHBUSFX  video/44Ee_Oa6dYc_V1-0009.mp4           456   \n",
              "298  VPHBUSFX  video/44Ee_Oa6dYc_V1-0011.mp4           325   \n",
              "299  VPHBUSFX  video/44Ee_Oa6dYc_V1-0012.mp4           448   \n",
              "\n",
              "                                              token_id  \n",
              "0    451 539 784 296 894 102 975 291 28 538 334 980...  \n",
              "1    108 306 374 579 233 892 828 847 183 980 795 89...  \n",
              "2    928 835 33 359 633 321 886 262 237 861 663 649...  \n",
              "3    663 352 578 363 345 817 210 51 906 237 115 675...  \n",
              "4    653 33 651 581 622 741 687 877 997 28 655 321 ...  \n",
              "..                                                 ...  \n",
              "295  944 633 237 881 78 538 814 157 553 29 237 538 ...  \n",
              "296  749 153 835 60 741 306 189 664 237 28 141 786 ...  \n",
              "297  237 538 709 331 642 835 593 786 287 183 347 80...  \n",
              "298  997 567 955 786 205 664 786 649 345 956 232 92...  \n",
              "299  997 345 786 649 674 316 786 367 847 438 345 32...  \n",
              "\n",
              "[300 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-994044e9-3586-40ce-aa71-93d30ecdbaec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>ruta</th>\n",
              "      <th>input_length</th>\n",
              "      <th>token_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0001.mp4</td>\n",
              "      <td>298</td>\n",
              "      <td>451 539 784 296 894 102 975 291 28 538 334 980...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0002.mp4</td>\n",
              "      <td>280</td>\n",
              "      <td>108 306 374 579 233 892 828 847 183 980 795 89...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0003.mp4</td>\n",
              "      <td>360</td>\n",
              "      <td>928 835 33 359 633 321 886 262 237 861 663 649...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0004.mp4</td>\n",
              "      <td>241</td>\n",
              "      <td>663 352 578 363 345 817 210 51 906 237 115 675...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0005.mp4</td>\n",
              "      <td>419</td>\n",
              "      <td>653 33 651 581 622 741 687 877 997 28 655 321 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/44Ee_Oa6dYc_V1-0007.mp4</td>\n",
              "      <td>422</td>\n",
              "      <td>944 633 237 881 78 538 814 157 553 29 237 538 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/44Ee_Oa6dYc_V1-0008.mp4</td>\n",
              "      <td>207</td>\n",
              "      <td>749 153 835 60 741 306 189 664 237 28 141 786 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/44Ee_Oa6dYc_V1-0009.mp4</td>\n",
              "      <td>456</td>\n",
              "      <td>237 538 709 331 642 835 593 786 287 183 347 80...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/44Ee_Oa6dYc_V1-0011.mp4</td>\n",
              "      <td>325</td>\n",
              "      <td>997 567 955 786 205 664 786 649 345 956 232 92...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/44Ee_Oa6dYc_V1-0012.mp4</td>\n",
              "      <td>448</td>\n",
              "      <td>997 345 786 649 674 316 786 367 847 438 345 32...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-994044e9-3586-40ce-aa71-93d30ecdbaec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-994044e9-3586-40ce-aa71-93d30ecdbaec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-994044e9-3586-40ce-aa71-93d30ecdbaec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d274ff95-b025-490f-819a-de00f811aacb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d274ff95-b025-490f-819a-de00f811aacb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d274ff95-b025-490f-819a-de00f811aacb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_841887d6-b658-41ce-b217-2542a0d3327b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_csv')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_841887d6-b658-41ce-b217-2542a0d3327b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_csv');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_csv",
              "summary": "{\n  \"name\": \"df_csv\",\n  \"rows\": 300,\n  \"fields\": [\n    {\n      \"column\": \"Dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"VPHBUSFX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ruta\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 300,\n        \"samples\": [\n          \"video/MNX4n4on6fI_V1-0014.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94,\n        \"min\": 74,\n        \"max\": 670,\n        \"num_unique_values\": 202,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 300,\n        \"samples\": [\n          \"358 538 633 1002 665 358 51 903 795 633 29 924 306 271 5 287 574 455 633 416 237 835 238 51 540 237 835 238 538 801 408 508 251 934 665 693 467 237 172 95 93 649 345 956 906 145 795 45 853 60 283 921 932 850 877 251 695 359 633 321 5 229 237 237 93 903 366 906\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtener train, val"
      ],
      "metadata": {
        "id": "E3pA7PK_-z6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "vz2kU0p-1U9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df_csv, test_size=0.1)"
      ],
      "metadata": {
        "id": "gPKGInxa2f9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape, df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPYiFpJs3DYU",
        "outputId": "a88f7b2c-a037-43f4-ea94-08da32fdf91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((270, 4), (30, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_csv(f'{ruta}/labels/VPHBUSFX_unigram1000_train.csv')\n",
        "df_test.to_csv(f'{ruta}/labels/VPHBUSFX_unigram1000_val.csv')"
      ],
      "metadata": {
        "id": "-zu5jv8E99-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0dxNRb6-3G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmKRDsSp-2_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RN8l2ftSeKiq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}