{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1jLgxJNjUvK0",
        "2rIk5Xf7Urtz",
        "zAvtziY3UFbe",
        "CiL6IrYoYdOy",
        "2Vf5mUY66pRJ",
        "bu4mQFgBzwGY",
        "E3pA7PK_-z6c"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZgCs7dGcNVGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7ETCcaQGNNWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74418ea0-bea5-4c8d-8854-27703aafb354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting av\n",
            "  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg-python\n",
        "!pip install av"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "XZlrG64BUNYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchvision\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import transform as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xREsj1urUPos"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MediaPipe"
      ],
      "metadata": {
        "id": "1jLgxJNjUvK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "6iuJF2_VXmWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp"
      ],
      "metadata": {
        "id": "XkGKjc61XbJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self):\n",
        "        self.mp_face_detection = mp.solutions.face_detection\n",
        "        self.short_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=0)\n",
        "        self.full_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=1)\n",
        "\n",
        "    def __call__(self, video_frames):\n",
        "        landmarks = self.detect(video_frames, self.full_range_detector)\n",
        "        if all(element is None for element in landmarks):\n",
        "            landmarks = self.detect(video_frames, self.short_range_detector)\n",
        "            assert any(l is not None for l in landmarks), \"Cannot detect any frames in the video\"\n",
        "        return landmarks\n",
        "\n",
        "    def detect(self, video_frames, detector):\n",
        "        landmarks = []\n",
        "        for frame in video_frames:\n",
        "            results = detector.process(frame)\n",
        "            if not results.detections:\n",
        "                landmarks.append(None)\n",
        "                continue\n",
        "            face_points = []\n",
        "            for idx, detected_faces in enumerate(results.detections):\n",
        "                max_id, max_size = 0, 0\n",
        "                bboxC = detected_faces.location_data.relative_bounding_box\n",
        "                ih, iw, ic = frame.shape\n",
        "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
        "                bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "                if bbox_size > max_size:\n",
        "                    max_id, max_size = idx, bbox_size\n",
        "                lmx = [\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].y * ih)],\n",
        "                    ]\n",
        "                face_points.append(lmx)\n",
        "            landmarks.append(np.array(face_points[max_id]))\n",
        "        return landmarks"
      ],
      "metadata": {
        "id": "7TAzoxo8XVxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = np.array([[ 70.92383848,  97.13757949],[ 72.62515288, 114.90361188],\n",
        "  [ 75.98941827, 131.07402473],[ 79.26959318, 146.2116596 ],[ 83.61348894, 163.26020701],\n",
        "  [ 91.33134186, 177.44535288],[100.27169245, 187.0885567 ],[112.12435016, 196.00353535],\n",
        "  [130.74175801, 200.52998862],[149.8553027 , 195.31198065],[163.101687  , 186.44060881],\n",
        "  [173.65334172, 176.57250158],[182.43614459, 162.27992572],[187.16738556, 145.09391978],\n",
        "  [190.22905333, 129.72418731],[193.02118502, 113.45923358],[194.43863372,  95.5795392 ],\n",
        "  [ 81.33095967,  80.79544511],[ 87.75906556,  75.27980275],[ 96.22692544,  73.83857497],\n",
        "  [104.55524335,  74.74029382],[112.23186144,  76.97670954],[144.49576387,  76.42387471],\n",
        "  [152.34799901,  73.83329748],[161.13054079,  72.63570385],[170.58715674,  73.84785054],\n",
        "  [178.21409885,  79.43802857],[128.7337425 ,  95.35962566],[128.48854473, 106.92459506],\n",
        "  [128.24475936, 118.27285086],[128.26596547, 127.69870727],[118.76000113, 135.19357677],\n",
        "  [122.96307973, 136.14619774],[128.87017961, 137.30253356],[134.9428314 , 135.99720543],\n",
        "  [139.48259748, 134.87763793],[ 92.52245553,  94.36876014],[ 97.58518219,  90.95977781],\n",
        "  [105.41368273,  90.91345887],[112.77241724,  94.9436087 ],[106.103635  ,  97.08485693],\n",
        "  [ 98.04628565,  97.36335869],[145.52511509,  94.53499862],[152.58953438,  90.21485666],\n",
        "  [160.61170666,  90.19938514],[166.67710071,  93.56562296],[160.55971572,  96.48125958],\n",
        "  [152.20465993,  96.47281336],[107.16760614, 157.19606764],[114.47611216, 152.12006957],\n",
        "  [123.84852759, 148.51863199],[128.97628288, 149.41552527],[134.14360703, 148.42628211],\n",
        "  [144.17717842, 151.79343262],[152.19284005, 156.98711116],[143.85966895, 164.00347101],\n",
        "  [136.7441507 , 167.9430006 ],[129.15278278, 168.81853366],[121.79511074, 168.02271929],\n",
        "  [115.27508573, 164.15159355],[109.23088653, 157.00172103],[122.50270762, 154.40733649],\n",
        "  [129.02862236, 154.12104227],[135.83648069, 154.31214998],[150.75782809, 156.79506004],\n",
        "  [135.66204627, 160.62976732],[128.95218547, 161.28762709],[122.48775432, 160.50878431]])\n",
        "\n",
        "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
        "    start_landmarks = landmarks[start_idx]\n",
        "    stop_landmarks = landmarks[stop_idx]\n",
        "    delta = stop_landmarks - start_landmarks\n",
        "    for idx in range(1, stop_idx - start_idx):\n",
        "        landmarks[start_idx + idx] = (\n",
        "            start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
        "        )\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def warp_img(src, dst, img, std_size):\n",
        "    tform = tf.estimate_transform(\"similarity\", src, dst)\n",
        "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped, tform\n",
        "\n",
        "\n",
        "def apply_transform(transform, img, std_size):\n",
        "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped\n",
        "\n",
        "\n",
        "def cut_patch(img, landmarks, height, width, threshold=5):\n",
        "    center_x, center_y = np.mean(landmarks, axis=0)\n",
        "    # Check for too much bias in height and width\n",
        "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
        "        raise OverflowError(\"too much bias in height\")\n",
        "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
        "        raise OverflowError(\"too much bias in width\")\n",
        "    # Calculate bounding box coordinates\n",
        "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
        "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
        "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
        "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
        "    # Cut the image\n",
        "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
        "    return cutted_img\n",
        "\n",
        "\n",
        "class VideoProcess:\n",
        "    def __init__(\n",
        "        self,\n",
        "        mean_face_path=\"20words_mean_face.npy\",\n",
        "        crop_width=96,\n",
        "        crop_height=96,\n",
        "        start_idx=3,\n",
        "        stop_idx=4,\n",
        "        window_margin=12,\n",
        "        convert_gray=True,\n",
        "    ):\n",
        "        # self.reference = np.load(\n",
        "        #     os.path.join(os.path.dirname(__file__), mean_face_path)\n",
        "        # )\n",
        "        self.reference = reference\n",
        "        self.crop_width = crop_width\n",
        "        self.crop_height = crop_height\n",
        "        self.start_idx = start_idx\n",
        "        self.stop_idx = stop_idx\n",
        "        self.window_margin = window_margin\n",
        "        self.convert_gray = convert_gray\n",
        "\n",
        "    def __call__(self, video, landmarks):\n",
        "        # Pre-process landmarks: interpolate frames that are not detected\n",
        "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
        "        # Exclude corner cases: no landmark in all frames\n",
        "        if not preprocessed_landmarks:\n",
        "            return\n",
        "        # Affine transformation and crop patch\n",
        "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
        "        assert sequence is not None, \"crop an empty patch.\"\n",
        "        return sequence\n",
        "\n",
        "    def crop_patch(self, video, landmarks):\n",
        "        sequence = []\n",
        "        for frame_idx, frame in enumerate(video):\n",
        "            window_margin = min(\n",
        "                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx\n",
        "            )\n",
        "            smoothed_landmarks = np.mean(\n",
        "                [\n",
        "                    landmarks[x]\n",
        "                    for x in range(\n",
        "                        frame_idx - window_margin, frame_idx + window_margin + 1\n",
        "                    )\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "            smoothed_landmarks += landmarks[frame_idx].mean(\n",
        "                axis=0\n",
        "            ) - smoothed_landmarks.mean(axis=0)\n",
        "            transformed_frame, transformed_landmarks = self.affine_transform(\n",
        "                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray\n",
        "            )\n",
        "            patch = cut_patch(\n",
        "                transformed_frame,\n",
        "                transformed_landmarks[self.start_idx : self.stop_idx],\n",
        "                self.crop_height // 2,\n",
        "                self.crop_width // 2,\n",
        "            )\n",
        "            sequence.append(patch)\n",
        "        return np.array(sequence)\n",
        "\n",
        "    def interpolate_landmarks(self, landmarks):\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        if not valid_frames_idx:\n",
        "            return None\n",
        "\n",
        "        for idx in range(1, len(valid_frames_idx)):\n",
        "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
        "                landmarks = linear_interpolate(\n",
        "                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]\n",
        "                )\n",
        "\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
        "        if valid_frames_idx:\n",
        "            landmarks[: valid_frames_idx[0]] = [\n",
        "                landmarks[valid_frames_idx[0]]\n",
        "            ] * valid_frames_idx[0]\n",
        "            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (\n",
        "                len(landmarks) - valid_frames_idx[-1]\n",
        "            )\n",
        "\n",
        "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    def affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        reference,\n",
        "        grayscale=False,\n",
        "        target_size=(256, 256),\n",
        "        reference_size=(256, 256),\n",
        "        stable_points=(0, 1, 2, 3),\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=0,\n",
        "    ):\n",
        "        if grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        stable_reference = self.get_stable_reference(\n",
        "            reference, reference_size, target_size\n",
        "        )\n",
        "        transform = self.estimate_affine_transform(\n",
        "            landmarks, stable_points, stable_reference\n",
        "        )\n",
        "        transformed_frame, transformed_landmarks = self.apply_affine_transform(\n",
        "            frame,\n",
        "            landmarks,\n",
        "            transform,\n",
        "            target_size,\n",
        "            interpolation,\n",
        "            border_mode,\n",
        "            border_value,\n",
        "        )\n",
        "\n",
        "        return transformed_frame, transformed_landmarks\n",
        "\n",
        "    def get_stable_reference(self, reference, reference_size, target_size):\n",
        "        # -- right eye, left eye, nose tip, mouth center\n",
        "        stable_reference = np.vstack(\n",
        "            [\n",
        "                np.mean(reference[36:42], axis=0),\n",
        "                np.mean(reference[42:48], axis=0),\n",
        "                np.mean(reference[31:36], axis=0),\n",
        "                np.mean(reference[48:68], axis=0),\n",
        "            ]\n",
        "        )\n",
        "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
        "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
        "        return stable_reference\n",
        "\n",
        "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
        "        return cv2.estimateAffinePartial2D(\n",
        "            np.vstack([landmarks[x] for x in stable_points]),\n",
        "            stable_reference,\n",
        "            method=cv2.LMEDS,\n",
        "        )[0]\n",
        "\n",
        "    def apply_affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        transform,\n",
        "        target_size,\n",
        "        interpolation,\n",
        "        border_mode,\n",
        "        border_value,\n",
        "    ):\n",
        "        transformed_frame = cv2.warpAffine(\n",
        "            frame,\n",
        "            transform,\n",
        "            dsize=(target_size[0], target_size[1]),\n",
        "            flags=interpolation,\n",
        "            borderMode=border_mode,\n",
        "            borderValue=border_value,\n",
        "        )\n",
        "        transformed_landmarks = (\n",
        "            np.matmul(landmarks, transform[:, :2].transpose())\n",
        "            + transform[:, 2].transpose()\n",
        "        )\n",
        "        return transformed_frame, transformed_landmarks"
      ],
      "metadata": {
        "id": "k6ahhX7gXohu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetinaFace and FAN"
      ],
      "metadata": {
        "id": "2rIk5Xf7Urtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hhj1897/face_alignment.git\n",
        "!git clone https://github.com/hhj1897/face_detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE9lkZSTV0YI",
        "outputId": "2c32b321-35c1-4920-e0d2-e8358682401f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face_alignment'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 190 (delta 27), reused 27 (delta 26), pack-reused 158\u001b[K\n",
            "Receiving objects: 100% (190/190), 213.82 MiB | 16.34 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Updating files: 100% (14/14), done.\n",
            "Cloning into 'face_detection'...\n",
            "remote: Enumerating objects: 300, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 300 (delta 41), reused 39 (delta 39), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (300/300), 81.19 MiB | 18.55 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "Updating files: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from face_detection.ibug.face_detection import RetinaFacePredictor\n",
        "from face_alignment.ibug.face_alignment import FANPredictor"
      ],
      "metadata": {
        "id": "p_IOSBvMV4ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self, device=\"cuda:0\", model_name=\"resnet50\"):\n",
        "        self.face_detector = RetinaFacePredictor(\n",
        "            device=device,\n",
        "            threshold=0.8,\n",
        "            model=RetinaFacePredictor.get_model(model_name),\n",
        "        )\n",
        "        self.landmark_detector = FANPredictor(device=device, model=None)\n",
        "\n",
        "    def __call__(self, video_frames):\n",
        "        landmarks = []\n",
        "        for frame in video_frames:\n",
        "            detected_faces = self.face_detector(frame, rgb=False)\n",
        "            face_points, _ = self.landmark_detector(frame, detected_faces, rgb=True)\n",
        "            if len(detected_faces) == 0:\n",
        "                landmarks.append(None)\n",
        "            else:\n",
        "                max_id, max_size = 0, 0\n",
        "                for idx, bbox in enumerate(detected_faces):\n",
        "                    bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "                    if bbox_size > max_size:\n",
        "                        max_id, max_size = idx, bbox_size\n",
        "                landmarks.append(face_points[max_id])\n",
        "        return landmarks"
      ],
      "metadata": {
        "id": "u2_Tl0YFUu2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = np.array([[ 70.92383848,  97.13757949],[ 72.62515288, 114.90361188],\n",
        "  [ 75.98941827, 131.07402473],[ 79.26959318, 146.2116596 ],[ 83.61348894, 163.26020701],\n",
        "  [ 91.33134186, 177.44535288],[100.27169245, 187.0885567 ],[112.12435016, 196.00353535],\n",
        "  [130.74175801, 200.52998862],[149.8553027 , 195.31198065],[163.101687  , 186.44060881],\n",
        "  [173.65334172, 176.57250158],[182.43614459, 162.27992572],[187.16738556, 145.09391978],\n",
        "  [190.22905333, 129.72418731],[193.02118502, 113.45923358],[194.43863372,  95.5795392 ],\n",
        "  [ 81.33095967,  80.79544511],[ 87.75906556,  75.27980275],[ 96.22692544,  73.83857497],\n",
        "  [104.55524335,  74.74029382],[112.23186144,  76.97670954],[144.49576387,  76.42387471],\n",
        "  [152.34799901,  73.83329748],[161.13054079,  72.63570385],[170.58715674,  73.84785054],\n",
        "  [178.21409885,  79.43802857],[128.7337425 ,  95.35962566],[128.48854473, 106.92459506],\n",
        "  [128.24475936, 118.27285086],[128.26596547, 127.69870727],[118.76000113, 135.19357677],\n",
        "  [122.96307973, 136.14619774],[128.87017961, 137.30253356],[134.9428314 , 135.99720543],\n",
        "  [139.48259748, 134.87763793],[ 92.52245553,  94.36876014],[ 97.58518219,  90.95977781],\n",
        "  [105.41368273,  90.91345887],[112.77241724,  94.9436087 ],[106.103635  ,  97.08485693],\n",
        "  [ 98.04628565,  97.36335869],[145.52511509,  94.53499862],[152.58953438,  90.21485666],\n",
        "  [160.61170666,  90.19938514],[166.67710071,  93.56562296],[160.55971572,  96.48125958],\n",
        "  [152.20465993,  96.47281336],[107.16760614, 157.19606764],[114.47611216, 152.12006957],\n",
        "  [123.84852759, 148.51863199],[128.97628288, 149.41552527],[134.14360703, 148.42628211],\n",
        "  [144.17717842, 151.79343262],[152.19284005, 156.98711116],[143.85966895, 164.00347101],\n",
        "  [136.7441507 , 167.9430006 ],[129.15278278, 168.81853366],[121.79511074, 168.02271929],\n",
        "  [115.27508573, 164.15159355],[109.23088653, 157.00172103],[122.50270762, 154.40733649],\n",
        "  [129.02862236, 154.12104227],[135.83648069, 154.31214998],[150.75782809, 156.79506004],\n",
        "  [135.66204627, 160.62976732],[128.95218547, 161.28762709],[122.48775432, 160.50878431]])\n",
        "\n",
        "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
        "    start_landmarks = landmarks[start_idx]\n",
        "    stop_landmarks = landmarks[stop_idx]\n",
        "    delta = stop_landmarks - start_landmarks\n",
        "    for idx in range(1, stop_idx - start_idx):\n",
        "        landmarks[start_idx + idx] = (\n",
        "            start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
        "        )\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def warp_img(src, dst, img, std_size):\n",
        "    tform = tf.estimate_transform(\"similarity\", src, dst)\n",
        "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped, tform\n",
        "\n",
        "\n",
        "def apply_transform(transform, img, std_size):\n",
        "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped\n",
        "\n",
        "\n",
        "def cut_patch(img, landmarks, height, width, threshold=5):\n",
        "    center_x, center_y = np.mean(landmarks, axis=0)\n",
        "    # Check for too much bias in height and width\n",
        "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
        "        raise OverflowError(\"too much bias in height\")\n",
        "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
        "        raise OverflowError(\"too much bias in width\")\n",
        "    # Calculate bounding box coordinates\n",
        "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
        "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
        "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
        "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
        "    # Cut the image\n",
        "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
        "    return cutted_img\n",
        "\n",
        "\n",
        "class VideoProcess:\n",
        "    def __init__(\n",
        "        self,\n",
        "        mean_face_path=\"20words_mean_face.npy\",\n",
        "        crop_width=96,\n",
        "        crop_height=96,\n",
        "        start_idx=48,\n",
        "        stop_idx=68,\n",
        "        window_margin=12,\n",
        "        convert_gray=True,\n",
        "    ):\n",
        "        # self.reference = np.load(\n",
        "        #     os.path.join(os.path.dirname(__file__), mean_face_path)\n",
        "        # )\n",
        "        self.reference = reference\n",
        "        self.crop_width = crop_width\n",
        "        self.crop_height = crop_height\n",
        "        self.start_idx = start_idx\n",
        "        self.stop_idx = stop_idx\n",
        "        self.window_margin = window_margin\n",
        "        self.convert_gray = convert_gray\n",
        "\n",
        "    def __call__(self, video, landmarks):\n",
        "        # Pre-process landmarks: interpolate frames that are not detected\n",
        "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
        "        # Exclude corner cases: no landmark in all frames or number of frames is less than window length\n",
        "        if (\n",
        "            not preprocessed_landmarks\n",
        "            or len(preprocessed_landmarks) < self.window_margin\n",
        "        ):\n",
        "            return\n",
        "        # Affine transformation and crop patch\n",
        "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
        "        assert sequence is not None, \"crop an empty patch.\"\n",
        "        return sequence\n",
        "\n",
        "    def crop_patch(self, video, landmarks):\n",
        "        sequence = []\n",
        "        for frame_idx, frame in enumerate(video):\n",
        "            window_margin = min(\n",
        "                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx\n",
        "            )\n",
        "            smoothed_landmarks = np.mean(\n",
        "                [\n",
        "                    landmarks[x]\n",
        "                    for x in range(\n",
        "                        frame_idx - window_margin, frame_idx + window_margin + 1\n",
        "                    )\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "            smoothed_landmarks += landmarks[frame_idx].mean(\n",
        "                axis=0\n",
        "            ) - smoothed_landmarks.mean(axis=0)\n",
        "            transformed_frame, transformed_landmarks = self.affine_transform(\n",
        "                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray\n",
        "            )\n",
        "            patch = cut_patch(\n",
        "                transformed_frame,\n",
        "                transformed_landmarks[self.start_idx : self.stop_idx],\n",
        "                self.crop_height // 2,\n",
        "                self.crop_width // 2,\n",
        "            )\n",
        "            sequence.append(patch)\n",
        "        return np.array(sequence)\n",
        "\n",
        "    def interpolate_landmarks(self, landmarks):\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        if not valid_frames_idx:\n",
        "            return None\n",
        "\n",
        "        for idx in range(1, len(valid_frames_idx)):\n",
        "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
        "                landmarks = linear_interpolate(\n",
        "                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]\n",
        "                )\n",
        "\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
        "        if valid_frames_idx:\n",
        "            landmarks[: valid_frames_idx[0]] = [\n",
        "                landmarks[valid_frames_idx[0]]\n",
        "            ] * valid_frames_idx[0]\n",
        "            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (\n",
        "                len(landmarks) - valid_frames_idx[-1]\n",
        "            )\n",
        "\n",
        "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    def affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        reference,\n",
        "        grayscale=True,\n",
        "        target_size=(256, 256),\n",
        "        reference_size=(256, 256),\n",
        "        stable_points=(28, 33, 36, 39, 42, 45, 48, 54),\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=0,\n",
        "    ):\n",
        "        if grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        stable_reference = self.get_stable_reference(\n",
        "            reference, stable_points, reference_size, target_size\n",
        "        )\n",
        "        transform = self.estimate_affine_transform(\n",
        "            landmarks, stable_points, stable_reference\n",
        "        )\n",
        "        transformed_frame, transformed_landmarks = self.apply_affine_transform(\n",
        "            frame,\n",
        "            landmarks,\n",
        "            transform,\n",
        "            target_size,\n",
        "            interpolation,\n",
        "            border_mode,\n",
        "            border_value,\n",
        "        )\n",
        "\n",
        "        return transformed_frame, transformed_landmarks\n",
        "\n",
        "    def get_stable_reference(\n",
        "        self, reference, stable_points, reference_size, target_size\n",
        "    ):\n",
        "        stable_reference = np.vstack([reference[x] for x in stable_points])\n",
        "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
        "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
        "        return stable_reference\n",
        "\n",
        "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
        "        return cv2.estimateAffinePartial2D(\n",
        "            np.vstack([landmarks[x] for x in stable_points]),\n",
        "            stable_reference,\n",
        "            method=cv2.LMEDS,\n",
        "        )[0]\n",
        "\n",
        "    def apply_affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        transform,\n",
        "        target_size,\n",
        "        interpolation,\n",
        "        border_mode,\n",
        "        border_value,\n",
        "    ):\n",
        "        transformed_frame = cv2.warpAffine(\n",
        "            frame,\n",
        "            transform,\n",
        "            dsize=(target_size[0], target_size[1]),\n",
        "            flags=interpolation,\n",
        "            borderMode=border_mode,\n",
        "            borderValue=border_value,\n",
        "        )\n",
        "        transformed_landmarks = (\n",
        "            np.matmul(landmarks, transform[:, :2].transpose())\n",
        "            + transform[:, 2].transpose()\n",
        "        )\n",
        "        return transformed_frame, transformed_landmarks"
      ],
      "metadata": {
        "id": "DAVsQMOLWsGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AVSRDataLoader"
      ],
      "metadata": {
        "id": "zAvtziY3UFbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AVSRDataLoader:\n",
        "    def __init__(self, modality, detector=\"retinaface\", convert_gray=True):\n",
        "        self.modality = modality\n",
        "        if modality == \"video\":\n",
        "            # if detector == \"retinaface\":\n",
        "            #     from detectors.retinaface.detector import LandmarksDetector\n",
        "            #     from detectors.retinaface.video_process import VideoProcess\n",
        "            #     self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
        "            #     self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "\n",
        "            # if detector == \"mediapipe\":\n",
        "            #     from detectors.mediapipe.detector import LandmarksDetector\n",
        "            #     from detectors.mediapipe.video_process import VideoProcess\n",
        "            #     self.landmarks_detector = LandmarksDetector()\n",
        "            #     self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "            self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
        "            self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "\n",
        "    def load_data(self, data_filename, landmarks=None, transform=True):\n",
        "        if self.modality == \"audio\":\n",
        "            audio, sample_rate = self.load_audio(data_filename)\n",
        "            audio = self.audio_process(audio, sample_rate)\n",
        "            return audio\n",
        "        if self.modality == \"video\":\n",
        "            video = self.load_video(data_filename)\n",
        "            if not landmarks:\n",
        "                landmarks = self.landmarks_detector(video)\n",
        "            video = self.video_process(video, landmarks)\n",
        "            if video is None:\n",
        "                raise TypeError(\"video cannot be None\")\n",
        "            video = torch.tensor(video)\n",
        "            return video\n",
        "\n",
        "    def load_audio(self, data_filename):\n",
        "        waveform, sample_rate = torchaudio.load(data_filename, normalize=True)\n",
        "        return waveform, sample_rate\n",
        "\n",
        "    def load_video(self, data_filename):\n",
        "        return torchvision.io.read_video(data_filename, pts_unit=\"sec\")[0].numpy()\n",
        "\n",
        "    def audio_process(self, waveform, sample_rate, target_sample_rate=16000):\n",
        "        if sample_rate != target_sample_rate:\n",
        "            waveform = torchaudio.functional.resample(\n",
        "                waveform, sample_rate, target_sample_rate\n",
        "            )\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        return waveform"
      ],
      "metadata": {
        "id": "8jQdMjnlNd9e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util"
      ],
      "metadata": {
        "id": "CiL6IrYoYdOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_file(filename, max_frames=600, fps=25.0):\n",
        "\n",
        "    lines = open(filename).read().splitlines()\n",
        "\n",
        "    flag = 0\n",
        "    stack = []\n",
        "    res = []\n",
        "\n",
        "    tmp = 0\n",
        "    start_timestamp = 0.0\n",
        "\n",
        "    threshold = max_frames / fps\n",
        "\n",
        "    for line in lines:\n",
        "        if \"WORD START END ASDSCORE\" in line:\n",
        "            flag = 1\n",
        "            continue\n",
        "        if flag:\n",
        "            word, start, end, score = line.split(\" \")\n",
        "            start, end, score = float(start), float(end), float(score)\n",
        "            if end < tmp + threshold:\n",
        "                stack.append(word)\n",
        "                last_timestamp = end\n",
        "            else:\n",
        "                res.append(\n",
        "                    [\n",
        "                        \" \".join(stack),\n",
        "                        start_timestamp,\n",
        "                        last_timestamp,\n",
        "                        last_timestamp - start_timestamp,\n",
        "                    ]\n",
        "                )\n",
        "                tmp = start\n",
        "                start_timestamp = start\n",
        "                stack = [word]\n",
        "    if stack:\n",
        "        res.append([\" \".join(stack), start_timestamp, end, end - start_timestamp])\n",
        "    return res\n",
        "\n",
        "\n",
        "def save_vid_txt(\n",
        "    dst_vid_filename, dst_txt_filename, trim_video_data, content, video_fps=25\n",
        "):\n",
        "    # -- save video\n",
        "    save2vid(dst_vid_filename, trim_video_data, video_fps)\n",
        "    # -- save text\n",
        "    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)\n",
        "    f = open(dst_txt_filename, \"w\", encoding=\"utf-8\")\n",
        "    f.write(f\"{content}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def save_vid_aud(\n",
        "    dst_vid_filename,\n",
        "    dst_aud_filename,\n",
        "    trim_vid_data,\n",
        "    trim_aud_data,\n",
        "    video_fps=25,\n",
        "    audio_sample_rate=16000,\n",
        "):\n",
        "    # -- save video\n",
        "    save2vid(dst_vid_filename, trim_vid_data, video_fps)\n",
        "    # -- save audio\n",
        "    save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)\n",
        "\n",
        "\n",
        "def save_vid_aud_txt(\n",
        "    dst_vid_filename,\n",
        "    dst_aud_filename,\n",
        "    dst_txt_filename,\n",
        "    trim_vid_data,\n",
        "    trim_aud_data,\n",
        "    content,\n",
        "    video_fps=25,\n",
        "    audio_sample_rate=16000,\n",
        "):\n",
        "    # -- save video\n",
        "    if dst_vid_filename is not None:\n",
        "        save2vid(dst_vid_filename, trim_vid_data, video_fps)\n",
        "    # -- save audio\n",
        "    if dst_aud_filename is not None:\n",
        "        save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)\n",
        "    # -- save text\n",
        "    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)\n",
        "    f = open(dst_txt_filename, \"w\", encoding=\"utf-8\")\n",
        "    f.write(f\"{content}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def save2vid(filename, vid, frames_per_second):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchvision.io.write_video(filename, vid, frames_per_second)\n",
        "\n",
        "\n",
        "def save2aud(filename, aud, sample_rate):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchaudio.save(filename, aud, sample_rate)"
      ],
      "metadata": {
        "id": "zA3biEp1UXj2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SJuNFb_Ycq0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper"
      ],
      "metadata": {
        "id": "Wq50mSe0jOLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfH5SDeijQTL",
        "outputId": "3b682001-ba92-4f1e-a990-4ada76c75174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-3knfobhl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-3knfobhl\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=4ee1fa22b8982e813a7405df89415f423ba98928483f74b58e699639fb0b4a0c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iivclny3/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from moviepy.editor import VideoFileClip"
      ],
      "metadata": {
        "id": "YQNyCAtujbq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_whisper = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "id": "8w4WWs7okcK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680d7490-f3ed-4184-c7bb-4522690e7480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:18<00:00, 80.8MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_texto(ruta, name):\n",
        "    video = VideoFileClip(ruta)\n",
        "    video.audio.write_audiofile(f'{name}.mp3', logger=None)\n",
        "    i = 0\n",
        "    while True:\n",
        "        result = model_whisper.transcribe(f'{name}.mp3')\n",
        "        temperature = result['segments'][0]['temperature']\n",
        "        compression_ratio = result['segments'][0]['compression_ratio']\n",
        "        no_speech_prob = result['segments'][0]['no_speech_prob']\n",
        "        #print(result)\n",
        "        if temperature == 0.0 and compression_ratio > 0.8:\n",
        "            text = result['text']\n",
        "            break\n",
        "        elif temperature != 1.0 and temperature > 0.2 and compression_ratio > 1.0:\n",
        "            text = result['text']\n",
        "            break\n",
        "        i += 1\n",
        "        if i>3:\n",
        "            print(result)\n",
        "\n",
        "    text = text[1:]\n",
        "    print(text)\n",
        "    print(f'temperature={temperature}')\n",
        "    print(f'compression ratio={compression_ratio}')\n",
        "    print(f'no_speech_prob={no_speech_prob}')\n",
        "    text = text.replace(',','')\n",
        "    text = text.replace('.','')\n",
        "    text = text.replace('¿','')\n",
        "    text = text.replace('?','')\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# def obtener_texto(ruta, audio):\n",
        "#     video = VideoFileClip(ruta)\n",
        "#     video.audio.write_audiofile(f'{audio}.mp3')\n",
        "#     # load audio and pad/trim it to fit 30 seconds\n",
        "#     audio = whisper.load_audio(f'{audio}.mp3')\n",
        "#     audio = whisper.pad_or_trim(audio)\n",
        "#     # make log-Mel spectrogram and move to the same device as the model\n",
        "#     mel = whisper.log_mel_spectrogram(audio).to(model_whisper.device)\n",
        "#     # detect the spoken language\n",
        "#     # _, probs = model.detect_language(mel)\n",
        "#     # print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "#     # decode the audio\n",
        "#     options = whisper.DecodingOptions()\n",
        "#     result = whisper.decode(model_whisper, mel, options)\n",
        "#     # recognized text\n",
        "#     return result.text"
      ],
      "metadata": {
        "id": "BHh8cPc_jumK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generacion de video procesado, y obtencion de texto"
      ],
      "metadata": {
        "id": "yPwPvqS9Y2Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_videos = '/content/drive/MyDrive/vsr/VPHB_USFX/videos'\n",
        "ruta_dataset = '/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing'\n",
        "list_videos_fa = os.listdir(ruta_videos)\n",
        "list_videos_pre = os.listdir(f'{ruta_dataset}/VPHBUSFX/video')\n",
        "list_videos = [v for v in list_videos_fa if v not in list_videos_pre]"
      ],
      "metadata": {
        "id": "aJZtCRYc81Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_videos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLCa_a0j85YD",
        "outputId": "9a0042da-9845-4327-8c11-7f31da800761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_dataloader = AVSRDataLoader(modality=\"video\", detector=\"retinaface\", convert_gray=False)\n",
        "audio_dataloader = AVSRDataLoader(modality=\"audio\")\n",
        "\n",
        "for data_filename in tqdm(list_videos[0:2]):\n",
        "    print(data_filename)\n",
        "    name = data_filename.split('.')[0]\n",
        "    ruta = f'{ruta_videos}/{data_filename}'\n",
        "    # Obtener video y texto\n",
        "    text_data = obtener_texto(ruta, name)\n",
        "    print(text_data)\n",
        "    video_data = video_dataloader.load_data(ruta)\n",
        "    audio_data = audio_dataloader.load_data(ruta)\n",
        "    # Guardar texto y video\n",
        "    output_video_path = f'{ruta_dataset}/VPHBUSFX/video/{name}.mp4'\n",
        "    output_audio_path = f'{ruta_dataset}/VPHBUSFX/video/{name}.wav'\n",
        "    output_text_path = f'{ruta_dataset}/VPHBUSFX/text/{name}.txt'\n",
        "    # save_vid_txt(output_video_path, output_text_path,\n",
        "    #              video_data, text_data)\n",
        "    save_vid_aud_txt(output_video_path, output_audio_path, output_text_path,\n",
        "                    video_data, audio_data, text_data,\n",
        "                    video_fps=25, audio_sample_rate=16000)\n",
        "    print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "PrOUrv8lY4RF",
        "outputId": "4fcefdf8-6d7e-488f-f0e9-d6fac8370704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c1PQUkqTm3A_V1-0003.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-33ba731f00f5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mruta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{ruta_videos}/{data_filename}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Obtener video y texto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobtener_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvideo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-59be49fd51c3>\u001b[0m in \u001b[0;36mobtener_texto\u001b[0;34m(ruta, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_whisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{name}.mp3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mcompression_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mno_speech_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no_speech_prob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ly65uzLSD_rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5jVbsPynfIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextTransform"
      ],
      "metadata": {
        "id": "2Vf5mUY66pRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece"
      ],
      "metadata": {
        "id": "n_Mh7W8j66-Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SP_MODEL_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"drive\",\n",
        "    \"MyDrive\",\n",
        "    \"vsr\",\n",
        "    \"Models\",\n",
        "    \"SentencePiece_castellano_v3\",\n",
        "    \"unigram2000.model\",\n",
        ")\n",
        "\n",
        "DICT_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"drive\",\n",
        "    \"MyDrive\",\n",
        "    \"vsr\",\n",
        "    \"Models\",\n",
        "    \"SentencePiece_castellano_v3\",\n",
        "    \"unigram2000_units.txt\",\n",
        ")\n",
        "\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Mapping Dictionary Class for SentencePiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sp_model_path=SP_MODEL_PATH,\n",
        "        dict_path=DICT_PATH,\n",
        "    ):\n",
        "\n",
        "        # Load SentencePiece model\n",
        "        self.spm = sentencepiece.SentencePieceProcessor(model_file=sp_model_path)\n",
        "\n",
        "        # Load units and create dictionary\n",
        "        units = open(dict_path, encoding='utf8').read().splitlines()\n",
        "        self.hashmap = {unit.split()[0]: unit.split()[-1] for unit in units}\n",
        "        # 0 will be used for \"blank\" in CTC\n",
        "        self.token_list = [\"<blank>\"] + list(self.hashmap.keys()) + [\"<eos>\"]\n",
        "        self.ignore_id = -1\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = self.spm.EncodeAsPieces(text)\n",
        "        token_ids = [self.hashmap.get(token, self.hashmap[\"<unk>\"]) for token in tokens]\n",
        "        return torch.tensor(list(map(int, token_ids)))\n",
        "\n",
        "    def post_process(self, token_ids):\n",
        "        token_ids = token_ids[token_ids != -1]\n",
        "        text = self._ids_to_str(token_ids, self.token_list)\n",
        "        text = text.replace(\"\\u2581\", \" \").strip()\n",
        "        return text\n",
        "\n",
        "    def _ids_to_str(self, token_ids, char_list):\n",
        "        token_as_list = [char_list[idx] for idx in token_ids]\n",
        "        return \"\".join(token_as_list).replace(\"<space>\", \" \")"
      ],
      "metadata": {
        "id": "U2pjDXiA6o0S"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer y generacion de csv"
      ],
      "metadata": {
        "id": "bu4mQFgBzwGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_transform = TextTransform()"
      ],
      "metadata": {
        "id": "z2lOzc6P6Xio"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = \"/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing\""
      ],
      "metadata": {
        "id": "c0EKk85mnfFB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_video = [video for video in os.listdir(f'{ruta}/VPHBUSFX/video') if video.split('.')[1]=='mp4']\n",
        "list_video_text = [[f'{ruta}/VPHBUSFX/video/{name}.mp4',f'{ruta}/VPHBUSFX/text/{name}.txt'] for name in [n.split('.')[0] for n in list_video]]\n",
        "list_csv = []\n",
        "for r_vid, r_text in tqdm(list_video_text):\n",
        "    video = torchvision.io.read_video(r_vid, pts_unit=\"sec\")[0].numpy()\n",
        "    text = open(r_text, 'r').read()\n",
        "    token_id_str = \" \".join(\n",
        "            map(str, [_.item() for _ in text_transform.tokenize(text)])\n",
        "    )\n",
        "    name_video = r_vid.split('/')[-1]\n",
        "    list_csv.append(['VPHBUSFX', f'video/{name_video}', video.shape[0], token_id_str])\n",
        "df_csv = pd.DataFrame(list_csv, columns=['Dataset','ruta','input_length','token_id'])\n",
        "df_csv.to_csv(f'{ruta}/labels/VPHBUSFX_unigram2000.csv')\n",
        "df_csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Yl3NGeGd0_Ev",
        "outputId": "5db51f2c-3d9f-4342-8a52-f9242146b484"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 712/712 [03:26<00:00,  3.45it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Dataset                           ruta  input_length  \\\n",
              "0    VPHBUSFX  video/mZxweoeJEcc_V1-0007.mp4           444   \n",
              "1    VPHBUSFX  video/mZxweoeJEcc_V1-0008.mp4           309   \n",
              "2    VPHBUSFX  video/mZxweoeJEcc_V1-0009.mp4           254   \n",
              "3    VPHBUSFX  video/mZxweoeJEcc_V1-0010.mp4           324   \n",
              "4    VPHBUSFX  video/mZxweoeJEcc_V1-0011.mp4           313   \n",
              "..        ...                            ...           ...   \n",
              "707  VPHBUSFX  video/mZxweoeJEcc_V1-0002.mp4           315   \n",
              "708  VPHBUSFX  video/mZxweoeJEcc_V1-0003.mp4           366   \n",
              "709  VPHBUSFX  video/mZxweoeJEcc_V1-0004.mp4           336   \n",
              "710  VPHBUSFX  video/mZxweoeJEcc_V1-0005.mp4           285   \n",
              "711  VPHBUSFX  video/mZxweoeJEcc_V1-0006.mp4           301   \n",
              "\n",
              "                                              token_id  \n",
              "0    1332 125 1397 586 1930 1277 60 1022 1908 543 1...  \n",
              "1    730 1490 667 1341 523 60 1115 1474 258 1930 12...  \n",
              "2    668 1835 1293 21 586 1687 589 1115 414 657 163...  \n",
              "3    1726 657 1391 662 712 1348 451 551 731 1491 51...  \n",
              "4    1993 1726 1324 1166 891 1293 910 523 125 1391 ...  \n",
              "..                                                 ...  \n",
              "707  1582 731 1744 1293 1174 1574 1127 714 1174 451...  \n",
              "708  1573 639 730 1166 1574 1583 1347 1574 707 61 1...  \n",
              "709  1324 1365 676 1574 1166 1574 714 1174 451 551 ...  \n",
              "710  523 60 1115 1695 1471 60 1850 657 1391 662 172...  \n",
              "711  1726 657 1391 662 891 713 451 551 731 60 761 1...  \n",
              "\n",
              "[712 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dbc5a019-df0a-49a1-8600-2b820efd35bc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>ruta</th>\n",
              "      <th>input_length</th>\n",
              "      <th>token_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0007.mp4</td>\n",
              "      <td>444</td>\n",
              "      <td>1332 125 1397 586 1930 1277 60 1022 1908 543 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0008.mp4</td>\n",
              "      <td>309</td>\n",
              "      <td>730 1490 667 1341 523 60 1115 1474 258 1930 12...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0009.mp4</td>\n",
              "      <td>254</td>\n",
              "      <td>668 1835 1293 21 586 1687 589 1115 414 657 163...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0010.mp4</td>\n",
              "      <td>324</td>\n",
              "      <td>1726 657 1391 662 712 1348 451 551 731 1491 51...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0011.mp4</td>\n",
              "      <td>313</td>\n",
              "      <td>1993 1726 1324 1166 891 1293 910 523 125 1391 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>707</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0002.mp4</td>\n",
              "      <td>315</td>\n",
              "      <td>1582 731 1744 1293 1174 1574 1127 714 1174 451...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>708</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0003.mp4</td>\n",
              "      <td>366</td>\n",
              "      <td>1573 639 730 1166 1574 1583 1347 1574 707 61 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>709</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0004.mp4</td>\n",
              "      <td>336</td>\n",
              "      <td>1324 1365 676 1574 1166 1574 714 1174 451 551 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>710</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0005.mp4</td>\n",
              "      <td>285</td>\n",
              "      <td>523 60 1115 1695 1471 60 1850 657 1391 662 172...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/mZxweoeJEcc_V1-0006.mp4</td>\n",
              "      <td>301</td>\n",
              "      <td>1726 657 1391 662 891 713 451 551 731 60 761 1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>712 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbc5a019-df0a-49a1-8600-2b820efd35bc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dbc5a019-df0a-49a1-8600-2b820efd35bc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dbc5a019-df0a-49a1-8600-2b820efd35bc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d45244cf-59c2-481e-9feb-e279ecd1cdb1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d45244cf-59c2-481e-9feb-e279ecd1cdb1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d45244cf-59c2-481e-9feb-e279ecd1cdb1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_41016fb4-eac6-4047-84ac-e065eeeae263\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_csv')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_41016fb4-eac6-4047-84ac-e065eeeae263 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_csv');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_csv",
              "summary": "{\n  \"name\": \"df_csv\",\n  \"rows\": 712,\n  \"fields\": [\n    {\n      \"column\": \"Dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"VPHBUSFX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ruta\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 712,\n        \"samples\": [\n          \"video/20qxEI3vPk4_V1-0007.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95,\n        \"min\": 74,\n        \"max\": 670,\n        \"num_unique_values\": 320,\n        \"samples\": [\n          268\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 712,\n        \"samples\": [\n          \"1324 1409 1574 905 1277 1906 60 1687 112 6 1592 950 1324 1409 1574 1935 1277 125 275 1010 1993 60 1426 183 60 125 887 1177 851 260 1078 730 1409 523 1716 1573 639 761 1994 901 967 392 1248 1967 733 1967 1574 6 1884 1223 891 1687 1630 126 112 1573 639 761 1994 901 967 1744 1994 1906 268 1401 631 1744 1994 1906 268 918 1744 1994 1906 268 913 1744 1994 1906 268 360\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtener train, val"
      ],
      "metadata": {
        "id": "E3pA7PK_-z6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "vz2kU0p-1U9U"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df_csv, test_size=0.1)"
      ],
      "metadata": {
        "id": "gPKGInxa2f9j"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape, df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPYiFpJs3DYU",
        "outputId": "312f11bc-3448-4b05-b04e-ffbe673b1156"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((640, 4), (72, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_csv(f'{ruta}/labels/VPHBUSFX_unigram2000_train.csv')\n",
        "df_test.to_csv(f'{ruta}/labels/VPHBUSFX_unigram2000_val.csv')"
      ],
      "metadata": {
        "id": "-zu5jv8E99-7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0dxNRb6-3G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmKRDsSp-2_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RN8l2ftSeKiq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}