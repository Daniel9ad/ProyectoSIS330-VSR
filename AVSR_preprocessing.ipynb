{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1jLgxJNjUvK0",
        "2rIk5Xf7Urtz",
        "zAvtziY3UFbe",
        "CiL6IrYoYdOy",
        "Wq50mSe0jOLG",
        "2Vf5mUY66pRJ",
        "bu4mQFgBzwGY",
        "E3pA7PK_-z6c"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZgCs7dGcNVGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ETCcaQGNNWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251e7bd3-7f97-4c1a-eb2b-e0fd11097935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting av\n",
            "  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg-python\n",
        "!pip install av"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "XZlrG64BUNYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchvision\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import transform as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xREsj1urUPos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MediaPipe"
      ],
      "metadata": {
        "id": "1jLgxJNjUvK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "6iuJF2_VXmWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp"
      ],
      "metadata": {
        "id": "XkGKjc61XbJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self):\n",
        "        self.mp_face_detection = mp.solutions.face_detection\n",
        "        self.short_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=0)\n",
        "        self.full_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=1)\n",
        "\n",
        "    def __call__(self, video_frames):\n",
        "        landmarks = self.detect(video_frames, self.full_range_detector)\n",
        "        if all(element is None for element in landmarks):\n",
        "            landmarks = self.detect(video_frames, self.short_range_detector)\n",
        "            assert any(l is not None for l in landmarks), \"Cannot detect any frames in the video\"\n",
        "        return landmarks\n",
        "\n",
        "    def detect(self, video_frames, detector):\n",
        "        landmarks = []\n",
        "        for frame in video_frames:\n",
        "            results = detector.process(frame)\n",
        "            if not results.detections:\n",
        "                landmarks.append(None)\n",
        "                continue\n",
        "            face_points = []\n",
        "            for idx, detected_faces in enumerate(results.detections):\n",
        "                max_id, max_size = 0, 0\n",
        "                bboxC = detected_faces.location_data.relative_bounding_box\n",
        "                ih, iw, ic = frame.shape\n",
        "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
        "                bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "                if bbox_size > max_size:\n",
        "                    max_id, max_size = idx, bbox_size\n",
        "                lmx = [\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].y * ih)],\n",
        "                    ]\n",
        "                face_points.append(lmx)\n",
        "            landmarks.append(np.array(face_points[max_id]))\n",
        "        return landmarks"
      ],
      "metadata": {
        "id": "7TAzoxo8XVxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = np.array([[ 70.92383848,  97.13757949],[ 72.62515288, 114.90361188],\n",
        "  [ 75.98941827, 131.07402473],[ 79.26959318, 146.2116596 ],[ 83.61348894, 163.26020701],\n",
        "  [ 91.33134186, 177.44535288],[100.27169245, 187.0885567 ],[112.12435016, 196.00353535],\n",
        "  [130.74175801, 200.52998862],[149.8553027 , 195.31198065],[163.101687  , 186.44060881],\n",
        "  [173.65334172, 176.57250158],[182.43614459, 162.27992572],[187.16738556, 145.09391978],\n",
        "  [190.22905333, 129.72418731],[193.02118502, 113.45923358],[194.43863372,  95.5795392 ],\n",
        "  [ 81.33095967,  80.79544511],[ 87.75906556,  75.27980275],[ 96.22692544,  73.83857497],\n",
        "  [104.55524335,  74.74029382],[112.23186144,  76.97670954],[144.49576387,  76.42387471],\n",
        "  [152.34799901,  73.83329748],[161.13054079,  72.63570385],[170.58715674,  73.84785054],\n",
        "  [178.21409885,  79.43802857],[128.7337425 ,  95.35962566],[128.48854473, 106.92459506],\n",
        "  [128.24475936, 118.27285086],[128.26596547, 127.69870727],[118.76000113, 135.19357677],\n",
        "  [122.96307973, 136.14619774],[128.87017961, 137.30253356],[134.9428314 , 135.99720543],\n",
        "  [139.48259748, 134.87763793],[ 92.52245553,  94.36876014],[ 97.58518219,  90.95977781],\n",
        "  [105.41368273,  90.91345887],[112.77241724,  94.9436087 ],[106.103635  ,  97.08485693],\n",
        "  [ 98.04628565,  97.36335869],[145.52511509,  94.53499862],[152.58953438,  90.21485666],\n",
        "  [160.61170666,  90.19938514],[166.67710071,  93.56562296],[160.55971572,  96.48125958],\n",
        "  [152.20465993,  96.47281336],[107.16760614, 157.19606764],[114.47611216, 152.12006957],\n",
        "  [123.84852759, 148.51863199],[128.97628288, 149.41552527],[134.14360703, 148.42628211],\n",
        "  [144.17717842, 151.79343262],[152.19284005, 156.98711116],[143.85966895, 164.00347101],\n",
        "  [136.7441507 , 167.9430006 ],[129.15278278, 168.81853366],[121.79511074, 168.02271929],\n",
        "  [115.27508573, 164.15159355],[109.23088653, 157.00172103],[122.50270762, 154.40733649],\n",
        "  [129.02862236, 154.12104227],[135.83648069, 154.31214998],[150.75782809, 156.79506004],\n",
        "  [135.66204627, 160.62976732],[128.95218547, 161.28762709],[122.48775432, 160.50878431]])\n",
        "\n",
        "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
        "    start_landmarks = landmarks[start_idx]\n",
        "    stop_landmarks = landmarks[stop_idx]\n",
        "    delta = stop_landmarks - start_landmarks\n",
        "    for idx in range(1, stop_idx - start_idx):\n",
        "        landmarks[start_idx + idx] = (\n",
        "            start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
        "        )\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def warp_img(src, dst, img, std_size):\n",
        "    tform = tf.estimate_transform(\"similarity\", src, dst)\n",
        "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped, tform\n",
        "\n",
        "\n",
        "def apply_transform(transform, img, std_size):\n",
        "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped\n",
        "\n",
        "\n",
        "def cut_patch(img, landmarks, height, width, threshold=5):\n",
        "    center_x, center_y = np.mean(landmarks, axis=0)\n",
        "    # Check for too much bias in height and width\n",
        "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
        "        raise OverflowError(\"too much bias in height\")\n",
        "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
        "        raise OverflowError(\"too much bias in width\")\n",
        "    # Calculate bounding box coordinates\n",
        "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
        "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
        "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
        "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
        "    # Cut the image\n",
        "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
        "    return cutted_img\n",
        "\n",
        "\n",
        "class VideoProcess:\n",
        "    def __init__(\n",
        "        self,\n",
        "        mean_face_path=\"20words_mean_face.npy\",\n",
        "        crop_width=96,\n",
        "        crop_height=96,\n",
        "        start_idx=3,\n",
        "        stop_idx=4,\n",
        "        window_margin=12,\n",
        "        convert_gray=True,\n",
        "    ):\n",
        "        # self.reference = np.load(\n",
        "        #     os.path.join(os.path.dirname(__file__), mean_face_path)\n",
        "        # )\n",
        "        self.reference = reference\n",
        "        self.crop_width = crop_width\n",
        "        self.crop_height = crop_height\n",
        "        self.start_idx = start_idx\n",
        "        self.stop_idx = stop_idx\n",
        "        self.window_margin = window_margin\n",
        "        self.convert_gray = convert_gray\n",
        "\n",
        "    def __call__(self, video, landmarks):\n",
        "        # Pre-process landmarks: interpolate frames that are not detected\n",
        "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
        "        # Exclude corner cases: no landmark in all frames\n",
        "        if not preprocessed_landmarks:\n",
        "            return\n",
        "        # Affine transformation and crop patch\n",
        "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
        "        assert sequence is not None, \"crop an empty patch.\"\n",
        "        return sequence\n",
        "\n",
        "    def crop_patch(self, video, landmarks):\n",
        "        sequence = []\n",
        "        for frame_idx, frame in enumerate(video):\n",
        "            window_margin = min(\n",
        "                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx\n",
        "            )\n",
        "            smoothed_landmarks = np.mean(\n",
        "                [\n",
        "                    landmarks[x]\n",
        "                    for x in range(\n",
        "                        frame_idx - window_margin, frame_idx + window_margin + 1\n",
        "                    )\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "            smoothed_landmarks += landmarks[frame_idx].mean(\n",
        "                axis=0\n",
        "            ) - smoothed_landmarks.mean(axis=0)\n",
        "            transformed_frame, transformed_landmarks = self.affine_transform(\n",
        "                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray\n",
        "            )\n",
        "            patch = cut_patch(\n",
        "                transformed_frame,\n",
        "                transformed_landmarks[self.start_idx : self.stop_idx],\n",
        "                self.crop_height // 2,\n",
        "                self.crop_width // 2,\n",
        "            )\n",
        "            sequence.append(patch)\n",
        "        return np.array(sequence)\n",
        "\n",
        "    def interpolate_landmarks(self, landmarks):\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        if not valid_frames_idx:\n",
        "            return None\n",
        "\n",
        "        for idx in range(1, len(valid_frames_idx)):\n",
        "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
        "                landmarks = linear_interpolate(\n",
        "                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]\n",
        "                )\n",
        "\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
        "        if valid_frames_idx:\n",
        "            landmarks[: valid_frames_idx[0]] = [\n",
        "                landmarks[valid_frames_idx[0]]\n",
        "            ] * valid_frames_idx[0]\n",
        "            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (\n",
        "                len(landmarks) - valid_frames_idx[-1]\n",
        "            )\n",
        "\n",
        "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    def affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        reference,\n",
        "        grayscale=False,\n",
        "        target_size=(256, 256),\n",
        "        reference_size=(256, 256),\n",
        "        stable_points=(0, 1, 2, 3),\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=0,\n",
        "    ):\n",
        "        if grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        stable_reference = self.get_stable_reference(\n",
        "            reference, reference_size, target_size\n",
        "        )\n",
        "        transform = self.estimate_affine_transform(\n",
        "            landmarks, stable_points, stable_reference\n",
        "        )\n",
        "        transformed_frame, transformed_landmarks = self.apply_affine_transform(\n",
        "            frame,\n",
        "            landmarks,\n",
        "            transform,\n",
        "            target_size,\n",
        "            interpolation,\n",
        "            border_mode,\n",
        "            border_value,\n",
        "        )\n",
        "\n",
        "        return transformed_frame, transformed_landmarks\n",
        "\n",
        "    def get_stable_reference(self, reference, reference_size, target_size):\n",
        "        # -- right eye, left eye, nose tip, mouth center\n",
        "        stable_reference = np.vstack(\n",
        "            [\n",
        "                np.mean(reference[36:42], axis=0),\n",
        "                np.mean(reference[42:48], axis=0),\n",
        "                np.mean(reference[31:36], axis=0),\n",
        "                np.mean(reference[48:68], axis=0),\n",
        "            ]\n",
        "        )\n",
        "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
        "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
        "        return stable_reference\n",
        "\n",
        "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
        "        return cv2.estimateAffinePartial2D(\n",
        "            np.vstack([landmarks[x] for x in stable_points]),\n",
        "            stable_reference,\n",
        "            method=cv2.LMEDS,\n",
        "        )[0]\n",
        "\n",
        "    def apply_affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        transform,\n",
        "        target_size,\n",
        "        interpolation,\n",
        "        border_mode,\n",
        "        border_value,\n",
        "    ):\n",
        "        transformed_frame = cv2.warpAffine(\n",
        "            frame,\n",
        "            transform,\n",
        "            dsize=(target_size[0], target_size[1]),\n",
        "            flags=interpolation,\n",
        "            borderMode=border_mode,\n",
        "            borderValue=border_value,\n",
        "        )\n",
        "        transformed_landmarks = (\n",
        "            np.matmul(landmarks, transform[:, :2].transpose())\n",
        "            + transform[:, 2].transpose()\n",
        "        )\n",
        "        return transformed_frame, transformed_landmarks"
      ],
      "metadata": {
        "id": "k6ahhX7gXohu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetinaFace and FAN"
      ],
      "metadata": {
        "id": "2rIk5Xf7Urtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hhj1897/face_alignment.git\n",
        "!git clone https://github.com/hhj1897/face_detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE9lkZSTV0YI",
        "outputId": "c5668ecf-785e-4c07-d0b7-1dd2abf444dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face_alignment'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 190 (delta 27), reused 27 (delta 26), pack-reused 158\u001b[K\n",
            "Receiving objects: 100% (190/190), 213.82 MiB | 37.31 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Updating files: 100% (14/14), done.\n",
            "Cloning into 'face_detection'...\n",
            "remote: Enumerating objects: 300, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 300 (delta 41), reused 39 (delta 39), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (300/300), 81.19 MiB | 31.14 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from face_detection.ibug.face_detection import RetinaFacePredictor\n",
        "from face_alignment.ibug.face_alignment import FANPredictor"
      ],
      "metadata": {
        "id": "p_IOSBvMV4ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self, device=\"cuda:0\", model_name=\"resnet50\"):\n",
        "        self.face_detector = RetinaFacePredictor(\n",
        "            device=device,\n",
        "            threshold=0.8,\n",
        "            model=RetinaFacePredictor.get_model(model_name),\n",
        "        )\n",
        "        self.landmark_detector = FANPredictor(device=device, model=None)\n",
        "\n",
        "    def __call__(self, video_frames):\n",
        "        landmarks = []\n",
        "        for frame in video_frames:\n",
        "            detected_faces = self.face_detector(frame, rgb=False)\n",
        "            face_points, _ = self.landmark_detector(frame, detected_faces, rgb=True)\n",
        "            if len(detected_faces) == 0:\n",
        "                landmarks.append(None)\n",
        "            else:\n",
        "                max_id, max_size = 0, 0\n",
        "                for idx, bbox in enumerate(detected_faces):\n",
        "                    bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "                    if bbox_size > max_size:\n",
        "                        max_id, max_size = idx, bbox_size\n",
        "                landmarks.append(face_points[max_id])\n",
        "        return landmarks"
      ],
      "metadata": {
        "id": "u2_Tl0YFUu2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = np.array([[ 70.92383848,  97.13757949],[ 72.62515288, 114.90361188],\n",
        "  [ 75.98941827, 131.07402473],[ 79.26959318, 146.2116596 ],[ 83.61348894, 163.26020701],\n",
        "  [ 91.33134186, 177.44535288],[100.27169245, 187.0885567 ],[112.12435016, 196.00353535],\n",
        "  [130.74175801, 200.52998862],[149.8553027 , 195.31198065],[163.101687  , 186.44060881],\n",
        "  [173.65334172, 176.57250158],[182.43614459, 162.27992572],[187.16738556, 145.09391978],\n",
        "  [190.22905333, 129.72418731],[193.02118502, 113.45923358],[194.43863372,  95.5795392 ],\n",
        "  [ 81.33095967,  80.79544511],[ 87.75906556,  75.27980275],[ 96.22692544,  73.83857497],\n",
        "  [104.55524335,  74.74029382],[112.23186144,  76.97670954],[144.49576387,  76.42387471],\n",
        "  [152.34799901,  73.83329748],[161.13054079,  72.63570385],[170.58715674,  73.84785054],\n",
        "  [178.21409885,  79.43802857],[128.7337425 ,  95.35962566],[128.48854473, 106.92459506],\n",
        "  [128.24475936, 118.27285086],[128.26596547, 127.69870727],[118.76000113, 135.19357677],\n",
        "  [122.96307973, 136.14619774],[128.87017961, 137.30253356],[134.9428314 , 135.99720543],\n",
        "  [139.48259748, 134.87763793],[ 92.52245553,  94.36876014],[ 97.58518219,  90.95977781],\n",
        "  [105.41368273,  90.91345887],[112.77241724,  94.9436087 ],[106.103635  ,  97.08485693],\n",
        "  [ 98.04628565,  97.36335869],[145.52511509,  94.53499862],[152.58953438,  90.21485666],\n",
        "  [160.61170666,  90.19938514],[166.67710071,  93.56562296],[160.55971572,  96.48125958],\n",
        "  [152.20465993,  96.47281336],[107.16760614, 157.19606764],[114.47611216, 152.12006957],\n",
        "  [123.84852759, 148.51863199],[128.97628288, 149.41552527],[134.14360703, 148.42628211],\n",
        "  [144.17717842, 151.79343262],[152.19284005, 156.98711116],[143.85966895, 164.00347101],\n",
        "  [136.7441507 , 167.9430006 ],[129.15278278, 168.81853366],[121.79511074, 168.02271929],\n",
        "  [115.27508573, 164.15159355],[109.23088653, 157.00172103],[122.50270762, 154.40733649],\n",
        "  [129.02862236, 154.12104227],[135.83648069, 154.31214998],[150.75782809, 156.79506004],\n",
        "  [135.66204627, 160.62976732],[128.95218547, 161.28762709],[122.48775432, 160.50878431]])\n",
        "\n",
        "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
        "    start_landmarks = landmarks[start_idx]\n",
        "    stop_landmarks = landmarks[stop_idx]\n",
        "    delta = stop_landmarks - start_landmarks\n",
        "    for idx in range(1, stop_idx - start_idx):\n",
        "        landmarks[start_idx + idx] = (\n",
        "            start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
        "        )\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def warp_img(src, dst, img, std_size):\n",
        "    tform = tf.estimate_transform(\"similarity\", src, dst)\n",
        "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped, tform\n",
        "\n",
        "\n",
        "def apply_transform(transform, img, std_size):\n",
        "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped\n",
        "\n",
        "\n",
        "def cut_patch(img, landmarks, height, width, threshold=5):\n",
        "    center_x, center_y = np.mean(landmarks, axis=0)\n",
        "    # Check for too much bias in height and width\n",
        "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
        "        raise OverflowError(\"too much bias in height\")\n",
        "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
        "        raise OverflowError(\"too much bias in width\")\n",
        "    # Calculate bounding box coordinates\n",
        "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
        "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
        "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
        "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
        "    # Cut the image\n",
        "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
        "    return cutted_img\n",
        "\n",
        "\n",
        "class VideoProcess:\n",
        "    def __init__(\n",
        "        self,\n",
        "        mean_face_path=\"20words_mean_face.npy\",\n",
        "        crop_width=96,\n",
        "        crop_height=96,\n",
        "        start_idx=48,\n",
        "        stop_idx=68,\n",
        "        window_margin=12,\n",
        "        convert_gray=True,\n",
        "    ):\n",
        "        # self.reference = np.load(\n",
        "        #     os.path.join(os.path.dirname(__file__), mean_face_path)\n",
        "        # )\n",
        "        self.reference = reference\n",
        "        self.crop_width = crop_width\n",
        "        self.crop_height = crop_height\n",
        "        self.start_idx = start_idx\n",
        "        self.stop_idx = stop_idx\n",
        "        self.window_margin = window_margin\n",
        "        self.convert_gray = convert_gray\n",
        "\n",
        "    def __call__(self, video, landmarks):\n",
        "        # Pre-process landmarks: interpolate frames that are not detected\n",
        "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
        "        # Exclude corner cases: no landmark in all frames or number of frames is less than window length\n",
        "        if (\n",
        "            not preprocessed_landmarks\n",
        "            or len(preprocessed_landmarks) < self.window_margin\n",
        "        ):\n",
        "            return\n",
        "        # Affine transformation and crop patch\n",
        "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
        "        assert sequence is not None, \"crop an empty patch.\"\n",
        "        return sequence\n",
        "\n",
        "    def crop_patch(self, video, landmarks):\n",
        "        sequence = []\n",
        "        for frame_idx, frame in enumerate(video):\n",
        "            window_margin = min(\n",
        "                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx\n",
        "            )\n",
        "            smoothed_landmarks = np.mean(\n",
        "                [\n",
        "                    landmarks[x]\n",
        "                    for x in range(\n",
        "                        frame_idx - window_margin, frame_idx + window_margin + 1\n",
        "                    )\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "            smoothed_landmarks += landmarks[frame_idx].mean(\n",
        "                axis=0\n",
        "            ) - smoothed_landmarks.mean(axis=0)\n",
        "            transformed_frame, transformed_landmarks = self.affine_transform(\n",
        "                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray\n",
        "            )\n",
        "            patch = cut_patch(\n",
        "                transformed_frame,\n",
        "                transformed_landmarks[self.start_idx : self.stop_idx],\n",
        "                self.crop_height // 2,\n",
        "                self.crop_width // 2,\n",
        "            )\n",
        "            sequence.append(patch)\n",
        "        return np.array(sequence)\n",
        "\n",
        "    def interpolate_landmarks(self, landmarks):\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        if not valid_frames_idx:\n",
        "            return None\n",
        "\n",
        "        for idx in range(1, len(valid_frames_idx)):\n",
        "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
        "                landmarks = linear_interpolate(\n",
        "                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]\n",
        "                )\n",
        "\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
        "        if valid_frames_idx:\n",
        "            landmarks[: valid_frames_idx[0]] = [\n",
        "                landmarks[valid_frames_idx[0]]\n",
        "            ] * valid_frames_idx[0]\n",
        "            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (\n",
        "                len(landmarks) - valid_frames_idx[-1]\n",
        "            )\n",
        "\n",
        "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    def affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        reference,\n",
        "        grayscale=True,\n",
        "        target_size=(256, 256),\n",
        "        reference_size=(256, 256),\n",
        "        stable_points=(28, 33, 36, 39, 42, 45, 48, 54),\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=0,\n",
        "    ):\n",
        "        if grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        stable_reference = self.get_stable_reference(\n",
        "            reference, stable_points, reference_size, target_size\n",
        "        )\n",
        "        transform = self.estimate_affine_transform(\n",
        "            landmarks, stable_points, stable_reference\n",
        "        )\n",
        "        transformed_frame, transformed_landmarks = self.apply_affine_transform(\n",
        "            frame,\n",
        "            landmarks,\n",
        "            transform,\n",
        "            target_size,\n",
        "            interpolation,\n",
        "            border_mode,\n",
        "            border_value,\n",
        "        )\n",
        "\n",
        "        return transformed_frame, transformed_landmarks\n",
        "\n",
        "    def get_stable_reference(\n",
        "        self, reference, stable_points, reference_size, target_size\n",
        "    ):\n",
        "        stable_reference = np.vstack([reference[x] for x in stable_points])\n",
        "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
        "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
        "        return stable_reference\n",
        "\n",
        "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
        "        return cv2.estimateAffinePartial2D(\n",
        "            np.vstack([landmarks[x] for x in stable_points]),\n",
        "            stable_reference,\n",
        "            method=cv2.LMEDS,\n",
        "        )[0]\n",
        "\n",
        "    def apply_affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        transform,\n",
        "        target_size,\n",
        "        interpolation,\n",
        "        border_mode,\n",
        "        border_value,\n",
        "    ):\n",
        "        transformed_frame = cv2.warpAffine(\n",
        "            frame,\n",
        "            transform,\n",
        "            dsize=(target_size[0], target_size[1]),\n",
        "            flags=interpolation,\n",
        "            borderMode=border_mode,\n",
        "            borderValue=border_value,\n",
        "        )\n",
        "        transformed_landmarks = (\n",
        "            np.matmul(landmarks, transform[:, :2].transpose())\n",
        "            + transform[:, 2].transpose()\n",
        "        )\n",
        "        return transformed_frame, transformed_landmarks"
      ],
      "metadata": {
        "id": "DAVsQMOLWsGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AVSRDataLoader"
      ],
      "metadata": {
        "id": "zAvtziY3UFbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AVSRDataLoader:\n",
        "    def __init__(self, modality, detector=\"retinaface\", convert_gray=True):\n",
        "        self.modality = modality\n",
        "        if modality == \"video\":\n",
        "            # if detector == \"retinaface\":\n",
        "            #     from detectors.retinaface.detector import LandmarksDetector\n",
        "            #     from detectors.retinaface.video_process import VideoProcess\n",
        "            #     self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
        "            #     self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "\n",
        "            # if detector == \"mediapipe\":\n",
        "            #     from detectors.mediapipe.detector import LandmarksDetector\n",
        "            #     from detectors.mediapipe.video_process import VideoProcess\n",
        "            #     self.landmarks_detector = LandmarksDetector()\n",
        "            #     self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "            self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
        "            self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "\n",
        "    def load_data(self, data_filename, landmarks=None, transform=True):\n",
        "        if self.modality == \"audio\":\n",
        "            audio, sample_rate = self.load_audio(data_filename)\n",
        "            audio = self.audio_process(audio, sample_rate)\n",
        "            return audio\n",
        "        if self.modality == \"video\":\n",
        "            video = self.load_video(data_filename)\n",
        "            if not landmarks:\n",
        "                landmarks = self.landmarks_detector(video)\n",
        "            video = self.video_process(video, landmarks)\n",
        "            if video is None:\n",
        "                raise TypeError(\"video cannot be None\")\n",
        "            video = torch.tensor(video)\n",
        "            return video\n",
        "\n",
        "    def load_audio(self, data_filename):\n",
        "        waveform, sample_rate = torchaudio.load(data_filename, normalize=True)\n",
        "        return waveform, sample_rate\n",
        "\n",
        "    def load_video(self, data_filename):\n",
        "        return torchvision.io.read_video(data_filename, pts_unit=\"sec\")[0].numpy()\n",
        "\n",
        "    def audio_process(self, waveform, sample_rate, target_sample_rate=16000):\n",
        "        if sample_rate != target_sample_rate:\n",
        "            waveform = torchaudio.functional.resample(\n",
        "                waveform, sample_rate, target_sample_rate\n",
        "            )\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        return waveform"
      ],
      "metadata": {
        "id": "8jQdMjnlNd9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util"
      ],
      "metadata": {
        "id": "CiL6IrYoYdOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_file(filename, max_frames=600, fps=25.0):\n",
        "\n",
        "    lines = open(filename).read().splitlines()\n",
        "\n",
        "    flag = 0\n",
        "    stack = []\n",
        "    res = []\n",
        "\n",
        "    tmp = 0\n",
        "    start_timestamp = 0.0\n",
        "\n",
        "    threshold = max_frames / fps\n",
        "\n",
        "    for line in lines:\n",
        "        if \"WORD START END ASDSCORE\" in line:\n",
        "            flag = 1\n",
        "            continue\n",
        "        if flag:\n",
        "            word, start, end, score = line.split(\" \")\n",
        "            start, end, score = float(start), float(end), float(score)\n",
        "            if end < tmp + threshold:\n",
        "                stack.append(word)\n",
        "                last_timestamp = end\n",
        "            else:\n",
        "                res.append(\n",
        "                    [\n",
        "                        \" \".join(stack),\n",
        "                        start_timestamp,\n",
        "                        last_timestamp,\n",
        "                        last_timestamp - start_timestamp,\n",
        "                    ]\n",
        "                )\n",
        "                tmp = start\n",
        "                start_timestamp = start\n",
        "                stack = [word]\n",
        "    if stack:\n",
        "        res.append([\" \".join(stack), start_timestamp, end, end - start_timestamp])\n",
        "    return res\n",
        "\n",
        "\n",
        "def save_vid_txt(\n",
        "    dst_vid_filename, dst_txt_filename, trim_video_data, content, video_fps=25\n",
        "):\n",
        "    # -- save video\n",
        "    save2vid(dst_vid_filename, trim_video_data, video_fps)\n",
        "    # -- save text\n",
        "    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)\n",
        "    f = open(dst_txt_filename, \"w\", encoding=\"utf-8\")\n",
        "    f.write(f\"{content}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def save_vid_aud(\n",
        "    dst_vid_filename,\n",
        "    dst_aud_filename,\n",
        "    trim_vid_data,\n",
        "    trim_aud_data,\n",
        "    video_fps=25,\n",
        "    audio_sample_rate=16000,\n",
        "):\n",
        "    # -- save video\n",
        "    save2vid(dst_vid_filename, trim_vid_data, video_fps)\n",
        "    # -- save audio\n",
        "    save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)\n",
        "\n",
        "\n",
        "def save_vid_aud_txt(\n",
        "    dst_vid_filename,\n",
        "    dst_aud_filename,\n",
        "    dst_txt_filename,\n",
        "    trim_vid_data,\n",
        "    trim_aud_data,\n",
        "    content,\n",
        "    video_fps=25,\n",
        "    audio_sample_rate=16000,\n",
        "):\n",
        "    # -- save video\n",
        "    if dst_vid_filename is not None:\n",
        "        save2vid(dst_vid_filename, trim_vid_data, video_fps)\n",
        "    # -- save audio\n",
        "    if dst_aud_filename is not None:\n",
        "        save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)\n",
        "    # -- save text\n",
        "    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)\n",
        "    f = open(dst_txt_filename, \"w\")\n",
        "    f.write(f\"{content}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def save2vid(filename, vid, frames_per_second):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchvision.io.write_video(filename, vid, frames_per_second)\n",
        "\n",
        "\n",
        "def save2aud(filename, aud, sample_rate):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchaudio.save(filename, aud, sample_rate)"
      ],
      "metadata": {
        "id": "zA3biEp1UXj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SJuNFb_Ycq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper"
      ],
      "metadata": {
        "id": "Wq50mSe0jOLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfH5SDeijQTL",
        "outputId": "96783f66-4aaf-40e1-91d5-908f2b84f164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-8eet18pj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-8eet18pj\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=3532ba8869fdd3291a62a26eabf9cd5fdc6fe7edbd384048fada8675c90583c2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zng2tkey/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from moviepy.editor import VideoFileClip"
      ],
      "metadata": {
        "id": "YQNyCAtujbq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_whisper = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "id": "8w4WWs7okcK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769a3fee-279c-4651-a87a-b6c853722d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:12<00:00, 123MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_texto(ruta, name):\n",
        "    video = VideoFileClip(ruta)\n",
        "    video.audio.write_audiofile(f'{name}.mp3')\n",
        "    while True:\n",
        "        result = model_whisper.transcribe(f'{name}.mp3')\n",
        "        temperature = result['segments'][0]['temperature']\n",
        "        compression_ratio = result['segments'][0]['compression_ratio']\n",
        "        if temperature == 0.0 and compression_ratio > 1.2:\n",
        "            text = result['text']\n",
        "            break\n",
        "        elif temperature != 1.0 and temperature > 0.2 and compression_ratio > 1.2:\n",
        "            text = result['text']\n",
        "            break\n",
        "    text = text[1:]\n",
        "    print(text)\n",
        "    print(f'temperature={temperature}')\n",
        "    print(f'compression ratio={compression_ratio}')\n",
        "    text = text.replace(',','')\n",
        "    text = text.replace('.','')\n",
        "    text = text.replace('¿','')\n",
        "    text = text.replace('?','')\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# def obtener_texto(ruta, audio):\n",
        "#     video = VideoFileClip(ruta)\n",
        "#     video.audio.write_audiofile(f'{audio}.mp3')\n",
        "#     # load audio and pad/trim it to fit 30 seconds\n",
        "#     audio = whisper.load_audio(f'{audio}.mp3')\n",
        "#     audio = whisper.pad_or_trim(audio)\n",
        "#     # make log-Mel spectrogram and move to the same device as the model\n",
        "#     mel = whisper.log_mel_spectrogram(audio).to(model_whisper.device)\n",
        "#     # detect the spoken language\n",
        "#     # _, probs = model.detect_language(mel)\n",
        "#     # print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "#     # decode the audio\n",
        "#     options = whisper.DecodingOptions()\n",
        "#     result = whisper.decode(model_whisper, mel, options)\n",
        "#     # recognized text\n",
        "#     return result.text"
      ],
      "metadata": {
        "id": "BHh8cPc_jumK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generacion de video procesado, y obtencion de texto"
      ],
      "metadata": {
        "id": "yPwPvqS9Y2Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_videos = '/content/drive/MyDrive/vsr/VPHB_USFX/videos'\n",
        "ruta_dataset = '/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing'\n",
        "list_videos_fa = os.listdir(ruta_videos)\n",
        "list_videos_pre = os.listdir(f'{ruta_dataset}/data/video')\n",
        "list_videos = [v for v in list_videos_fa if v not in list_videos_pre]\n",
        "\n",
        "video_dataloader = AVSRDataLoader(modality=\"video\", detector=\"retinaface\", convert_gray=False)\n",
        "\n",
        "for data_filename in tqdm(list_videos[0:2]):\n",
        "    print(data_filename)\n",
        "    name = data_filename.split('.')[0]\n",
        "    ruta = f'{ruta_videos}/{data_filename}'\n",
        "    # Obtener video y texto\n",
        "    text_data = obtener_texto(ruta, name)\n",
        "    print(text_data)\n",
        "    video_data = video_dataloader.load_data(ruta)\n",
        "    # Guardar texto y video\n",
        "    output_video_path = f'{ruta_dataset}/data/video/{name}.mp4'\n",
        "    output_text_path = f'{ruta_dataset}/data/text/{name}.txt'\n",
        "    save_vid_txt(output_video_path, output_text_path,\n",
        "                 video_data, text_data)\n",
        "    print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "PrOUrv8lY4RF",
        "outputId": "c03435f4-3c83-4ae6-90b6-6b6b8e941ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IvveiO9WMp4_V1-0003.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'obtener_texto' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-5d0718d2b689>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mruta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{ruta_videos}/{data_filename}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Obtener video y texto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobtener_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mvideo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'obtener_texto' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ly65uzLSD_rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5jVbsPynfIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextTransform"
      ],
      "metadata": {
        "id": "2Vf5mUY66pRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece"
      ],
      "metadata": {
        "id": "n_Mh7W8j66-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SP_MODEL_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"unigram\",\n",
        "    \"unigram40.model\",\n",
        ")\n",
        "\n",
        "DICT_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"unigram\",\n",
        "    \"unigram40_units.txt\",\n",
        ")\n",
        "\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Mapping Dictionary Class for SentencePiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sp_model_path=SP_MODEL_PATH,\n",
        "        dict_path=DICT_PATH,\n",
        "    ):\n",
        "\n",
        "        # Load SentencePiece model\n",
        "        self.spm = sentencepiece.SentencePieceProcessor(model_file=sp_model_path)\n",
        "\n",
        "        # Load units and create dictionary\n",
        "        units = open(dict_path, encoding='utf8').read().splitlines()\n",
        "        self.hashmap = {unit.split()[0]: unit.split()[-1] for unit in units}\n",
        "        # 0 will be used for \"blank\" in CTC\n",
        "        self.token_list = [\"<blank>\"] + list(self.hashmap.keys()) + [\"<eos>\"]\n",
        "        self.ignore_id = -1\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = self.spm.EncodeAsPieces(text)\n",
        "        token_ids = [self.hashmap.get(token, self.hashmap[\"<unk>\"]) for token in tokens]\n",
        "        return torch.tensor(list(map(int, token_ids)))\n",
        "\n",
        "    def post_process(self, token_ids):\n",
        "        token_ids = token_ids[token_ids != -1]\n",
        "        text = self._ids_to_str(token_ids, self.token_list)\n",
        "        text = text.replace(\"\\u2581\", \" \").strip()\n",
        "        return text\n",
        "\n",
        "    def _ids_to_str(self, token_ids, char_list):\n",
        "        token_as_list = [char_list[idx] for idx in token_ids]\n",
        "        return \"\".join(token_as_list).replace(\"<space>\", \" \")"
      ],
      "metadata": {
        "id": "U2pjDXiA6o0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer y generacion de csv"
      ],
      "metadata": {
        "id": "bu4mQFgBzwGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_transform = TextTransform()"
      ],
      "metadata": {
        "id": "z2lOzc6P6Xio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = \"/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing\""
      ],
      "metadata": {
        "id": "c0EKk85mnfFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_video_text = [[f'{ruta}/data/video/{name}.mp4',f'{ruta}/data/text/{name}.txt'] for name in [n.split('.')[0] for n in os.listdir(f'{ruta}/data/video')]]\n",
        "list_csv = []\n",
        "for r_vid, r_text in tqdm(list_video_text):\n",
        "    video = torchvision.io.read_video(r_vid, pts_unit=\"sec\")[0].numpy()\n",
        "    text = open(r_text, 'r').read()\n",
        "    token_id_str = \" \".join(\n",
        "            map(str, [_.item() for _ in text_transform.tokenize(text)])\n",
        "    )\n",
        "    name_video = r_vid.split('/')[-1]\n",
        "    list_csv.append(['VPHBUSFX', f'video/{name_video}', video.shape[0], token_id_str])\n",
        "df_csv = pd.DataFrame(list_csv, columns=['Dataset','ruta','input_length','token_id'])\n",
        "df_csv.to_csv(f'{ruta}/labels/VPHBUSFX.csv')\n",
        "df_csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yl3NGeGd0_Ev",
        "outputId": "53fe8471-a9b1-4c12-bdf7-f7efeddbceaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:13<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Dataset                           ruta  input_length  \\\n",
              "0   VPHBUSFX  video/4yquiL-r1J0_V1-0001.mp4           298   \n",
              "1   VPHBUSFX  video/4yquiL-r1J0_V1-0002.mp4           280   \n",
              "2   VPHBUSFX  video/4yquiL-r1J0_V1-0003.mp4           360   \n",
              "3   VPHBUSFX  video/4yquiL-r1J0_V1-0004.mp4           241   \n",
              "4   VPHBUSFX  video/4yquiL-r1J0_V1-0005.mp4           419   \n",
              "5   VPHBUSFX  video/4yquiL-r1J0_V1-0006.mp4           419   \n",
              "6   VPHBUSFX  video/4yquiL-r1J0_V1-0007.mp4           405   \n",
              "7   VPHBUSFX  video/4yquiL-r1J0_V1-0008.mp4           333   \n",
              "8   VPHBUSFX  video/4yquiL-r1J0_V1-0009.mp4           346   \n",
              "9   VPHBUSFX  video/4yquiL-r1J0_V1-0010.mp4           220   \n",
              "10  VPHBUSFX  video/4yquiL-r1J0_V1-0011.mp4           267   \n",
              "11  VPHBUSFX  video/4yquiL-r1J0_V1-0012.mp4           295   \n",
              "12  VPHBUSFX  video/4yquiL-r1J0_V1-0013.mp4           490   \n",
              "13  VPHBUSFX  video/4yquiL-r1J0_V1-0014.mp4           214   \n",
              "14  VPHBUSFX  video/4yquiL-r1J0_V1-0015.mp4           653   \n",
              "15  VPHBUSFX  video/4yquiL-r1J0_V1-0016.mp4           466   \n",
              "16  VPHBUSFX  video/4yquiL-r1J0_V1-0017.mp4           371   \n",
              "17  VPHBUSFX  video/4yquiL-r1J0_V1-0018.mp4           463   \n",
              "18  VPHBUSFX  video/4yquiL-r1J0_V1-0019.mp4           424   \n",
              "19  VPHBUSFX  video/4yquiL-r1J0_V1-0020.mp4           387   \n",
              "20  VPHBUSFX  video/4yquiL-r1J0_V1-0021.mp4           310   \n",
              "21  VPHBUSFX  video/4yquiL-r1J0_V1-0022.mp4           361   \n",
              "22  VPHBUSFX  video/4yquiL-r1J0_V1-0023.mp4           586   \n",
              "23  VPHBUSFX  video/4yquiL-r1J0_V1-0024.mp4           470   \n",
              "24  VPHBUSFX  video/4yquiL-r1J0_V1-0025.mp4           278   \n",
              "25  VPHBUSFX  video/4yquiL-r1J0_V1-0026.mp4           474   \n",
              "26  VPHBUSFX  video/4yquiL-r1J0_V1-0027.mp4           437   \n",
              "27  VPHBUSFX  video/4yquiL-r1J0_V1-0028.mp4           421   \n",
              "28  VPHBUSFX  video/4yquiL-r1J0_V1-0029.mp4           448   \n",
              "29  VPHBUSFX  video/4yquiL-r1J0_V1-0030.mp4           309   \n",
              "30  VPHBUSFX  video/4yquiL-r1J0_V1-0031.mp4           364   \n",
              "31  VPHBUSFX  video/4yquiL-r1J0_V1-0032.mp4           670   \n",
              "32  VPHBUSFX  video/4yquiL-r1J0_V1-0033.mp4           420   \n",
              "33  VPHBUSFX  video/4yquiL-r1J0_V1-0034.mp4           316   \n",
              "34  VPHBUSFX  video/4yquiL-r1J0_V1-0035.mp4           447   \n",
              "35  VPHBUSFX  video/4yquiL-r1J0_V1-0036.mp4           439   \n",
              "36  VPHBUSFX  video/4yquiL-r1J0_V1-0037.mp4           434   \n",
              "37  VPHBUSFX  video/ilYqM94AdmY_V1-0001.mp4           183   \n",
              "38  VPHBUSFX  video/ilYqM94AdmY_V1-0002.mp4           192   \n",
              "39  VPHBUSFX  video/ilYqM94AdmY_V1-0003.mp4           234   \n",
              "40  VPHBUSFX  video/IvveiO9WMp4_V1-0001.mp4           485   \n",
              "41  VPHBUSFX  video/IvveiO9WMp4_V1-0002.mp4           170   \n",
              "\n",
              "                                             token_id  \n",
              "0   2 18 27 23 9 2 30 35 15 2 34 9 23 2 11 19 14 2...  \n",
              "1   2 11 27 23 19 37 19 9 2 14 23 2 14 38 2 24 9 2...  \n",
              "2   2 34 27 13 27 33 2 9 12 10 2 14 33 34 10 25 2 ...  \n",
              "3   2 27 2 14 33 14 2 24 9 23 14 33 34 9 32 2 14 3...  \n",
              "4   2 25 27 33 27 34 32 27 33 2 9 12 10 2 25 27 33...  \n",
              "5   2 33 27 12 19 9 23 2 12 35 10 25 34 27 33 2 37...  \n",
              "6   2 39 9 2 25 27 2 18 9 39 2 17 32 9 12 19 9 33 ...  \n",
              "7   2 32 14 29 23 9 25 34 14 9 32 2 35 25 9 2 25 3...  \n",
              "8   2 23 9 2 9 12 34 19 34 35 13 31 2 14 33 34 10 ...  \n",
              "9   2 33 35 2 17 14 33 34 19 28 25 2 13 14 33 13 1...  \n",
              "10  2 33 19 2 14 33 2 18 9 11 19 23 19 34 9 13 27 ...  \n",
              "11  2 13 14 12 19 32 2 14 25 31 2 35 25 9 2 17 14 ...  \n",
              "12  2 14 33 34 27 2 24 10 33 2 29 9 33 9 2 29 27 3...  \n",
              "13  2 14 33 34 10 25 2 23 9 33 2 24 9 25 27 33 2 1...  \n",
              "14  2 14 23 2 29 27 13 14 32 2 29 27 23 20 34 19 1...  \n",
              "15  2 33 27 12 19 9 23 19 33 24 27 2 9 33 20 2 12 ...  \n",
              "16  2 39 2 33 27 24 27 33 2 32 14 29 32 14 33 14 2...  \n",
              "17  2 14 25 2 14 33 14 2 24 27 24 14 25 34 27 2 29...  \n",
              "18  2 34 9 24 11 19 15 25 2 18 9 39 2 24 9 12 18 1...  \n",
              "19  2 12 27 25 2 16 14 23 19 29 14 2 22 19 33 29 1...  \n",
              "20  31 2 14 33 2 25 27 32 24 9 23 31 32 14 32 2 33...  \n",
              "21  2 33 19 2 14 25 2 9 23 17 36 25 2 24 27 24 14 ...  \n",
              "22  2 14 23 2 12 35 23 34 27 2 13 14 2 23 9 2 29 1...  \n",
              "23  2 37 27 34 9 13 9 33 2 39 27 2 17 9 25 27 2 12...  \n",
              "24  2 29 9 20 33 2 29 27 32 2 23 9 2 29 14 32 33 1...  \n",
              "25  2 14 33 34 32 35 12 34 35 32 9 23 14 33 2 24 1...  \n",
              "26  2 33 9 11 19 14 25 13 27 31 2 29 27 13 20 9 2 ...  \n",
              "27  2 13 14 2 23 9 2 12 19 35 13 9 13 2 13 14 2 24...  \n",
              "28  2 14 23 2 29 27 23 20 34 19 12 27 2 24 19 32 9...  \n",
              "29  2 13 14 2 24 35 21 14 32 14 33 2 14 33 29 14 3...  \n",
              "30  2 33 19 2 11 19 14 25 2 33 27 39 2 35 25 9 2 1...  \n",
              "31  2 39 2 34 9 24 11 19 15 25 2 35 25 9 2 13 14 2...  \n",
              "32  2 35 25 2 12 9 24 11 19 27 2 14 33 34 32 35 12...  \n",
              "33  31 2 34 14 25 20 9 24 27 33 2 12 35 9 25 13 27...  \n",
              "34  2 16 27 32 34 9 23 14 12 14 32 2 24 35 12 18 2...  \n",
              "35  2 39 2 37 27 23 37 14 32 2 9 2 34 14 25 14 32 ...  \n",
              "36  2 39 2 23 27 31 2 14 33 34 10 2 18 9 12 19 14 ...  \n",
              "37  2 14 33 2 24 35 39 2 19 24 29 27 32 34 9 25 34...  \n",
              "38  2 23 27 33 2 13 14 29 27 32 34 19 33 34 9 33 2...  \n",
              "39  2 25 27 2 18 9 39 31 2 9 11 32 19 32 2 14 23 2...  \n",
              "40  2 14 25 2 23 9 2 16 19 16 9 2 33 14 2 19 32 20...  \n",
              "41  2 13 27 25 13 14 2 23 9 2 17 14 25 34 14 2 23 ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ad11743-8e52-45d8-a780-070a5ca3eae3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>ruta</th>\n",
              "      <th>input_length</th>\n",
              "      <th>token_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0001.mp4</td>\n",
              "      <td>298</td>\n",
              "      <td>2 18 27 23 9 2 30 35 15 2 34 9 23 2 11 19 14 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0002.mp4</td>\n",
              "      <td>280</td>\n",
              "      <td>2 11 27 23 19 37 19 9 2 14 23 2 14 38 2 24 9 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0003.mp4</td>\n",
              "      <td>360</td>\n",
              "      <td>2 34 27 13 27 33 2 9 12 10 2 14 33 34 10 25 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0004.mp4</td>\n",
              "      <td>241</td>\n",
              "      <td>2 27 2 14 33 14 2 24 9 23 14 33 34 9 32 2 14 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0005.mp4</td>\n",
              "      <td>419</td>\n",
              "      <td>2 25 27 33 27 34 32 27 33 2 9 12 10 2 25 27 33...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0006.mp4</td>\n",
              "      <td>419</td>\n",
              "      <td>2 33 27 12 19 9 23 2 12 35 10 25 34 27 33 2 37...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0007.mp4</td>\n",
              "      <td>405</td>\n",
              "      <td>2 39 9 2 25 27 2 18 9 39 2 17 32 9 12 19 9 33 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0008.mp4</td>\n",
              "      <td>333</td>\n",
              "      <td>2 32 14 29 23 9 25 34 14 9 32 2 35 25 9 2 25 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0009.mp4</td>\n",
              "      <td>346</td>\n",
              "      <td>2 23 9 2 9 12 34 19 34 35 13 31 2 14 33 34 10 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0010.mp4</td>\n",
              "      <td>220</td>\n",
              "      <td>2 33 35 2 17 14 33 34 19 28 25 2 13 14 33 13 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0011.mp4</td>\n",
              "      <td>267</td>\n",
              "      <td>2 33 19 2 14 33 2 18 9 11 19 23 19 34 9 13 27 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0012.mp4</td>\n",
              "      <td>295</td>\n",
              "      <td>2 13 14 12 19 32 2 14 25 31 2 35 25 9 2 17 14 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0013.mp4</td>\n",
              "      <td>490</td>\n",
              "      <td>2 14 33 34 27 2 24 10 33 2 29 9 33 9 2 29 27 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0014.mp4</td>\n",
              "      <td>214</td>\n",
              "      <td>2 14 33 34 10 25 2 23 9 33 2 24 9 25 27 33 2 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0015.mp4</td>\n",
              "      <td>653</td>\n",
              "      <td>2 14 23 2 29 27 13 14 32 2 29 27 23 20 34 19 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0016.mp4</td>\n",
              "      <td>466</td>\n",
              "      <td>2 33 27 12 19 9 23 19 33 24 27 2 9 33 20 2 12 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0017.mp4</td>\n",
              "      <td>371</td>\n",
              "      <td>2 39 2 33 27 24 27 33 2 32 14 29 32 14 33 14 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0018.mp4</td>\n",
              "      <td>463</td>\n",
              "      <td>2 14 25 2 14 33 14 2 24 27 24 14 25 34 27 2 29...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0019.mp4</td>\n",
              "      <td>424</td>\n",
              "      <td>2 34 9 24 11 19 15 25 2 18 9 39 2 24 9 12 18 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0020.mp4</td>\n",
              "      <td>387</td>\n",
              "      <td>2 12 27 25 2 16 14 23 19 29 14 2 22 19 33 29 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0021.mp4</td>\n",
              "      <td>310</td>\n",
              "      <td>31 2 14 33 2 25 27 32 24 9 23 31 32 14 32 2 33...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0022.mp4</td>\n",
              "      <td>361</td>\n",
              "      <td>2 33 19 2 14 25 2 9 23 17 36 25 2 24 27 24 14 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0023.mp4</td>\n",
              "      <td>586</td>\n",
              "      <td>2 14 23 2 12 35 23 34 27 2 13 14 2 23 9 2 29 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0024.mp4</td>\n",
              "      <td>470</td>\n",
              "      <td>2 37 27 34 9 13 9 33 2 39 27 2 17 9 25 27 2 12...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0025.mp4</td>\n",
              "      <td>278</td>\n",
              "      <td>2 29 9 20 33 2 29 27 32 2 23 9 2 29 14 32 33 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0026.mp4</td>\n",
              "      <td>474</td>\n",
              "      <td>2 14 33 34 32 35 12 34 35 32 9 23 14 33 2 24 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0027.mp4</td>\n",
              "      <td>437</td>\n",
              "      <td>2 33 9 11 19 14 25 13 27 31 2 29 27 13 20 9 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0028.mp4</td>\n",
              "      <td>421</td>\n",
              "      <td>2 13 14 2 23 9 2 12 19 35 13 9 13 2 13 14 2 24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0029.mp4</td>\n",
              "      <td>448</td>\n",
              "      <td>2 14 23 2 29 27 23 20 34 19 12 27 2 24 19 32 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0030.mp4</td>\n",
              "      <td>309</td>\n",
              "      <td>2 13 14 2 24 35 21 14 32 14 33 2 14 33 29 14 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0031.mp4</td>\n",
              "      <td>364</td>\n",
              "      <td>2 33 19 2 11 19 14 25 2 33 27 39 2 35 25 9 2 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0032.mp4</td>\n",
              "      <td>670</td>\n",
              "      <td>2 39 2 34 9 24 11 19 15 25 2 35 25 9 2 13 14 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0033.mp4</td>\n",
              "      <td>420</td>\n",
              "      <td>2 35 25 2 12 9 24 11 19 27 2 14 33 34 32 35 12...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0034.mp4</td>\n",
              "      <td>316</td>\n",
              "      <td>31 2 34 14 25 20 9 24 27 33 2 12 35 9 25 13 27...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0035.mp4</td>\n",
              "      <td>447</td>\n",
              "      <td>2 16 27 32 34 9 23 14 12 14 32 2 24 35 12 18 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0036.mp4</td>\n",
              "      <td>439</td>\n",
              "      <td>2 39 2 37 27 23 37 14 32 2 9 2 34 14 25 14 32 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/4yquiL-r1J0_V1-0037.mp4</td>\n",
              "      <td>434</td>\n",
              "      <td>2 39 2 23 27 31 2 14 33 34 10 2 18 9 12 19 14 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/ilYqM94AdmY_V1-0001.mp4</td>\n",
              "      <td>183</td>\n",
              "      <td>2 14 33 2 24 35 39 2 19 24 29 27 32 34 9 25 34...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/ilYqM94AdmY_V1-0002.mp4</td>\n",
              "      <td>192</td>\n",
              "      <td>2 23 27 33 2 13 14 29 27 32 34 19 33 34 9 33 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/ilYqM94AdmY_V1-0003.mp4</td>\n",
              "      <td>234</td>\n",
              "      <td>2 25 27 2 18 9 39 31 2 9 11 32 19 32 2 14 23 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/IvveiO9WMp4_V1-0001.mp4</td>\n",
              "      <td>485</td>\n",
              "      <td>2 14 25 2 23 9 2 16 19 16 9 2 33 14 2 19 32 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/IvveiO9WMp4_V1-0002.mp4</td>\n",
              "      <td>170</td>\n",
              "      <td>2 13 27 25 13 14 2 23 9 2 17 14 25 34 14 2 23 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ad11743-8e52-45d8-a780-070a5ca3eae3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6ad11743-8e52-45d8-a780-070a5ca3eae3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6ad11743-8e52-45d8-a780-070a5ca3eae3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-17a3196a-bd0a-47bf-a285-9f2d1de1c920\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-17a3196a-bd0a-47bf-a285-9f2d1de1c920')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-17a3196a-bd0a-47bf-a285-9f2d1de1c920 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7a63c4d0-6f2f-4aa7-8cad-20ad712eeb32\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_csv')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7a63c4d0-6f2f-4aa7-8cad-20ad712eeb32 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_csv');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_csv",
              "summary": "{\n  \"name\": \"df_csv\",\n  \"rows\": 42,\n  \"fields\": [\n    {\n      \"column\": \"Dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"VPHBUSFX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ruta\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"video/4yquiL-r1J0_V1-0026.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 116,\n        \"min\": 170,\n        \"max\": 670,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          474\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"2 14 33 34 32 35 12 34 35 32 9 23 14 33 2 24 19 32 9 2 39 27 2 14 33 34 27 39 2 24 35 39 2 9 17 32 9 13 14 12 19 13 9 2 12 27 25 2 24 19 2 12 19 35 13 9 13 2 29 27 32 30 35 14 2 14 25 2 14 23 2 24 27 24 14 25 34 27 2 14 25 2 14 23 31 2 15 23 2 24 10 33 2 24 14 2 18 9 2 13 9 13 27 2 23 9 2 14 33 29 9 23 13 9 2 39 2 24 14 2 18 14 2 33 14 25 34 19 13 27 2 33 27 23 9 2 39 27 2 24 14 2 18 14 2 29 32 14 33 14 25 34 9 13 27 2 12 27 24 27 2 33 14 32 2 9 23 12 9 23 13 14 33 9 2 29 27 32 2 13 19 17 25 19 13 9 13 2 29 27 32 30 35 14 2 25 27 2 37 27 39 2 9 2 29 14 32 24 19 34 19 32 31 2 25 9 13 19 14 2 24 14 2 37 14 25 17 9 2 9 2 13 14 12 19 32 2 30 35 15 2 14 33 2 23 27 31 2 34 14 25 17 27 31 2 18 9 12 14 32 2 27 2 23 27 31 2 25 27 2 34 14 25 17 27 31 2 18 9 12 14 32 2 33 9 11 19 14 25 13 27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQ3u5E8X992V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtener train, val"
      ],
      "metadata": {
        "id": "E3pA7PK_-z6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.head(20).to_csv(f'{ruta}/labels/VPHBUSFX_train.csv')\n",
        "df_csv.tail(20).to_csv(f'{ruta}/labels/VPHBUSFX_val.csv')"
      ],
      "metadata": {
        "id": "-zu5jv8E99-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0dxNRb6-3G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmKRDsSp-2_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RN8l2ftSeKiq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}