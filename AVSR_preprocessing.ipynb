{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgCs7dGcNVGc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ETCcaQGNNWp",
        "outputId": "5197c1a3-8f4b-4767-efb9-b0c59a2705f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting av\n",
            "  Downloading av-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg-python\n",
        "!pip install av"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZlrG64BUNYe"
      },
      "source": [
        "# Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xREsj1urUPos"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchvision\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import transform as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jLgxJNjUvK0"
      },
      "source": [
        "# MediaPipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iuJF2_VXmWh"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkGKjc61XbJJ"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TAzoxo8XVxj"
      },
      "outputs": [],
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self):\n",
        "        self.mp_face_detection = mp.solutions.face_detection\n",
        "        self.short_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=0)\n",
        "        self.full_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=1)\n",
        "\n",
        "    def __call__(self, video_frames):\n",
        "        landmarks = self.detect(video_frames, self.full_range_detector)\n",
        "        if all(element is None for element in landmarks):\n",
        "            landmarks = self.detect(video_frames, self.short_range_detector)\n",
        "            assert any(l is not None for l in landmarks), \"Cannot detect any frames in the video\"\n",
        "        return landmarks\n",
        "\n",
        "    def detect(self, video_frames, detector):\n",
        "        landmarks = []\n",
        "        for frame in video_frames:\n",
        "            results = detector.process(frame)\n",
        "            if not results.detections:\n",
        "                landmarks.append(None)\n",
        "                continue\n",
        "            face_points = []\n",
        "            for idx, detected_faces in enumerate(results.detections):\n",
        "                max_id, max_size = 0, 0\n",
        "                bboxC = detected_faces.location_data.relative_bounding_box\n",
        "                ih, iw, ic = frame.shape\n",
        "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
        "                bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "                if bbox_size > max_size:\n",
        "                    max_id, max_size = idx, bbox_size\n",
        "                lmx = [\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].y * ih)],\n",
        "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].x * iw),\n",
        "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].y * ih)],\n",
        "                    ]\n",
        "                face_points.append(lmx)\n",
        "            landmarks.append(np.array(face_points[max_id]))\n",
        "        return landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6ahhX7gXohu"
      },
      "outputs": [],
      "source": [
        "reference = np.array([[ 70.92383848,  97.13757949],[ 72.62515288, 114.90361188],\n",
        "  [ 75.98941827, 131.07402473],[ 79.26959318, 146.2116596 ],[ 83.61348894, 163.26020701],\n",
        "  [ 91.33134186, 177.44535288],[100.27169245, 187.0885567 ],[112.12435016, 196.00353535],\n",
        "  [130.74175801, 200.52998862],[149.8553027 , 195.31198065],[163.101687  , 186.44060881],\n",
        "  [173.65334172, 176.57250158],[182.43614459, 162.27992572],[187.16738556, 145.09391978],\n",
        "  [190.22905333, 129.72418731],[193.02118502, 113.45923358],[194.43863372,  95.5795392 ],\n",
        "  [ 81.33095967,  80.79544511],[ 87.75906556,  75.27980275],[ 96.22692544,  73.83857497],\n",
        "  [104.55524335,  74.74029382],[112.23186144,  76.97670954],[144.49576387,  76.42387471],\n",
        "  [152.34799901,  73.83329748],[161.13054079,  72.63570385],[170.58715674,  73.84785054],\n",
        "  [178.21409885,  79.43802857],[128.7337425 ,  95.35962566],[128.48854473, 106.92459506],\n",
        "  [128.24475936, 118.27285086],[128.26596547, 127.69870727],[118.76000113, 135.19357677],\n",
        "  [122.96307973, 136.14619774],[128.87017961, 137.30253356],[134.9428314 , 135.99720543],\n",
        "  [139.48259748, 134.87763793],[ 92.52245553,  94.36876014],[ 97.58518219,  90.95977781],\n",
        "  [105.41368273,  90.91345887],[112.77241724,  94.9436087 ],[106.103635  ,  97.08485693],\n",
        "  [ 98.04628565,  97.36335869],[145.52511509,  94.53499862],[152.58953438,  90.21485666],\n",
        "  [160.61170666,  90.19938514],[166.67710071,  93.56562296],[160.55971572,  96.48125958],\n",
        "  [152.20465993,  96.47281336],[107.16760614, 157.19606764],[114.47611216, 152.12006957],\n",
        "  [123.84852759, 148.51863199],[128.97628288, 149.41552527],[134.14360703, 148.42628211],\n",
        "  [144.17717842, 151.79343262],[152.19284005, 156.98711116],[143.85966895, 164.00347101],\n",
        "  [136.7441507 , 167.9430006 ],[129.15278278, 168.81853366],[121.79511074, 168.02271929],\n",
        "  [115.27508573, 164.15159355],[109.23088653, 157.00172103],[122.50270762, 154.40733649],\n",
        "  [129.02862236, 154.12104227],[135.83648069, 154.31214998],[150.75782809, 156.79506004],\n",
        "  [135.66204627, 160.62976732],[128.95218547, 161.28762709],[122.48775432, 160.50878431]])\n",
        "\n",
        "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
        "    start_landmarks = landmarks[start_idx]\n",
        "    stop_landmarks = landmarks[stop_idx]\n",
        "    delta = stop_landmarks - start_landmarks\n",
        "    for idx in range(1, stop_idx - start_idx):\n",
        "        landmarks[start_idx + idx] = (\n",
        "            start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
        "        )\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def warp_img(src, dst, img, std_size):\n",
        "    tform = tf.estimate_transform(\"similarity\", src, dst)\n",
        "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped, tform\n",
        "\n",
        "\n",
        "def apply_transform(transform, img, std_size):\n",
        "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped\n",
        "\n",
        "\n",
        "def cut_patch(img, landmarks, height, width, threshold=5):\n",
        "    center_x, center_y = np.mean(landmarks, axis=0)\n",
        "    # Check for too much bias in height and width\n",
        "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
        "        raise OverflowError(\"too much bias in height\")\n",
        "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
        "        raise OverflowError(\"too much bias in width\")\n",
        "    # Calculate bounding box coordinates\n",
        "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
        "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
        "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
        "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
        "    # Cut the image\n",
        "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
        "    return cutted_img\n",
        "\n",
        "\n",
        "class VideoProcess:\n",
        "    def __init__(\n",
        "        self,\n",
        "        mean_face_path=\"20words_mean_face.npy\",\n",
        "        crop_width=96,\n",
        "        crop_height=96,\n",
        "        start_idx=3,\n",
        "        stop_idx=4,\n",
        "        window_margin=12,\n",
        "        convert_gray=True,\n",
        "    ):\n",
        "        # self.reference = np.load(\n",
        "        #     os.path.join(os.path.dirname(__file__), mean_face_path)\n",
        "        # )\n",
        "        self.reference = reference\n",
        "        self.crop_width = crop_width\n",
        "        self.crop_height = crop_height\n",
        "        self.start_idx = start_idx\n",
        "        self.stop_idx = stop_idx\n",
        "        self.window_margin = window_margin\n",
        "        self.convert_gray = convert_gray\n",
        "\n",
        "    def __call__(self, video, landmarks):\n",
        "        # Pre-process landmarks: interpolate frames that are not detected\n",
        "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
        "        # Exclude corner cases: no landmark in all frames\n",
        "        if not preprocessed_landmarks:\n",
        "            return\n",
        "        # Affine transformation and crop patch\n",
        "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
        "        assert sequence is not None, \"crop an empty patch.\"\n",
        "        return sequence\n",
        "\n",
        "    def crop_patch(self, video, landmarks):\n",
        "        sequence = []\n",
        "        for frame_idx, frame in enumerate(video):\n",
        "            window_margin = min(\n",
        "                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx\n",
        "            )\n",
        "            smoothed_landmarks = np.mean(\n",
        "                [\n",
        "                    landmarks[x]\n",
        "                    for x in range(\n",
        "                        frame_idx - window_margin, frame_idx + window_margin + 1\n",
        "                    )\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "            smoothed_landmarks += landmarks[frame_idx].mean(\n",
        "                axis=0\n",
        "            ) - smoothed_landmarks.mean(axis=0)\n",
        "            transformed_frame, transformed_landmarks = self.affine_transform(\n",
        "                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray\n",
        "            )\n",
        "            patch = cut_patch(\n",
        "                transformed_frame,\n",
        "                transformed_landmarks[self.start_idx : self.stop_idx],\n",
        "                self.crop_height // 2,\n",
        "                self.crop_width // 2,\n",
        "            )\n",
        "            sequence.append(patch)\n",
        "        return np.array(sequence)\n",
        "\n",
        "    def interpolate_landmarks(self, landmarks):\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        if not valid_frames_idx:\n",
        "            return None\n",
        "\n",
        "        for idx in range(1, len(valid_frames_idx)):\n",
        "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
        "                landmarks = linear_interpolate(\n",
        "                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]\n",
        "                )\n",
        "\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
        "        if valid_frames_idx:\n",
        "            landmarks[: valid_frames_idx[0]] = [\n",
        "                landmarks[valid_frames_idx[0]]\n",
        "            ] * valid_frames_idx[0]\n",
        "            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (\n",
        "                len(landmarks) - valid_frames_idx[-1]\n",
        "            )\n",
        "\n",
        "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    def affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        reference,\n",
        "        grayscale=False,\n",
        "        target_size=(256, 256),\n",
        "        reference_size=(256, 256),\n",
        "        stable_points=(0, 1, 2, 3),\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=0,\n",
        "    ):\n",
        "        if grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        stable_reference = self.get_stable_reference(\n",
        "            reference, reference_size, target_size\n",
        "        )\n",
        "        transform = self.estimate_affine_transform(\n",
        "            landmarks, stable_points, stable_reference\n",
        "        )\n",
        "        transformed_frame, transformed_landmarks = self.apply_affine_transform(\n",
        "            frame,\n",
        "            landmarks,\n",
        "            transform,\n",
        "            target_size,\n",
        "            interpolation,\n",
        "            border_mode,\n",
        "            border_value,\n",
        "        )\n",
        "\n",
        "        return transformed_frame, transformed_landmarks\n",
        "\n",
        "    def get_stable_reference(self, reference, reference_size, target_size):\n",
        "        # -- right eye, left eye, nose tip, mouth center\n",
        "        stable_reference = np.vstack(\n",
        "            [\n",
        "                np.mean(reference[36:42], axis=0),\n",
        "                np.mean(reference[42:48], axis=0),\n",
        "                np.mean(reference[31:36], axis=0),\n",
        "                np.mean(reference[48:68], axis=0),\n",
        "            ]\n",
        "        )\n",
        "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
        "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
        "        return stable_reference\n",
        "\n",
        "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
        "        return cv2.estimateAffinePartial2D(\n",
        "            np.vstack([landmarks[x] for x in stable_points]),\n",
        "            stable_reference,\n",
        "            method=cv2.LMEDS,\n",
        "        )[0]\n",
        "\n",
        "    def apply_affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        transform,\n",
        "        target_size,\n",
        "        interpolation,\n",
        "        border_mode,\n",
        "        border_value,\n",
        "    ):\n",
        "        transformed_frame = cv2.warpAffine(\n",
        "            frame,\n",
        "            transform,\n",
        "            dsize=(target_size[0], target_size[1]),\n",
        "            flags=interpolation,\n",
        "            borderMode=border_mode,\n",
        "            borderValue=border_value,\n",
        "        )\n",
        "        transformed_landmarks = (\n",
        "            np.matmul(landmarks, transform[:, :2].transpose())\n",
        "            + transform[:, 2].transpose()\n",
        "        )\n",
        "        return transformed_frame, transformed_landmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rIk5Xf7Urtz"
      },
      "source": [
        "# RetinaFace and FAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE9lkZSTV0YI",
        "outputId": "8b666457-8e41-4b0e-a40a-b4a5df5bf33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face_alignment'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 190 (delta 27), reused 27 (delta 26), pack-reused 158\u001b[K\n",
            "Receiving objects: 100% (190/190), 213.82 MiB | 35.71 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Updating files: 100% (14/14), done.\n",
            "Cloning into 'face_detection'...\n",
            "remote: Enumerating objects: 300, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 300 (delta 41), reused 39 (delta 39), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (300/300), 81.19 MiB | 38.28 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hhj1897/face_alignment.git\n",
        "!git clone https://github.com/hhj1897/face_detection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p_IOSBvMV4ox"
      },
      "outputs": [],
      "source": [
        "from face_detection.ibug.face_detection import RetinaFacePredictor\n",
        "from face_alignment.ibug.face_alignment import FANPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u2_Tl0YFUu2N"
      },
      "outputs": [],
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self, device=\"cuda:0\", model_name=\"resnet50\"):\n",
        "        self.face_detector = RetinaFacePredictor(\n",
        "            device=device,\n",
        "            threshold=0.8,\n",
        "            model=RetinaFacePredictor.get_model(model_name),\n",
        "        )\n",
        "        self.landmark_detector = FANPredictor(device=device, model=None)\n",
        "\n",
        "    def __call__(self, video_frames):\n",
        "        landmarks = []\n",
        "        for frame in video_frames:\n",
        "            detected_faces = self.face_detector(frame, rgb=False)\n",
        "            face_points, _ = self.landmark_detector(frame, detected_faces, rgb=True)\n",
        "            if len(detected_faces) == 0:\n",
        "                landmarks.append(None)\n",
        "            else:\n",
        "                max_id, max_size = 0, 0\n",
        "                for idx, bbox in enumerate(detected_faces):\n",
        "                    bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "                    if bbox_size > max_size:\n",
        "                        max_id, max_size = idx, bbox_size\n",
        "                landmarks.append(face_points[max_id])\n",
        "        return landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DAVsQMOLWsGp"
      },
      "outputs": [],
      "source": [
        "reference = np.array([[ 70.92383848,  97.13757949],[ 72.62515288, 114.90361188],\n",
        "  [ 75.98941827, 131.07402473],[ 79.26959318, 146.2116596 ],[ 83.61348894, 163.26020701],\n",
        "  [ 91.33134186, 177.44535288],[100.27169245, 187.0885567 ],[112.12435016, 196.00353535],\n",
        "  [130.74175801, 200.52998862],[149.8553027 , 195.31198065],[163.101687  , 186.44060881],\n",
        "  [173.65334172, 176.57250158],[182.43614459, 162.27992572],[187.16738556, 145.09391978],\n",
        "  [190.22905333, 129.72418731],[193.02118502, 113.45923358],[194.43863372,  95.5795392 ],\n",
        "  [ 81.33095967,  80.79544511],[ 87.75906556,  75.27980275],[ 96.22692544,  73.83857497],\n",
        "  [104.55524335,  74.74029382],[112.23186144,  76.97670954],[144.49576387,  76.42387471],\n",
        "  [152.34799901,  73.83329748],[161.13054079,  72.63570385],[170.58715674,  73.84785054],\n",
        "  [178.21409885,  79.43802857],[128.7337425 ,  95.35962566],[128.48854473, 106.92459506],\n",
        "  [128.24475936, 118.27285086],[128.26596547, 127.69870727],[118.76000113, 135.19357677],\n",
        "  [122.96307973, 136.14619774],[128.87017961, 137.30253356],[134.9428314 , 135.99720543],\n",
        "  [139.48259748, 134.87763793],[ 92.52245553,  94.36876014],[ 97.58518219,  90.95977781],\n",
        "  [105.41368273,  90.91345887],[112.77241724,  94.9436087 ],[106.103635  ,  97.08485693],\n",
        "  [ 98.04628565,  97.36335869],[145.52511509,  94.53499862],[152.58953438,  90.21485666],\n",
        "  [160.61170666,  90.19938514],[166.67710071,  93.56562296],[160.55971572,  96.48125958],\n",
        "  [152.20465993,  96.47281336],[107.16760614, 157.19606764],[114.47611216, 152.12006957],\n",
        "  [123.84852759, 148.51863199],[128.97628288, 149.41552527],[134.14360703, 148.42628211],\n",
        "  [144.17717842, 151.79343262],[152.19284005, 156.98711116],[143.85966895, 164.00347101],\n",
        "  [136.7441507 , 167.9430006 ],[129.15278278, 168.81853366],[121.79511074, 168.02271929],\n",
        "  [115.27508573, 164.15159355],[109.23088653, 157.00172103],[122.50270762, 154.40733649],\n",
        "  [129.02862236, 154.12104227],[135.83648069, 154.31214998],[150.75782809, 156.79506004],\n",
        "  [135.66204627, 160.62976732],[128.95218547, 161.28762709],[122.48775432, 160.50878431]])\n",
        "\n",
        "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
        "    start_landmarks = landmarks[start_idx]\n",
        "    stop_landmarks = landmarks[stop_idx]\n",
        "    delta = stop_landmarks - start_landmarks\n",
        "    for idx in range(1, stop_idx - start_idx):\n",
        "        landmarks[start_idx + idx] = (\n",
        "            start_landmarks + idx / float(stop_idx - start_idx) * delta\n",
        "        )\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def warp_img(src, dst, img, std_size):\n",
        "    tform = tf.estimate_transform(\"similarity\", src, dst)\n",
        "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped, tform\n",
        "\n",
        "\n",
        "def apply_transform(transform, img, std_size):\n",
        "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
        "    warped = (warped * 255).astype(\"uint8\")\n",
        "    return warped\n",
        "\n",
        "\n",
        "def cut_patch(img, landmarks, height, width, threshold=5):\n",
        "    center_x, center_y = np.mean(landmarks, axis=0)\n",
        "    # Check for too much bias in height and width\n",
        "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
        "        raise OverflowError(\"too much bias in height\")\n",
        "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
        "        raise OverflowError(\"too much bias in width\")\n",
        "    # Calculate bounding box coordinates\n",
        "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
        "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
        "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
        "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
        "    # Cut the image\n",
        "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
        "    return cutted_img\n",
        "\n",
        "\n",
        "class VideoProcess:\n",
        "    def __init__(\n",
        "        self,\n",
        "        mean_face_path=\"20words_mean_face.npy\",\n",
        "        crop_width=96,\n",
        "        crop_height=96,\n",
        "        start_idx=48,\n",
        "        stop_idx=68,\n",
        "        window_margin=12,\n",
        "        convert_gray=True,\n",
        "    ):\n",
        "        # self.reference = np.load(\n",
        "        #     os.path.join(os.path.dirname(__file__), mean_face_path)\n",
        "        # )\n",
        "        self.reference = reference\n",
        "        self.crop_width = crop_width\n",
        "        self.crop_height = crop_height\n",
        "        self.start_idx = start_idx\n",
        "        self.stop_idx = stop_idx\n",
        "        self.window_margin = window_margin\n",
        "        self.convert_gray = convert_gray\n",
        "\n",
        "    def __call__(self, video, landmarks):\n",
        "        # Pre-process landmarks: interpolate frames that are not detected\n",
        "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
        "        # Exclude corner cases: no landmark in all frames or number of frames is less than window length\n",
        "        if (\n",
        "            not preprocessed_landmarks\n",
        "            or len(preprocessed_landmarks) < self.window_margin\n",
        "        ):\n",
        "            return\n",
        "        # Affine transformation and crop patch\n",
        "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
        "        assert sequence is not None, \"crop an empty patch.\"\n",
        "        return sequence\n",
        "\n",
        "    def crop_patch(self, video, landmarks):\n",
        "        sequence = []\n",
        "        for frame_idx, frame in enumerate(video):\n",
        "            window_margin = min(\n",
        "                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx\n",
        "            )\n",
        "            smoothed_landmarks = np.mean(\n",
        "                [\n",
        "                    landmarks[x]\n",
        "                    for x in range(\n",
        "                        frame_idx - window_margin, frame_idx + window_margin + 1\n",
        "                    )\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "            smoothed_landmarks += landmarks[frame_idx].mean(\n",
        "                axis=0\n",
        "            ) - smoothed_landmarks.mean(axis=0)\n",
        "            transformed_frame, transformed_landmarks = self.affine_transform(\n",
        "                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray\n",
        "            )\n",
        "            patch = cut_patch(\n",
        "                transformed_frame,\n",
        "                transformed_landmarks[self.start_idx : self.stop_idx],\n",
        "                self.crop_height // 2,\n",
        "                self.crop_width // 2,\n",
        "            )\n",
        "            sequence.append(patch)\n",
        "        return np.array(sequence)\n",
        "\n",
        "    def interpolate_landmarks(self, landmarks):\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        if not valid_frames_idx:\n",
        "            return None\n",
        "\n",
        "        for idx in range(1, len(valid_frames_idx)):\n",
        "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
        "                landmarks = linear_interpolate(\n",
        "                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]\n",
        "                )\n",
        "\n",
        "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
        "\n",
        "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
        "        if valid_frames_idx:\n",
        "            landmarks[: valid_frames_idx[0]] = [\n",
        "                landmarks[valid_frames_idx[0]]\n",
        "            ] * valid_frames_idx[0]\n",
        "            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (\n",
        "                len(landmarks) - valid_frames_idx[-1]\n",
        "            )\n",
        "\n",
        "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    def affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        reference,\n",
        "        grayscale=True,\n",
        "        target_size=(256, 256),\n",
        "        reference_size=(256, 256),\n",
        "        stable_points=(28, 33, 36, 39, 42, 45, 48, 54),\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=0,\n",
        "    ):\n",
        "        if grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        stable_reference = self.get_stable_reference(\n",
        "            reference, stable_points, reference_size, target_size\n",
        "        )\n",
        "        transform = self.estimate_affine_transform(\n",
        "            landmarks, stable_points, stable_reference\n",
        "        )\n",
        "        transformed_frame, transformed_landmarks = self.apply_affine_transform(\n",
        "            frame,\n",
        "            landmarks,\n",
        "            transform,\n",
        "            target_size,\n",
        "            interpolation,\n",
        "            border_mode,\n",
        "            border_value,\n",
        "        )\n",
        "\n",
        "        return transformed_frame, transformed_landmarks\n",
        "\n",
        "    def get_stable_reference(\n",
        "        self, reference, stable_points, reference_size, target_size\n",
        "    ):\n",
        "        stable_reference = np.vstack([reference[x] for x in stable_points])\n",
        "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
        "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
        "        return stable_reference\n",
        "\n",
        "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
        "        return cv2.estimateAffinePartial2D(\n",
        "            np.vstack([landmarks[x] for x in stable_points]),\n",
        "            stable_reference,\n",
        "            method=cv2.LMEDS,\n",
        "        )[0]\n",
        "\n",
        "    def apply_affine_transform(\n",
        "        self,\n",
        "        frame,\n",
        "        landmarks,\n",
        "        transform,\n",
        "        target_size,\n",
        "        interpolation,\n",
        "        border_mode,\n",
        "        border_value,\n",
        "    ):\n",
        "        transformed_frame = cv2.warpAffine(\n",
        "            frame,\n",
        "            transform,\n",
        "            dsize=(target_size[0], target_size[1]),\n",
        "            flags=interpolation,\n",
        "            borderMode=border_mode,\n",
        "            borderValue=border_value,\n",
        "        )\n",
        "        transformed_landmarks = (\n",
        "            np.matmul(landmarks, transform[:, :2].transpose())\n",
        "            + transform[:, 2].transpose()\n",
        "        )\n",
        "        return transformed_frame, transformed_landmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAvtziY3UFbe"
      },
      "source": [
        "# AVSRDataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8jQdMjnlNd9e"
      },
      "outputs": [],
      "source": [
        "class AVSRDataLoader:\n",
        "    def __init__(self, modality, detector=\"retinaface\", convert_gray=True):\n",
        "        self.modality = modality\n",
        "        if modality == \"video\":\n",
        "            # if detector == \"retinaface\":\n",
        "            #     from detectors.retinaface.detector import LandmarksDetector\n",
        "            #     from detectors.retinaface.video_process import VideoProcess\n",
        "            #     self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
        "            #     self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "\n",
        "            # if detector == \"mediapipe\":\n",
        "            #     from detectors.mediapipe.detector import LandmarksDetector\n",
        "            #     from detectors.mediapipe.video_process import VideoProcess\n",
        "            #     self.landmarks_detector = LandmarksDetector()\n",
        "            #     self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "            self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
        "            self.video_process = VideoProcess(convert_gray=convert_gray)\n",
        "\n",
        "    def load_data(self, data_filename, landmarks=None, transform=True):\n",
        "        if self.modality == \"audio\":\n",
        "            audio, sample_rate = self.load_audio(data_filename)\n",
        "            audio = self.audio_process(audio, sample_rate)\n",
        "            return audio\n",
        "        if self.modality == \"video\":\n",
        "            video = self.load_video(data_filename)\n",
        "            if not landmarks:\n",
        "                landmarks = self.landmarks_detector(video)\n",
        "            video = self.video_process(video, landmarks)\n",
        "            if video is None:\n",
        "                raise TypeError(\"video cannot be None\")\n",
        "            video = torch.tensor(video)\n",
        "            return video\n",
        "\n",
        "    def load_audio(self, data_filename):\n",
        "        waveform, sample_rate = torchaudio.load(data_filename, normalize=True)\n",
        "        return waveform, sample_rate\n",
        "\n",
        "    def load_video(self, data_filename):\n",
        "        return torchvision.io.read_video(data_filename, pts_unit=\"sec\")[0].numpy()\n",
        "\n",
        "    def audio_process(self, waveform, sample_rate, target_sample_rate=16000):\n",
        "        if sample_rate != target_sample_rate:\n",
        "            waveform = torchaudio.functional.resample(\n",
        "                waveform, sample_rate, target_sample_rate\n",
        "            )\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        return waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiL6IrYoYdOy"
      },
      "source": [
        "# Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zA3biEp1UXj2"
      },
      "outputs": [],
      "source": [
        "def split_file(filename, max_frames=600, fps=25.0):\n",
        "\n",
        "    lines = open(filename).read().splitlines()\n",
        "\n",
        "    flag = 0\n",
        "    stack = []\n",
        "    res = []\n",
        "\n",
        "    tmp = 0\n",
        "    start_timestamp = 0.0\n",
        "\n",
        "    threshold = max_frames / fps\n",
        "\n",
        "    for line in lines:\n",
        "        if \"WORD START END ASDSCORE\" in line:\n",
        "            flag = 1\n",
        "            continue\n",
        "        if flag:\n",
        "            word, start, end, score = line.split(\" \")\n",
        "            start, end, score = float(start), float(end), float(score)\n",
        "            if end < tmp + threshold:\n",
        "                stack.append(word)\n",
        "                last_timestamp = end\n",
        "            else:\n",
        "                res.append(\n",
        "                    [\n",
        "                        \" \".join(stack),\n",
        "                        start_timestamp,\n",
        "                        last_timestamp,\n",
        "                        last_timestamp - start_timestamp,\n",
        "                    ]\n",
        "                )\n",
        "                tmp = start\n",
        "                start_timestamp = start\n",
        "                stack = [word]\n",
        "    if stack:\n",
        "        res.append([\" \".join(stack), start_timestamp, end, end - start_timestamp])\n",
        "    return res\n",
        "\n",
        "\n",
        "def save_vid_txt(\n",
        "    dst_vid_filename, dst_txt_filename, trim_video_data, content, video_fps=25\n",
        "):\n",
        "    # -- save video\n",
        "    save2vid(dst_vid_filename, trim_video_data, video_fps)\n",
        "    # -- save text\n",
        "    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)\n",
        "    f = open(dst_txt_filename, \"w\", encoding=\"utf-8\")\n",
        "    f.write(f\"{content}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def save_vid_aud(\n",
        "    dst_vid_filename,\n",
        "    dst_aud_filename,\n",
        "    trim_vid_data,\n",
        "    trim_aud_data,\n",
        "    video_fps=25,\n",
        "    audio_sample_rate=16000,\n",
        "):\n",
        "    # -- save video\n",
        "    save2vid(dst_vid_filename, trim_vid_data, video_fps)\n",
        "    # -- save audio\n",
        "    save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)\n",
        "\n",
        "\n",
        "def save_vid_aud_txt(\n",
        "    dst_vid_filename,\n",
        "    dst_aud_filename,\n",
        "    dst_txt_filename,\n",
        "    trim_vid_data,\n",
        "    trim_aud_data,\n",
        "    content,\n",
        "    video_fps=25,\n",
        "    audio_sample_rate=16000,\n",
        "):\n",
        "    # -- save video\n",
        "    if dst_vid_filename is not None:\n",
        "        save2vid(dst_vid_filename, trim_vid_data, video_fps)\n",
        "    # -- save audio\n",
        "    if dst_aud_filename is not None:\n",
        "        save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)\n",
        "    # -- save text\n",
        "    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)\n",
        "    f = open(dst_txt_filename, \"w\", encoding=\"utf-8\")\n",
        "    f.write(f\"{content}\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def save2vid(filename, vid, frames_per_second):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchvision.io.write_video(filename, vid, frames_per_second)\n",
        "\n",
        "\n",
        "def save2aud(filename, aud, sample_rate):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torchaudio.save(filename, aud, sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_SJuNFb_Ycq0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq50mSe0jOLG"
      },
      "source": [
        "# Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfH5SDeijQTL",
        "outputId": "3810c484-bd49-4d95-f998-8a76c95ae9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-jne0b4zn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-jne0b4zn\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802823 sha256=30e2fcf2340138d7985cdb1c3f111db52269dc460bf59039af226d7782ab0d95\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cvf66lii/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YQNyCAtujbq6"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "from moviepy.editor import VideoFileClip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w4WWs7okcK1",
        "outputId": "4ebff29b-4de8-499a-f7fe-6b49c728a75d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:13<00:00, 116MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model_whisper = whisper.load_model(\"medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BHh8cPc_jumK"
      },
      "outputs": [],
      "source": [
        "def obtener_texto(ruta, name):\n",
        "    video = VideoFileClip(ruta)\n",
        "    video.audio.write_audiofile(f'{name}.mp3', logger=None)\n",
        "    i = 0\n",
        "    while True:\n",
        "        result = model_whisper.transcribe(f'{name}.mp3')\n",
        "        temperature = result['segments'][0]['temperature']\n",
        "        compression_ratio = result['segments'][0]['compression_ratio']\n",
        "        no_speech_prob = result['segments'][0]['no_speech_prob']\n",
        "        #print(result)\n",
        "        if temperature == 0.0 and compression_ratio > 0.70:\n",
        "            text = result['text']\n",
        "            break\n",
        "        elif temperature != 1.0 and temperature > 0.2 and compression_ratio > 1.0:\n",
        "            text = result['text']\n",
        "            break\n",
        "        i += 1\n",
        "        if i>3:\n",
        "            print(result)\n",
        "\n",
        "    text = text[1:]\n",
        "    print(text)\n",
        "    print(f'temperature={temperature}')\n",
        "    print(f'compression ratio={compression_ratio}')\n",
        "    print(f'no_speech_prob={no_speech_prob}')\n",
        "    text = text.replace(',','')\n",
        "    text = text.replace('.','')\n",
        "    text = text.replace('¿','')\n",
        "    text = text.replace('?','')\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# def obtener_texto(ruta, audio):\n",
        "#     video = VideoFileClip(ruta)\n",
        "#     video.audio.write_audiofile(f'{audio}.mp3')\n",
        "#     # load audio and pad/trim it to fit 30 seconds\n",
        "#     audio = whisper.load_audio(f'{audio}.mp3')\n",
        "#     audio = whisper.pad_or_trim(audio)\n",
        "#     # make log-Mel spectrogram and move to the same device as the model\n",
        "#     mel = whisper.log_mel_spectrogram(audio).to(model_whisper.device)\n",
        "#     # detect the spoken language\n",
        "#     # _, probs = model.detect_language(mel)\n",
        "#     # print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "#     # decode the audio\n",
        "#     options = whisper.DecodingOptions()\n",
        "#     result = whisper.decode(model_whisper, mel, options)\n",
        "#     # recognized text\n",
        "#     return result.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPwPvqS9Y2Qh"
      },
      "source": [
        "# Generacion de video procesado, y obtencion de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aJZtCRYc81Fq"
      },
      "outputs": [],
      "source": [
        "ruta_videos = '/content/drive/MyDrive/vsr/VPHB_USFX/videos'\n",
        "ruta_dataset = '/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing'\n",
        "list_videos_fa = os.listdir(ruta_videos)\n",
        "list_videos_pre = os.listdir(f'{ruta_dataset}/VPHBUSFX/video')\n",
        "list_videos = [v for v in list_videos_fa if v not in list_videos_pre]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLCa_a0j85YD",
        "outputId": "329d1167-a4b5-4573-c131-d9d6af3784c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "len(list_videos)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_videos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJqCsdsHv9yz",
        "outputId": "b4d0c0a4-c289-4037-e736-8ff7cdaa618e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['c1PQUkqTm3A_V1-0003.mp4', 'j7aGeTco_bo_V1-0005.mp4']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrOUrv8lY4RF",
        "outputId": "deb1fd5f-ad43-41a2-f5f3-afd225568e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5IcvALMj1rI_V1-0004.mp4\n",
            "y la universidad a través del proyecto SUCRE Ciudad Universitario y la cooperación español.\n",
            "temperature=0.0\n",
            "compression ratio=1.0217391304347827\n",
            "no_speech_prob=0.02735489420592785\n",
            "y la universidad a través del proyecto sucre ciudad universitario y la cooperación español\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 1/6 [00:26<02:13, 26.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "5IcvALMj1rI_V1-0003.mp4\n",
            "El último día de trabajo y si es definitivo...\n",
            "temperature=0.0\n",
            "compression ratio=0.8727272727272727\n",
            "no_speech_prob=0.043153323233127594\n",
            "el último día de trabajo y si es definitivo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 2/6 [00:40<01:16, 19.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "5IcvALMj1rI_V1-0002.mp4\n",
            "Es el día de mañana de la escuela taller Sucre y con lo cual la Universidad de San Francisco Javier está abriendo las puertas que significa aquello que\n",
            "temperature=0.0\n",
            "compression ratio=1.2419354838709677\n",
            "no_speech_prob=0.025563877075910568\n",
            "es el día de mañana de la escuela taller sucre y con lo cual la universidad de san francisco javier está abriendo las puertas que significa aquello que\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 3/6 [01:08<01:09, 23.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "5IcvALMj1rI_V1-0001.mp4\n",
            "Que la escuela taller sur le pasaría a la Universidad de San Francisco Javier como una empresa desconcentrada de nuestra universidad.\n",
            "temperature=0.0\n",
            "compression ratio=1.2293577981651376\n",
            "no_speech_prob=0.019386716187000275\n",
            "que la escuela taller sur le pasaría a la universidad de san francisco javier como una empresa desconcentrada de nuestra universidad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 4/6 [01:31<00:46, 23.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "N5b15ZvYGu4_V1-0002.mp4\n",
            "La Universidad San Francisco Javier es la encargada y el ente académico para desarrollar la Olimpiada Boliviana.\n",
            "temperature=0.0\n",
            "compression ratio=1.0865384615384615\n",
            "no_speech_prob=0.03863007202744484\n",
            "la universidad san francisco javier es la encargada y el ente académico para desarrollar la olimpiada boliviana\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 5/6 [01:52<00:22, 22.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n",
            "N5b15ZvYGu4_V1-0001.mp4\n",
            "y orientar a los estudiantes en la culminación de sus trabajos.\n",
            "temperature=0.0\n",
            "compression ratio=0.9552238805970149\n",
            "no_speech_prob=0.02057894878089428\n",
            "y orientar a los estudiantes en la culminación de sus trabajos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [02:05<00:00, 20.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "video_dataloader = AVSRDataLoader(modality=\"video\", detector=\"retinaface\", convert_gray=False)\n",
        "audio_dataloader = AVSRDataLoader(modality=\"audio\")\n",
        "\n",
        "for data_filename in tqdm(list_videos[2:20]):\n",
        "    print(data_filename)\n",
        "    name = data_filename.split('.')[0]\n",
        "    ruta = f'{ruta_videos}/{data_filename}'\n",
        "    # Obtener video y texto\n",
        "    text_data = obtener_texto(ruta, name)\n",
        "    print(text_data)\n",
        "    video_data = video_dataloader.load_data(ruta)\n",
        "    audio_data = audio_dataloader.load_data(ruta)\n",
        "    # Guardar texto y video\n",
        "    output_video_path = f'{ruta_dataset}/VPHBUSFX/video/{name}.mp4'\n",
        "    output_audio_path = f'{ruta_dataset}/VPHBUSFX/video/{name}.wav'\n",
        "    output_text_path = f'{ruta_dataset}/VPHBUSFX/text/{name}.txt'\n",
        "    # save_vid_txt(output_video_path, output_text_path,\n",
        "    #              video_data, text_data)\n",
        "    save_vid_aud_txt(output_video_path, output_audio_path, output_text_path,\n",
        "                    video_data, audio_data, text_data,\n",
        "                    video_fps=25, audio_sample_rate=16000)\n",
        "    print('--------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly65uzLSD_rW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5jVbsPynfIP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vf5mUY66pRJ"
      },
      "source": [
        "# TextTransform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "n_Mh7W8j66-Q"
      },
      "outputs": [],
      "source": [
        "import sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U2pjDXiA6o0S"
      },
      "outputs": [],
      "source": [
        "SP_MODEL_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"drive\",\n",
        "    \"MyDrive\",\n",
        "    \"vsr\",\n",
        "    \"Models\",\n",
        "    \"SentencePiece_castellano_v5\",\n",
        "    \"unigram1000.model\",\n",
        ")\n",
        "\n",
        "DICT_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.dirname(os.path.abspath(''))),\n",
        "    \"content\",\n",
        "    \"drive\",\n",
        "    \"MyDrive\",\n",
        "    \"vsr\",\n",
        "    \"Models\",\n",
        "    \"SentencePiece_castellano_v5\",\n",
        "    \"unigram1000_units.txt\",\n",
        ")\n",
        "\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Mapping Dictionary Class for SentencePiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sp_model_path=SP_MODEL_PATH,\n",
        "        dict_path=DICT_PATH,\n",
        "    ):\n",
        "\n",
        "        # Load SentencePiece model\n",
        "        self.spm = sentencepiece.SentencePieceProcessor(model_file=sp_model_path)\n",
        "\n",
        "        # Load units and create dictionary\n",
        "        units = open(dict_path, encoding='utf8').read().splitlines()\n",
        "        self.hashmap = {unit.split()[0]: unit.split()[-1] for unit in units}\n",
        "        # 0 will be used for \"blank\" in CTC\n",
        "        self.token_list = [\"<blank>\"] + list(self.hashmap.keys()) + [\"<eos>\"]\n",
        "        self.ignore_id = -1\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = self.spm.EncodeAsPieces(text)\n",
        "        token_ids = [self.hashmap.get(token, self.hashmap[\"<unk>\"]) for token in tokens]\n",
        "        return torch.tensor(list(map(int, token_ids)))\n",
        "\n",
        "    def post_process(self, token_ids):\n",
        "        token_ids = token_ids[token_ids != -1]\n",
        "        text = self._ids_to_str(token_ids, self.token_list)\n",
        "        text = text.replace(\"\\u2581\", \" \").strip()\n",
        "        return text\n",
        "\n",
        "    def _ids_to_str(self, token_ids, char_list):\n",
        "        token_as_list = [char_list[idx] for idx in token_ids]\n",
        "        return \"\".join(token_as_list).replace(\"<space>\", \" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu4mQFgBzwGY"
      },
      "source": [
        "# Tokenizer y generacion de csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "z2lOzc6P6Xio"
      },
      "outputs": [],
      "source": [
        "text_transform = TextTransform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "c0EKk85mnfFB"
      },
      "outputs": [],
      "source": [
        "ruta = \"/content/drive/MyDrive/vsr/VPHB_USFX_preprocessing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Yl3NGeGd0_Ev",
        "outputId": "6fbb1878-fb82-41ee-d103-d3f6d17c6751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1241/1241 [03:54<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Dataset                           ruta  input_length  \\\n",
              "0     VPHBUSFX  video/iRF5d2N5wos_V1-0030.mp4           157   \n",
              "1     VPHBUSFX  video/iRF5d2N5wos_V1-0031.mp4            96   \n",
              "2     VPHBUSFX  video/iRF5d2N5wos_V1-0032.mp4           251   \n",
              "3     VPHBUSFX  video/iRF5d2N5wos_V1-0033.mp4            98   \n",
              "4     VPHBUSFX  video/iRF5d2N5wos_V1-0034.mp4           182   \n",
              "...        ...                            ...           ...   \n",
              "1236  VPHBUSFX  video/tV7Ed0l75cg_V1-0005.mp4           333   \n",
              "1237  VPHBUSFX  video/tV7Ed0l75cg_V1-0006.mp4           361   \n",
              "1238  VPHBUSFX  video/tV7Ed0l75cg_V1-0007.mp4           183   \n",
              "1239  VPHBUSFX  video/tV7Ed0l75cg_V1-0008.mp4           438   \n",
              "1240  VPHBUSFX  video/tV7Ed0l75cg_V1-0009.mp4           226   \n",
              "\n",
              "                                               token_id  \n",
              "0     333 120 672 640 551 453 977 349 407 491 63 437...  \n",
              "1                                  199 374 264 265 15 3  \n",
              "2     795 261 265 232 333 120 672 640 709 544 433 19...  \n",
              "3     1002 795 259 950 808 84 15 82 869 613 851 199 ...  \n",
              "4     851 143 950 262 314 808 956 142 531 839 795 65...  \n",
              "...                                                 ...  \n",
              "1236  265 70 694 178 671 63 435 658 359 799 808 189 ...  \n",
              "1237  575 554 192 660 568 643 575 554 795 877 633 55...  \n",
              "1238  665 349 916 6 585 903 808 472 1005 665 511 271...  \n",
              "1239  728 808 862 34 724 795 851 827 320 544 215 745...  \n",
              "1240  728 842 851 836 724 958 733 265 506 691 671 37...  \n",
              "\n",
              "[1241 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e6c86ce-dec2-4955-8ff8-ad08cdef1056\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>ruta</th>\n",
              "      <th>input_length</th>\n",
              "      <th>token_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/iRF5d2N5wos_V1-0030.mp4</td>\n",
              "      <td>157</td>\n",
              "      <td>333 120 672 640 551 453 977 349 407 491 63 437...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/iRF5d2N5wos_V1-0031.mp4</td>\n",
              "      <td>96</td>\n",
              "      <td>199 374 264 265 15 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/iRF5d2N5wos_V1-0032.mp4</td>\n",
              "      <td>251</td>\n",
              "      <td>795 261 265 232 333 120 672 640 709 544 433 19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/iRF5d2N5wos_V1-0033.mp4</td>\n",
              "      <td>98</td>\n",
              "      <td>1002 795 259 950 808 84 15 82 869 613 851 199 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/iRF5d2N5wos_V1-0034.mp4</td>\n",
              "      <td>182</td>\n",
              "      <td>851 143 950 262 314 808 956 142 531 839 795 65...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/tV7Ed0l75cg_V1-0005.mp4</td>\n",
              "      <td>333</td>\n",
              "      <td>265 70 694 178 671 63 435 658 359 799 808 189 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1237</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/tV7Ed0l75cg_V1-0006.mp4</td>\n",
              "      <td>361</td>\n",
              "      <td>575 554 192 660 568 643 575 554 795 877 633 55...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/tV7Ed0l75cg_V1-0007.mp4</td>\n",
              "      <td>183</td>\n",
              "      <td>665 349 916 6 585 903 808 472 1005 665 511 271...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/tV7Ed0l75cg_V1-0008.mp4</td>\n",
              "      <td>438</td>\n",
              "      <td>728 808 862 34 724 795 851 827 320 544 215 745...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>VPHBUSFX</td>\n",
              "      <td>video/tV7Ed0l75cg_V1-0009.mp4</td>\n",
              "      <td>226</td>\n",
              "      <td>728 842 851 836 724 958 733 265 506 691 671 37...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1241 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e6c86ce-dec2-4955-8ff8-ad08cdef1056')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5e6c86ce-dec2-4955-8ff8-ad08cdef1056 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5e6c86ce-dec2-4955-8ff8-ad08cdef1056');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-64fe299e-1b16-4ba1-9f45-49fed4c19b5f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-64fe299e-1b16-4ba1-9f45-49fed4c19b5f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-64fe299e-1b16-4ba1-9f45-49fed4c19b5f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f17b6418-cbb9-4607-b2e3-3a526d0f843f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_csv')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f17b6418-cbb9-4607-b2e3-3a526d0f843f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_csv');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_csv",
              "summary": "{\n  \"name\": \"df_csv\",\n  \"rows\": 1241,\n  \"fields\": [\n    {\n      \"column\": \"Dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"VPHBUSFX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ruta\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1241,\n        \"samples\": [\n          \"video/_oJZK5PcQSU_V1-0030.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 98,\n        \"min\": 54,\n        \"max\": 670,\n        \"num_unique_values\": 369,\n        \"samples\": [\n          448\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1232,\n        \"samples\": [\n          \"495 472 174 543 613 205 808 770 549 439 32 140 322 472 561 33 795 368 712 280 319 800 738 538 672 749 544 392 703 561 143 178 795 446 115 808 472 643 262 314 199 811 546 32 490 843 795 851 483 643 737 33 613\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "list_video = [video for video in os.listdir(f'{ruta}/VPHBUSFX/video') if video.split('.')[1]=='mp4']\n",
        "list_video_text = [[f'{ruta}/VPHBUSFX/video/{name}.mp4',f'{ruta}/VPHBUSFX/text/{name}.txt'] for name in [n.split('.')[0] for n in list_video]]\n",
        "list_csv = []\n",
        "for r_vid, r_text in tqdm(list_video_text):\n",
        "    video = torchvision.io.read_video(r_vid, pts_unit=\"sec\")[0].numpy()\n",
        "    text = open(r_text, 'r', encoding='utf-8').read()\n",
        "    token_id_str = \" \".join(\n",
        "            map(str, [_.item() for _ in text_transform.tokenize(text)])\n",
        "    )\n",
        "    name_video = r_vid.split('/')[-1]\n",
        "    list_csv.append(['VPHBUSFX', f'video/{name_video}', video.shape[0], token_id_str])\n",
        "df_csv = pd.DataFrame(list_csv, columns=['Dataset','ruta','input_length','token_id'])\n",
        "df_csv.to_csv(f'{ruta}/labels/VPHBUSFX_unigram1000.csv')\n",
        "df_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3pA7PK_-z6c"
      },
      "source": [
        "# Obtener train, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vz2kU0p-1U9U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gPKGInxa2f9j"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df_csv, test_size=0.08)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPYiFpJs3DYU",
        "outputId": "bbc5aae9-989d-4767-a7d2-64ea75ead23b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1141, 4), (100, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df_train.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['ruta'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB0MZIzB1Qvl",
        "outputId": "ca34be40-60fb-47b2-8114-bcd83d0af45b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['video/HKl3NUX0zqY_V1-0082.mp4',\n",
              " 'video/9zQHs2M672o_V1-0029.mp4',\n",
              " 'video/20qxEI3vPk4_V1-0008.mp4',\n",
              " 'video/tV7Ed0l75cg_V1-0029.mp4',\n",
              " 'video/il19JIlTFbk_V1-0005.mp4',\n",
              " 'video/djxKI27Wj0Y_V1-0042.mp4',\n",
              " 'video/iRF5d2N5wos_V1-0008.mp4',\n",
              " 'video/HKl3NUX0zqY_V1-0033.mp4',\n",
              " 'video/j98ytFr26rY_V1-0010.mp4',\n",
              " 'video/n4uXDD96dyc_V1-0009.mp4',\n",
              " 'video/_oJZK5PcQSU_V1-0034.mp4',\n",
              " 'video/rScey82jhoI_V1-0046.mp4',\n",
              " 'video/iRF5d2N5wos_V1-0019.mp4',\n",
              " 'video/iRF5d2N5wos_V1-0005.mp4',\n",
              " 'video/SuwmkMqxASM_V1-0021.mp4',\n",
              " 'video/il19JIlTFbk_V1-0070.mp4',\n",
              " 'video/mZxweoeJEcc_V1-0008.mp4',\n",
              " 'video/78ErCXX7DY4_V1-0001.mp4',\n",
              " 'video/MNX4n4on6fI_V1-0005.mp4',\n",
              " 'video/rScey82jhoI_V1-0009.mp4',\n",
              " 'video/wiD4D-jJ3SY_V1-0038.mp4',\n",
              " 'video/UcwFdX1Gbwc_V1-0001.mp4',\n",
              " 'video/iRF5d2N5wos_V1-0011.mp4',\n",
              " 'video/n4uXDD96dyc_V1-0002.mp4',\n",
              " 'video/rScey82jhoI_V1-0049.mp4',\n",
              " 'video/iRF5d2N5wos_V1-0052.mp4',\n",
              " 'video/kBqQfazsWUw_V1-0083.mp4',\n",
              " 'video/5IcvALMj1rI_V1-0006.mp4',\n",
              " 'video/j98ytFr26rY_V1-0039.mp4',\n",
              " 'video/tV7Ed0l75cg_V1-0027.mp4',\n",
              " 'video/djxKI27Wj0Y_V1-0036.mp4',\n",
              " 'video/rScey82jhoI_V1-0024.mp4',\n",
              " 'video/1dudfrIceDw_V1-0020.mp4',\n",
              " 'video/1dudfrIceDw_V1-0051.mp4',\n",
              " 'video/MNX4n4on6fI_V1-0007.mp4',\n",
              " 'video/SuwmkMqxASM_V1-0066.mp4',\n",
              " 'video/kBqQfazsWUw_V1-0066.mp4',\n",
              " 'video/AV0a6BAE35M_V1-0010.mp4',\n",
              " 'video/V3qgtN0q0iQ_V1-0014.mp4',\n",
              " 'video/V3qgtN0q0iQ_V1-0013.mp4',\n",
              " 'video/rScey82jhoI_V1-0001.mp4',\n",
              " 'video/SuwmkMqxASM_V1-0055.mp4',\n",
              " 'video/_oJZK5PcQSU_V1-0010.mp4',\n",
              " 'video/tV7Ed0l75cg_V1-0008.mp4',\n",
              " 'video/SuwmkMqxASM_V1-0029.mp4',\n",
              " 'video/SuwmkMqxASM_V1-0033.mp4',\n",
              " 'video/j98ytFr26rY_V1-0029.mp4',\n",
              " 'video/HKl3NUX0zqY_V1-0060.mp4',\n",
              " 'video/_oJZK5PcQSU_V1-0019.mp4',\n",
              " 'video/1dudfrIceDw_V1-0049.mp4',\n",
              " 'video/SaKZhaT47Yg_V1-0002.mp4',\n",
              " 'video/_oJZK5PcQSU_V1-0005.mp4',\n",
              " 'video/kBqQfazsWUw_V1-0053.mp4',\n",
              " 'video/4yquiL-r1J0_V1-0026.mp4',\n",
              " 'video/9zQHs2M672o_V1-0015.mp4',\n",
              " 'video/SuwmkMqxASM_V1-0049.mp4',\n",
              " 'video/SuwmkMqxASM_V1-0069.mp4',\n",
              " 'video/44Ee_Oa6dYc_V1-0026.mp4',\n",
              " 'video/9zQHs2M672o_V1-0018.mp4',\n",
              " 'video/SuwmkMqxASM_V1-0054.mp4',\n",
              " 'video/HKl3NUX0zqY_V1-0005.mp4',\n",
              " 'video/_oJZK5PcQSU_V1-0014.mp4',\n",
              " 'video/HKl3NUX0zqY_V1-0044.mp4',\n",
              " 'video/mZxweoeJEcc_V1-0017.mp4',\n",
              " 'video/c1PQUkqTm3A_V1-0002.mp4',\n",
              " 'video/tV7Ed0l75cg_V1-0048.mp4',\n",
              " 'video/V3qgtN0q0iQ_V1-0004.mp4',\n",
              " 'video/V3qgtN0q0iQ_V1-0009.mp4',\n",
              " 'video/n4uXDD96dyc_V1-0016.mp4',\n",
              " 'video/HKl3NUX0zqY_V1-0047.mp4',\n",
              " 'video/wiD4D-jJ3SY_V1-0043.mp4',\n",
              " 'video/il19JIlTFbk_V1-0009.mp4',\n",
              " 'video/zdxBYPoXfPY_V1-0004.mp4',\n",
              " 'video/wiD4D-jJ3SY_V1-0018.mp4',\n",
              " 'video/kBqQfazsWUw_V1-0080.mp4',\n",
              " 'video/tV7Ed0l75cg_V1-0039.mp4',\n",
              " 'video/HKl3NUX0zqY_V1-0085.mp4',\n",
              " 'video/4yquiL-r1J0_V1-0010.mp4',\n",
              " 'video/il19JIlTFbk_V1-0029.mp4',\n",
              " 'video/1dudfrIceDw_V1-0017.mp4',\n",
              " 'video/9zQHs2M672o_V1-0017.mp4',\n",
              " 'video/HKl3NUX0zqY_V1-0080.mp4',\n",
              " 'video/IvveiO9WMp4_V1-0003.mp4',\n",
              " 'video/V3qgtN0q0iQ_V1-0035.mp4',\n",
              " 'video/iRF5d2N5wos_V1-0031.mp4',\n",
              " 'video/wiD4D-jJ3SY_V1-0025.mp4',\n",
              " 'video/tdOLOgUWCWw_V1-0019.mp4',\n",
              " 'video/rScey82jhoI_V1-0010.mp4',\n",
              " 'video/n4uXDD96dyc_V1-0022.mp4',\n",
              " 'video/85G4PNDcxdI_V1-0004.mp4',\n",
              " 'video/iRF5d2N5wos_V1-0051.mp4',\n",
              " 'video/SaKZhaT47Yg_V1-0005.mp4',\n",
              " 'video/4yquiL-r1J0_V1-0018.mp4',\n",
              " 'video/wiD4D-jJ3SY_V1-0008.mp4',\n",
              " 'video/9zQHs2M672o_V1-0030.mp4',\n",
              " 'video/5IcvALMj1rI_V1-0003.mp4',\n",
              " 'video/wiD4D-jJ3SY_V1-0044.mp4',\n",
              " 'video/j98ytFr26rY_V1-0024.mp4',\n",
              " 'video/AV0a6BAE35M_V1-0011.mp4',\n",
              " 'video/djxKI27Wj0Y_V1-0047.mp4']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-zu5jv8E99-7"
      },
      "outputs": [],
      "source": [
        "df_train.to_csv(f'{ruta}/labels/VPHBUSFX_unigram1000_train.csv')\n",
        "df_test.to_csv(f'{ruta}/labels/VPHBUSFX_unigram1000_val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0dxNRb6-3G-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmKRDsSp-2_s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN8l2ftSeKiq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1jLgxJNjUvK0",
        "2rIk5Xf7Urtz",
        "zAvtziY3UFbe",
        "CiL6IrYoYdOy",
        "Wq50mSe0jOLG",
        "yPwPvqS9Y2Qh"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}